diff --git a/Documentation/ktsan.txt b/Documentation/ktsan.txt
new file mode 100644
index 000000000000..9233f11b6848
--- /dev/null
+++ b/Documentation/ktsan.txt
@@ -0,0 +1,242 @@
+KernelThreadSanitizer
+=====================
+
+0. Overview
+===========
+
+KernelThreadSanitizer (ktsan) is a dynamic data race detector for Linux kernel.
+Supports only x86_64 SMP kernel. Currently in development.
+A patched GCC is required to build the kernel with ktsan enabled.
+For the current state of the tool see the 'Current state' section.
+
+Project homepage: https://code.google.com/p/thread-sanitizer/wiki/ThreadSanitizerForKernel
+
+Project repository: https://github.com/google/ktsan
+
+
+1. Reports
+==========
+
+Here is an example of a report that can be obtained while running ktsan:
+
+==================================================================
+ThreadSanitizer: data-race in __kmem_cache_alias
+
+Write of size 4 by thread T1 (K1):
+ [<ffffffff812376e1>] __kmem_cache_alias+0x81/0xa0 mm/slab.c:2071
+ [<ffffffff811ed2ce>] kmem_cache_create+0x5e/0x2b0 mm/slab_common.c:389
+ [<ffffffff82564a99>] scsi_init_queue+0xac/0x1ac drivers/scsi/scsi_lib.c:2266
+ [<ffffffff82564964>] init_scsi+0x15/0x9e drivers/scsi/scsi.c:1228
+ [<ffffffff8100033d>] do_one_initcall+0xbd/0x220 init/main.c:802
+ [<     inlined    >] kernel_init_freeable+0x2c4/0x391 do_initcall_level init/main.c:867
+ [<     inlined    >] kernel_init_freeable+0x2c4/0x391 do_initcalls init/main.c:875
+ [<     inlined    >] kernel_init_freeable+0x2c4/0x391 do_basic_setup init/main.c:894
+ [<ffffffff82508923>] kernel_init_freeable+0x2c4/0x391 init/main.c:1020
+ [<ffffffff81e2aaf6>] kernel_init+0x16/0x150 init/main.c:945
+ [<ffffffff81e3f6bc>] ret_from_fork+0x7c/0xb0 arch/x86/kernel/entry_64.S:347
+DBG: cpu = ffff88053fc9df10
+
+Previous read of size 4 by thread T459 (K456):
+ [<ffffffff8123441c>] kmem_cache_alloc+0xfc/0x8e0 mm/slab.c:3404
+ [<ffffffff8126702c>] getname_kernel+0x6c/0xd0 fs/namei.c:230
+ [<ffffffff810a3100>] ____call_usermodehelper+0x230/0x2d0 kernel/kmod.c:256
+ [<ffffffff81e3f6bc>] ret_from_fork+0x7c/0xb0 arch/x86/kernel/entry_64.S:347
+DBG: cpu = 0
+
+DBG: addr: ffff880203400774
+DBG: first offset: 4, second offset: 4
+DBG: T1 clock: {T1: 6677455, T459: 0}
+DBG: T459 clock: {T459: 986}
+==================================================================
+
+This report was preprocessed using our symbolizer script that adds file and
+line info, see the homepage for details.
+
+As the report shows, there is a data race between a write of size 4 in
+__kmem_cache_alias and a read of size 4 in kmem_cache_alloc, both of which
+access cachep->object_size without any synchronization.
+
+This report haven't been sent upstream, since kernel developers tend to think
+that 4-byte sized loads and stores are atomic.
+
+
+2. Technical description
+========================
+
+Overall ktsan logic is similar to that of the userspace ThreadSanitizer.
+Compiler instrumentation is used to add a __tsan__loadN / __tsan_storeN
+calls before each memory access. Also a lot of hooks are added in scheduler
+and various synchronization primitives. All theese hooks are used to gather
+information about kernel's execution. A report is printed when a data race
+is detected.
+
+Information about ThreadSanitizer alorithm can be found on the homepage
+of the tool. For questions contact dvyukov@google.com.
+
+TODO: describe ThreadSanitizer algorithm
+
+Currently ktsan has support for the following synchronization primitives:
+spinlock, rwlock, sema, rwsem, completion, mutex, atomics, atomic bitops,
+rcu, rcu_bh, rcu_sched, bit_lock, per-cpu variables.
+
+Support for the following primitives is not added yet: srcu, memory barriers.
+
+
+3. Implementation details
+=========================
+
+Instrumentation
+---------------
+
+A patched GCC is required to built the kernel with ktsan enabled.
+More information can be found on the homepage.
+
+The following kernel parts are not instrumented:
+arch/x86/boot/, arch/x86/boot/compressed/, arch/x86/kernel/nmi.o,
+arch/x86/kernel/cpu/common.o, arch/x86/kernel/cpu/perf_event.o,
+arch/x86/realmode/, arch/x86/vdso/, kernel/sched/, kernel/softirq.o.
+
+Mainly the reason for these parts being not instrumented is that kernel
+doesn't boot when instrumentation is enabled. The exact reasons for this
+behavior weren't investigated. Also kernel/softirq.o instumentation had
+to be disabled, since it corrupts the trace (some functions are entered,
+but not exited).
+
+The lack of instrumentation can cause parts of the stack traces go missing.
+This happens only for stack traces of the previous access, since they are
+restored using trace. In particular this happens in the rb_insert_color report,
+because a few stack trace's frames come from kernel/sched/core.o.
+
+To disable the instrumentation of a particular kernel part add to makefile
+KTSAN_SANITIZE := n (to disable instrumentation of a whole module) or
+KTSAN_SANITIZE_file.o = n (to disable instrumentation of a particular file).
+
+Right now many kernel developers don't use atomics explicitly thinking that
+all less-than-8 bytes sized memory accesses should be atomic. Sometimes they
+even use ACCESS_ONCE to prevent compiler's reordering. Because of the large
+amount of "benign" races caused by this we decided to ignore memory accesses
+which are done using ACCESS_ONCE for now. This is done by disabling compiler
+instrumentation for volatile types' accesses. Therefore ACCESS_ONCE may be
+used to suppress unwanted race reports.
+
+
+Shadow
+------
+
+The shadow memory holds information about a few (4 right now) last memory
+accesses to each memory cell (with is 8-byte sized now). So for each 8 bytes
+of kernel memory threre are 32 bytes of shadow memory, which contain the
+thread id, clock, size, etc. of a few last accesses to these 8 bytes.
+
+For each physical memory page another 4 shadow pages are allocated. The
+implementation is almost same as in kmemcheck. The '__GFP_NOTRACK' flag
+can be used to disable allocating shadow for a particular memory page.
+
+
+Threads
+-------
+
+Each kernel thread has a corresponding ktsan thread structure. It contains
+various fields: ids, clock, trace, etc. (see mm/ktsan/ktsan.h).
+
+All the synchronization events and memory accesses that come from scheduler
+are ignored. This is done so two consequently executed theads not to be
+sychronized with each other.
+
+Each thread has a cpu pointer assigned to it. When a thread enters scheduler
+the cpu pointer is assigned to NULL. When it leaves scheduler it assigned
+to the pointer to the new cpu it's strted to run on. Before each synchronization
+event or memory access the cpu pointer of the current thread is checked. If
+it's equal to NULL the event is ignored.
+
+Also all ktsan events that come from interrupts are now ignored. It causes
+some issues since interrupts might be used for synchronization (see the
+'Current state' section).
+
+Since a data race may happen after a kernel thread was destroyed, the ktsan
+thread structure is put in quaratine instead of being freed.
+
+
+Synchronization primitives
+--------------------------
+
+A clock for each sync object (see ThreadSanitizer algorithm) is stored in a
+sync object structre (see mm/ktsan/ktsan.h), which is allocated when the
+sync object is accessed for the first time. When a block of memory is freed,
+we delete the structures of the sync objects lying in this block.
+
+Also when freeing a block of memory we make an artificial write into every byte
+of this block to catch racy use-after-free accesses. When allocating a memory
+block we imitate access to this block clearing all the previous accesses'
+description from the according shadow memory.
+
+TODO: sync primitives support implementation
+
+Atomics and memory barrier pairing are not supported yet. Right now every
+atomic operations is considered as one with an appropriate memory barrier(s).
+
+
+Other
+-----
+
+When kernel boots ktsan is not enabled from the very beginning, since it
+requires some kernel parts to be initialized before it can be enabled.
+After that ktsan is enabled by calling the 'ktsan_init()' function.
+There is also 'ktsan_init_early()' that reserves physical memory that must
+be called before 'kmem_cache_init()'. (Actually everything from 'ktsan_init()'
+can probably be moved to 'ktsan_init_early()'). We don't use kmem_caches, but
+have our own allocator.
+
+Before handling any ktsan event (memory accesses, syncronization events, etc.)
+we ensure that 1) ktsan is enabled, 2) we are not in an interrupt, 3) we are
+not in scheduler (unless this events should be handled even if it comes from
+scheduler, e.g. thread start and stop events), 4) we are not in ktsan runtime
+already (to avoid recursion). If any of the checks fail we ignore the event.
+See the macro 'ENTER' in mm/ktsan/ktsan.h for details.
+
+There a few tests for ktsan that deliberately try to do data races to see if
+ktsan can detect them. The tests can be run by writing 'tsan_run_tests' into
+the /proc/ktsan_tests file. Right now there a few false negatives in the tests.
+
+Also ktsan gather various statisticcs while running. It can be seen by reading
+the /proc/ktsan_stats file.
+
+
+4. Current state
+================
+
+Project repository: https://github.com/google/ktsan
+
+There are two important branches in the repository: tsan for ktsan changes
+itself, and tsan-fixes for various false positive fixes and suppressions.
+
+Currently the kernel with ktsan enabled boots, but still produces a lot of
+false positive reports. It seems that to get rid of these reports, events that
+come from interrupts shouldn't be ignored by ktsan since they are used for some
+synchronization.
+
+Right now ktsan requires a lot of physical memory (about 16 GB), but this value
+can be reduced by descreasing 'KT_MAX_THREAD_ID' in mm/ktsan.h. However it
+shouldn't be less than the maximum number of alive threads.
+
+
+5. Other notes
+==============
+
+Atomics + memory barriers
+-------------------------
+
+Stand-alone memory barrier (membar) support:
+You will need to add 2 additional vector clocks (VCs) per thread: one
+for non-materialized acquire synchronization (acquire_clk) and another
+for non-materialized release synchronization (release_clk).
+Then, instrument all relaxed/membar-less atomic loads with
+kt_clk_acquire(&thr->acquire_clk, &atomic_var->clk). And instrument
+rmb (read memory barrier) with kt_clk_acquire(&thr->clk,
+&thr->acquire_clk). This will effectively turn "atomic_load; rmb"
+sequence into load-acquire.
+Similarly for wmb (write memory barrier): instrument wmb with
+kt_clk_release(&thr->release_clk, &thr->clk); and relaxed/membar-less
+atomic stores with kt_clk_release(&atomic_var->clk,
+&thr->release_clk). This will effectively turn "wmb; atomic_store"
+sequence into store-release.
diff --git a/Makefile b/Makefile
index 3e4868a6498b..cdf0e1d58999 100644
--- a/Makefile
+++ b/Makefile
@@ -435,6 +435,7 @@ LDFLAGS_MODULE  =
 CFLAGS_KERNEL	=
 AFLAGS_KERNEL	=
 LDFLAGS_vmlinux =
+CFLAGS_KTSAN	= -fsanitize=thread
 
 # Use USERINCLUDE when you must reference the UAPI directories only.
 USERINCLUDE    := \
@@ -475,7 +476,7 @@ export HOSTCXX KBUILD_HOSTCXXFLAGS LDFLAGS_MODULE CHECK CHECKFLAGS
 
 export KBUILD_CPPFLAGS NOSTDINC_FLAGS LINUXINCLUDE OBJCOPYFLAGS KBUILD_LDFLAGS
 export KBUILD_CFLAGS CFLAGS_KERNEL CFLAGS_MODULE
-export CFLAGS_KASAN CFLAGS_KASAN_NOSANITIZE CFLAGS_UBSAN
+export CFLAGS_KASAN CFLAGS_KASAN_NOSANITIZE CFLAGS_UBSAN CFLAGS_KTSAN
 export KBUILD_AFLAGS AFLAGS_KERNEL AFLAGS_MODULE
 export KBUILD_AFLAGS_MODULE KBUILD_CFLAGS_MODULE KBUILD_LDFLAGS_MODULE
 export KBUILD_AFLAGS_KERNEL KBUILD_CFLAGS_KERNEL
diff --git a/arch/Kconfig b/arch/Kconfig
index c47b328eada0..a1ade84d8861 100644
--- a/arch/Kconfig
+++ b/arch/Kconfig
@@ -827,7 +827,7 @@ config HAVE_ARCH_VMAP_STACK
 config VMAP_STACK
 	default y
 	bool "Use a virtually-mapped stack"
-	depends on HAVE_ARCH_VMAP_STACK && !KASAN
+	depends on HAVE_ARCH_VMAP_STACK && !KASAN && !KTSAN
 	---help---
 	  Enable this if you want the use virtually-mapped kernel stacks
 	  with guard pages.  This causes kernel stack overflows to be
diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index 2bbbd4d1ba31..729d0bbdd208 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -217,6 +217,7 @@ config X86
 	select USER_STACKTRACE_SUPPORT
 	select VIRT_TO_BUS
 	select X86_FEATURE_NAMES		if PROC_FS
+	select HAVE_ARCH_KTSAN if X86_64
 
 config INSTRUCTION_DECODER
 	def_bool y
diff --git a/arch/x86/boot/Makefile b/arch/x86/boot/Makefile
index e2839b5c246c..1fc214c82190 100644
--- a/arch/x86/boot/Makefile
+++ b/arch/x86/boot/Makefile
@@ -10,6 +10,7 @@
 #
 
 KASAN_SANITIZE			:= n
+KTSAN_SANITIZE			:= n
 OBJECT_FILES_NON_STANDARD	:= y
 
 # Kernel does not boot with kcov instrumentation here.
diff --git a/arch/x86/boot/compressed/Makefile b/arch/x86/boot/compressed/Makefile
index 6b84afdd7538..be953ddde037 100644
--- a/arch/x86/boot/compressed/Makefile
+++ b/arch/x86/boot/compressed/Makefile
@@ -18,6 +18,7 @@
 #	compressed vmlinux.bin.all + u32 size of vmlinux.bin.all
 
 KASAN_SANITIZE			:= n
+KTSAN_SANITIZE			:= n
 OBJECT_FILES_NON_STANDARD	:= y
 
 # Prevents link failures: __sanitizer_cov_trace_pc() is not linked in.
diff --git a/arch/x86/entry/common.c b/arch/x86/entry/common.c
index 2418804e66b4..586bde7c327a 100644
--- a/arch/x86/entry/common.c
+++ b/arch/x86/entry/common.c
@@ -26,6 +26,7 @@
 #include <linux/livepatch.h>
 #include <linux/syscalls.h>
 #include <linux/uaccess.h>
+#include <linux/ktsan.h>
 
 #include <asm/desc.h>
 #include <asm/traps.h>
@@ -298,7 +299,9 @@ __visible void do_syscall_64(unsigned long nr, struct pt_regs *regs)
 	nr &= __SYSCALL_MASK;
 	if (likely(nr < NR_syscalls)) {
 		nr = array_index_nospec(nr, NR_syscalls);
+		ktsan_syscall_enter();
 		regs->ax = sys_call_table[nr](regs);
+		ktsan_syscall_exit();
 	}
 
 	syscall_return_slowpath(regs);
diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S
index 11aa3b2afa4d..ba4b5bf13c36 100644
--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@ -38,6 +38,7 @@
 #include <asm/export.h>
 #include <asm/frame.h>
 #include <asm/nospec-branch.h>
+#include <asm/ktsan.h>
 #include <linux/err.h>
 
 #include "calling.h"
@@ -580,7 +581,9 @@ common_interrupt:
 	addq	$-0x80, (%rsp)			/* Adjust vector to [-256, -1] range */
 	call	interrupt_entry
 	UNWIND_HINT_REGS indirect=1
+	KTSAN_INTERRUPT_ENTER
 	call	do_IRQ	/* rdi points to pt_regs */
+	KTSAN_INTERRUPT_EXIT
 	/* 0(%rsp): old RSP */
 ret_from_intr:
 	DISABLE_INTERRUPTS(CLBR_ANY)
diff --git a/arch/x86/entry/vdso/Makefile b/arch/x86/entry/vdso/Makefile
index 42fe42e82baf..9d43630a0575 100644
--- a/arch/x86/entry/vdso/Makefile
+++ b/arch/x86/entry/vdso/Makefile
@@ -6,6 +6,7 @@
 KBUILD_CFLAGS += $(DISABLE_LTO)
 KASAN_SANITIZE			:= n
 UBSAN_SANITIZE			:= n
+KTSAN_SANITIZE			:= n
 OBJECT_FILES_NON_STANDARD	:= y
 
 # Prevents link failures: __sanitizer_cov_trace_pc() is not linked in.
diff --git a/arch/x86/entry/vdso/vclock_gettime.c b/arch/x86/entry/vdso/vclock_gettime.c
index 4aed41f638bb..e63b289378c2 100644
--- a/arch/x86/entry/vdso/vclock_gettime.c
+++ b/arch/x86/entry/vdso/vclock_gettime.c
@@ -11,6 +11,9 @@
  * Check with readelf after changing.
  */
 
+/* Disable annotations in userspace code. */
+#undef CONFIG_KTSAN
+
 #include <uapi/linux/time.h>
 #include <asm/vgtod.h>
 #include <asm/vvar.h>
diff --git a/arch/x86/include/asm/atomic.h b/arch/x86/include/asm/atomic.h
index ea3d95275b43..cea7cf0484bf 100644
--- a/arch/x86/include/asm/atomic.h
+++ b/arch/x86/include/asm/atomic.h
@@ -4,6 +4,7 @@
 
 #include <linux/compiler.h>
 #include <linux/types.h>
+#include <linux/ktsan.h>
 #include <asm/alternative.h>
 #include <asm/cmpxchg.h>
 #include <asm/rmwcc.h>
@@ -24,11 +25,15 @@
  */
 static __always_inline int arch_atomic_read(const atomic_t *v)
 {
+#ifndef CONFIG_KTSAN
 	/*
 	 * Note for KASAN: we deliberately don't use READ_ONCE_NOCHECK() here,
 	 * it's non-inlined function that increases binary size and stack usage.
 	 */
 	return READ_ONCE((v)->counter);
+#else
+	return ktsan_atomic32_load((void *)v, ktsan_memory_order_relaxed);
+#endif
 }
 
 /**
@@ -40,7 +45,11 @@ static __always_inline int arch_atomic_read(const atomic_t *v)
  */
 static __always_inline void arch_atomic_set(atomic_t *v, int i)
 {
+#ifndef CONFIG_KTSAN
 	WRITE_ONCE(v->counter, i);
+#else
+	ktsan_atomic32_store((void *)v, i, ktsan_memory_order_relaxed);
+#endif
 }
 
 /**
@@ -52,9 +61,13 @@ static __always_inline void arch_atomic_set(atomic_t *v, int i)
  */
 static __always_inline void arch_atomic_add(int i, atomic_t *v)
 {
+#ifndef CONFIG_KTSAN
 	asm volatile(LOCK_PREFIX "addl %1,%0"
 		     : "+m" (v->counter)
 		     : "ir" (i));
+#else
+	ktsan_atomic32_fetch_add((void *)v, i, ktsan_memory_order_relaxed);
+#endif
 }
 
 /**
@@ -66,9 +79,13 @@ static __always_inline void arch_atomic_add(int i, atomic_t *v)
  */
 static __always_inline void arch_atomic_sub(int i, atomic_t *v)
 {
+#ifndef CONFIG_KTSAN
 	asm volatile(LOCK_PREFIX "subl %1,%0"
 		     : "+m" (v->counter)
 		     : "ir" (i));
+#else
+	ktsan_atomic32_fetch_add((void *)v, -i, ktsan_memory_order_relaxed);
+#endif
 }
 
 /**
@@ -82,7 +99,12 @@ static __always_inline void arch_atomic_sub(int i, atomic_t *v)
  */
 static __always_inline bool arch_atomic_sub_and_test(int i, atomic_t *v)
 {
+#ifndef CONFIG_KTSAN
 	return GEN_BINARY_RMWcc(LOCK_PREFIX "subl", v->counter, e, "er", i);
+#else
+	return (ktsan_atomic32_fetch_add((void *)v, -i,
+			ktsan_memory_order_acq_rel) - i) == 0;
+#endif
 }
 #define arch_atomic_sub_and_test arch_atomic_sub_and_test
 
@@ -94,8 +116,12 @@ static __always_inline bool arch_atomic_sub_and_test(int i, atomic_t *v)
  */
 static __always_inline void arch_atomic_inc(atomic_t *v)
 {
+#ifndef CONFIG_KTSAN
 	asm volatile(LOCK_PREFIX "incl %0"
 		     : "+m" (v->counter));
+#else
+	ktsan_atomic32_fetch_add((void *)v, 1, ktsan_memory_order_relaxed);
+#endif
 }
 #define arch_atomic_inc arch_atomic_inc
 
@@ -107,8 +133,12 @@ static __always_inline void arch_atomic_inc(atomic_t *v)
  */
 static __always_inline void arch_atomic_dec(atomic_t *v)
 {
+#ifndef CONFIG_KTSAN
 	asm volatile(LOCK_PREFIX "decl %0"
 		     : "+m" (v->counter));
+#else
+	ktsan_atomic32_fetch_add((void *)v, -1, ktsan_memory_order_relaxed);
+#endif
 }
 #define arch_atomic_dec arch_atomic_dec
 
@@ -122,7 +152,12 @@ static __always_inline void arch_atomic_dec(atomic_t *v)
  */
 static __always_inline bool arch_atomic_dec_and_test(atomic_t *v)
 {
+#ifndef CONFIG_KTSAN
 	return GEN_UNARY_RMWcc(LOCK_PREFIX "decl", v->counter, e);
+#else
+	return (ktsan_atomic32_fetch_add((void *)v, -1,
+			ktsan_memory_order_acq_rel) - 1) == 0;
+#endif
 }
 #define arch_atomic_dec_and_test arch_atomic_dec_and_test
 
@@ -136,7 +171,12 @@ static __always_inline bool arch_atomic_dec_and_test(atomic_t *v)
  */
 static __always_inline bool arch_atomic_inc_and_test(atomic_t *v)
 {
+#ifndef CONFIG_KTSAN
 	return GEN_UNARY_RMWcc(LOCK_PREFIX "incl", v->counter, e);
+#else
+	return (ktsan_atomic32_fetch_add((void *)v, 1,
+			ktsan_memory_order_acq_rel) + 1) == 0;
+#endif
 }
 #define arch_atomic_inc_and_test arch_atomic_inc_and_test
 
@@ -151,7 +191,12 @@ static __always_inline bool arch_atomic_inc_and_test(atomic_t *v)
  */
 static __always_inline bool arch_atomic_add_negative(int i, atomic_t *v)
 {
+#ifndef CONFIG_KTSAN
 	return GEN_BINARY_RMWcc(LOCK_PREFIX "addl", v->counter, s, "er", i);
+#else
+	return ((int)ktsan_atomic32_fetch_add((void *)v, i,
+			ktsan_memory_order_acq_rel) + i) < 0;
+#endif
 }
 #define arch_atomic_add_negative arch_atomic_add_negative
 
@@ -164,7 +209,12 @@ static __always_inline bool arch_atomic_add_negative(int i, atomic_t *v)
  */
 static __always_inline int arch_atomic_add_return(int i, atomic_t *v)
 {
+#ifndef CONFIG_KTSAN
 	return i + xadd(&v->counter, i);
+#else
+	return (ktsan_atomic32_fetch_add((void *)v, i,
+			ktsan_memory_order_acq_rel) + i);
+#endif
 }
 
 /**
@@ -176,7 +226,12 @@ static __always_inline int arch_atomic_add_return(int i, atomic_t *v)
  */
 static __always_inline int arch_atomic_sub_return(int i, atomic_t *v)
 {
+#ifndef CONFIG_KTSAN
 	return arch_atomic_add_return(-i, v);
+#else
+	return (ktsan_atomic32_fetch_add((void *)v, -i,
+			ktsan_memory_order_acq_rel) - i);
+#endif
 }
 
 static __always_inline int arch_atomic_fetch_add(int i, atomic_t *v)
@@ -191,7 +246,12 @@ static __always_inline int arch_atomic_fetch_sub(int i, atomic_t *v)
 
 static __always_inline int arch_atomic_cmpxchg(atomic_t *v, int old, int new)
 {
+#ifndef CONFIG_KTSAN
 	return arch_cmpxchg(&v->counter, old, new);
+#else
+	return ktsan_atomic32_compare_exchange((void *)v, old, new,
+			ktsan_memory_order_acq_rel);
+#endif
 }
 
 #define arch_atomic_try_cmpxchg arch_atomic_try_cmpxchg
@@ -202,7 +262,12 @@ static __always_inline bool arch_atomic_try_cmpxchg(atomic_t *v, int *old, int n
 
 static inline int arch_atomic_xchg(atomic_t *v, int new)
 {
+#ifndef CONFIG_KTSAN
 	return arch_xchg(&v->counter, new);
+#else
+	return ktsan_atomic32_exchange((void *)v, new,
+			ktsan_memory_order_acq_rel);
+#endif
 }
 
 static inline void arch_atomic_and(int i, atomic_t *v)
diff --git a/arch/x86/include/asm/atomic64_64.h b/arch/x86/include/asm/atomic64_64.h
index dadc20adba21..4efc92485f9e 100644
--- a/arch/x86/include/asm/atomic64_64.h
+++ b/arch/x86/include/asm/atomic64_64.h
@@ -3,6 +3,7 @@
 #define _ASM_X86_ATOMIC64_64_H
 
 #include <linux/types.h>
+#include <linux/ktsan.h>
 #include <asm/alternative.h>
 #include <asm/cmpxchg.h>
 
@@ -19,7 +20,11 @@
  */
 static inline long arch_atomic64_read(const atomic64_t *v)
 {
+#ifndef CONFIG_KTSAN
 	return READ_ONCE((v)->counter);
+#else
+	return ktsan_atomic64_load((void *)v, ktsan_memory_order_relaxed);
+#endif
 }
 
 /**
@@ -31,7 +36,11 @@ static inline long arch_atomic64_read(const atomic64_t *v)
  */
 static inline void arch_atomic64_set(atomic64_t *v, long i)
 {
+#ifndef CONFIG_KTSAN
 	WRITE_ONCE(v->counter, i);
+#else
+	ktsan_atomic64_store((void *)v, i, ktsan_memory_order_relaxed);
+#endif
 }
 
 /**
@@ -43,9 +52,13 @@ static inline void arch_atomic64_set(atomic64_t *v, long i)
  */
 static __always_inline void arch_atomic64_add(long i, atomic64_t *v)
 {
+#ifndef CONFIG_KTSAN
 	asm volatile(LOCK_PREFIX "addq %1,%0"
 		     : "=m" (v->counter)
 		     : "er" (i), "m" (v->counter));
+#else
+	ktsan_atomic64_fetch_add((void *)v, i, ktsan_memory_order_relaxed);
+#endif
 }
 
 /**
@@ -57,9 +70,13 @@ static __always_inline void arch_atomic64_add(long i, atomic64_t *v)
  */
 static inline void arch_atomic64_sub(long i, atomic64_t *v)
 {
+#ifndef CONFIG_KTSAN
 	asm volatile(LOCK_PREFIX "subq %1,%0"
 		     : "=m" (v->counter)
 		     : "er" (i), "m" (v->counter));
+#else
+	ktsan_atomic64_fetch_add((void *)v, -i, ktsan_memory_order_relaxed);
+#endif
 }
 
 /**
@@ -73,7 +90,12 @@ static inline void arch_atomic64_sub(long i, atomic64_t *v)
  */
 static inline bool arch_atomic64_sub_and_test(long i, atomic64_t *v)
 {
+#ifndef CONFIG_KTSAN
 	return GEN_BINARY_RMWcc(LOCK_PREFIX "subq", v->counter, e, "er", i);
+#else
+	return (ktsan_atomic64_fetch_add((void *)v, -i,
+			ktsan_memory_order_acq_rel) - i) == 0;
+#endif
 }
 #define arch_atomic64_sub_and_test arch_atomic64_sub_and_test
 
@@ -85,9 +107,13 @@ static inline bool arch_atomic64_sub_and_test(long i, atomic64_t *v)
  */
 static __always_inline void arch_atomic64_inc(atomic64_t *v)
 {
+#ifndef CONFIG_KTSAN
 	asm volatile(LOCK_PREFIX "incq %0"
 		     : "=m" (v->counter)
 		     : "m" (v->counter));
+#else
+	ktsan_atomic64_fetch_add((void *)v, 1, ktsan_memory_order_relaxed);
+#endif
 }
 #define arch_atomic64_inc arch_atomic64_inc
 
@@ -99,9 +125,13 @@ static __always_inline void arch_atomic64_inc(atomic64_t *v)
  */
 static __always_inline void arch_atomic64_dec(atomic64_t *v)
 {
+#ifndef CONFIG_KTSAN
 	asm volatile(LOCK_PREFIX "decq %0"
 		     : "=m" (v->counter)
 		     : "m" (v->counter));
+#else
+	ktsan_atomic64_fetch_add((void *)v, -1, ktsan_memory_order_relaxed);
+#endif
 }
 #define arch_atomic64_dec arch_atomic64_dec
 
@@ -115,7 +145,12 @@ static __always_inline void arch_atomic64_dec(atomic64_t *v)
  */
 static inline bool arch_atomic64_dec_and_test(atomic64_t *v)
 {
+#ifndef CONFIG_KTSAN
 	return GEN_UNARY_RMWcc(LOCK_PREFIX "decq", v->counter, e);
+#else
+	return (ktsan_atomic64_fetch_add((void *)v, -1,
+			ktsan_memory_order_acq_rel) - 1) == 0;
+#endif
 }
 #define arch_atomic64_dec_and_test arch_atomic64_dec_and_test
 
@@ -129,7 +164,12 @@ static inline bool arch_atomic64_dec_and_test(atomic64_t *v)
  */
 static inline bool arch_atomic64_inc_and_test(atomic64_t *v)
 {
+#ifndef CONFIG_KTSAN
 	return GEN_UNARY_RMWcc(LOCK_PREFIX "incq", v->counter, e);
+#else
+	return (ktsan_atomic64_fetch_add((void *)v, 1,
+			ktsan_memory_order_acq_rel) + 1) == 0;
+#endif
 }
 #define arch_atomic64_inc_and_test arch_atomic64_inc_and_test
 
@@ -144,7 +184,12 @@ static inline bool arch_atomic64_inc_and_test(atomic64_t *v)
  */
 static inline bool arch_atomic64_add_negative(long i, atomic64_t *v)
 {
+#ifndef CONFIG_KTSAN
 	return GEN_BINARY_RMWcc(LOCK_PREFIX "addq", v->counter, s, "er", i);
+#else
+	return ((long)ktsan_atomic64_fetch_add((void *)v, i,
+			ktsan_memory_order_acq_rel) + i) < 0;
+#endif
 }
 #define arch_atomic64_add_negative arch_atomic64_add_negative
 
@@ -157,12 +202,22 @@ static inline bool arch_atomic64_add_negative(long i, atomic64_t *v)
  */
 static __always_inline long arch_atomic64_add_return(long i, atomic64_t *v)
 {
+#ifndef CONFIG_KTSAN
 	return i + xadd(&v->counter, i);
+#else
+	return (ktsan_atomic64_fetch_add((void *)v, i,
+			ktsan_memory_order_acq_rel) + i);
+#endif
 }
 
 static inline long arch_atomic64_sub_return(long i, atomic64_t *v)
 {
+#ifndef CONFIG_KTSAN
 	return arch_atomic64_add_return(-i, v);
+#else
+	return (ktsan_atomic64_fetch_add((void *)v, -i,
+			ktsan_memory_order_acq_rel) - i);
+#endif
 }
 
 static inline long arch_atomic64_fetch_add(long i, atomic64_t *v)
@@ -177,7 +232,12 @@ static inline long arch_atomic64_fetch_sub(long i, atomic64_t *v)
 
 static inline long arch_atomic64_cmpxchg(atomic64_t *v, long old, long new)
 {
+#ifndef CONFIG_KTSAN
 	return arch_cmpxchg(&v->counter, old, new);
+#else
+	return ktsan_atomic64_compare_exchange((void *)v, old, new,
+			ktsan_memory_order_acq_rel);
+#endif
 }
 
 #define arch_atomic64_try_cmpxchg arch_atomic64_try_cmpxchg
@@ -188,7 +248,12 @@ static __always_inline bool arch_atomic64_try_cmpxchg(atomic64_t *v, s64 *old, l
 
 static inline long arch_atomic64_xchg(atomic64_t *v, long new)
 {
+#ifndef CONFIG_KTSAN
 	return arch_xchg(&v->counter, new);
+#else
+	return ktsan_atomic64_exchange((void *)v, new,
+			ktsan_memory_order_acq_rel);
+#endif
 }
 
 static inline void arch_atomic64_and(long i, atomic64_t *v)
diff --git a/arch/x86/include/asm/barrier.h b/arch/x86/include/asm/barrier.h
index 14de0432d288..6c2973fc1921 100644
--- a/arch/x86/include/asm/barrier.h
+++ b/arch/x86/include/asm/barrier.h
@@ -5,6 +5,8 @@
 #include <asm/alternative.h>
 #include <asm/nops.h>
 
+#include <linux/ktsan.h>
+
 /*
  * Force strict CPU ordering.
  * And yes, this might be required on UP too when we're talking
@@ -19,9 +21,15 @@
 #define wmb() asm volatile(ALTERNATIVE("lock; addl $0,-4(%%esp)", "sfence", \
 				       X86_FEATURE_XMM2) ::: "memory", "cc")
 #else
+#ifndef CONFIG_KTSAN
 #define mb() 	asm volatile("mfence":::"memory")
 #define rmb()	asm volatile("lfence":::"memory")
 #define wmb()	asm volatile("sfence" ::: "memory")
+#else /* CONFIG_KTSAN */
+#define mb()	ktsan_thread_fence(ktsan_memory_order_acq_rel)
+#define rmb()	ktsan_thread_fence(ktsan_memory_order_acquire)
+#define wmb()	ktsan_thread_fence(ktsan_memory_order_release)
+#endif
 #endif
 
 /**
@@ -55,6 +63,7 @@ static inline unsigned long array_index_mask_nospec(unsigned long index,
 #define dma_rmb()	barrier()
 #define dma_wmb()	barrier()
 
+#ifndef CONFIG_KTSAN
 #ifdef CONFIG_X86_32
 #define __smp_mb()	asm volatile("lock; addl $0,-4(%%esp)" ::: "memory", "cc")
 #else
@@ -62,8 +71,14 @@ static inline unsigned long array_index_mask_nospec(unsigned long index,
 #endif
 #define __smp_rmb()	dma_rmb()
 #define __smp_wmb()	barrier()
+#else /* CONFIG_KTSAN */
+#define __smp_mb()	ktsan_thread_fence(ktsan_memory_order_acq_rel)
+#define __smp_rmb()	ktsan_thread_fence(ktsan_memory_order_acquire)
+#define __smp_wmb()	ktsan_thread_fence(ktsan_memory_order_release)
+#endif
 #define __smp_store_mb(var, value) do { (void)xchg(&var, value); } while (0)
 
+#ifndef CONFIG_KTSAN
 #define __smp_store_release(p, v)					\
 do {									\
 	compiletime_assert_atomic_type(*p);				\
@@ -78,10 +93,52 @@ do {									\
 	barrier();							\
 	___p1;								\
 })
+#else /* CONFIG_KTSAN */
+
+#define __smp_store_release(p, v)                                                \
+do {                                                                   \
+	typeof(p) ___p1 = (p);                                          \
+	typeof(v) ___v1 = (v);                                          \
+	                                                               \
+	compiletime_assert_atomic_type(*___p1);                         \
+	                                                               \
+	switch (sizeof(*___p1)) {                                       \
+	case 1: ktsan_atomic8_store((void *)___p1, *((u8 *)&___v1), ktsan_memory_order_release); break; \
+	case 2: ktsan_atomic16_store((void *)___p1, *((u16 *)&___v1), ktsan_memory_order_release); break;       \
+	case 4: ktsan_atomic32_store((void *)___p1, *((u32 *)&___v1), ktsan_memory_order_release); break;       \
+	case 8: ktsan_atomic64_store((void *)___p1, *((u64 *)&___v1), ktsan_memory_order_release); break;       \
+	default: BUG(); break;                                          \
+	}                                                               \
+} while (0)
 
+#define __smp_load_acquire(p)                                            \
+({                                                                     \
+	typeof(p) ___p1 = (p);                                          \
+	typeof(*p) ___r;                                                \
+	                                                               \
+	compiletime_assert_atomic_type(*___p1);                         \
+	                                                               \
+	switch (sizeof(*___p1)) {                                       \
+	case 1: *(u8 *)&___r = ktsan_atomic8_load((const void *)___p1, ktsan_memory_order_acquire); break;    \
+	case 2: *(u16 *)&___r = ktsan_atomic16_load((const void *)___p1, ktsan_memory_order_acquire); break;  \
+	case 4: *(u32 *)&___r = ktsan_atomic32_load((const void *)___p1, ktsan_memory_order_acquire); break;  \
+	case 8: *(u64 *)&___r = ktsan_atomic64_load((const void *)___p1, ktsan_memory_order_acquire); break;  \
+	default: BUG(); break;                                          \
+	}                                                               \
+	                                                               \
+	___r;                                                           \
+})
+
+#endif /* CONFIG_KTSAN */
+
+#ifndef CONFIG_KTSAN
 /* Atomic operations are already serializing on x86 */
 #define __smp_mb__before_atomic()	barrier()
 #define __smp_mb__after_atomic()	barrier()
+#else /* CONFIG_KTSAN */
+#define __smp_mb__before_atomic()	ktsan_thread_fence(ktsan_memory_order_release)
+#define __smp_mb__after_atomic()	ktsan_thread_fence(ktsan_memory_order_acquire)
+#endif
 
 #include <asm-generic/barrier.h>
 
diff --git a/arch/x86/include/asm/bitops.h b/arch/x86/include/asm/bitops.h
index 8e790ec219a5..0f4b484aa138 100644
--- a/arch/x86/include/asm/bitops.h
+++ b/arch/x86/include/asm/bitops.h
@@ -14,6 +14,7 @@
 #endif
 
 #include <linux/compiler.h>
+#include <linux/ktsan.h>
 #include <asm/alternative.h>
 #include <asm/rmwcc.h>
 #include <asm/barrier.h>
@@ -67,6 +68,7 @@
 static __always_inline void
 set_bit(long nr, volatile unsigned long *addr)
 {
+#ifndef CONFIG_KTSAN
 	if (IS_IMMEDIATE(nr)) {
 		asm volatile(LOCK_PREFIX "orb %1,%0"
 			: CONST_MASK_ADDR(nr, addr)
@@ -76,6 +78,9 @@ set_bit(long nr, volatile unsigned long *addr)
 		asm volatile(LOCK_PREFIX __ASM_SIZE(bts) " %1,%0"
 			: : RLONG_ADDR(addr), "Ir" (nr) : "memory");
 	}
+#else /* CONFIG_KTSAN */
+	ktsan_atomic_set_bit((void *)addr, nr, ktsan_memory_order_relaxed);
+#endif /* CONFIG_KTSAN */
 }
 
 /**
@@ -105,6 +110,7 @@ static __always_inline void __set_bit(long nr, volatile unsigned long *addr)
 static __always_inline void
 clear_bit(long nr, volatile unsigned long *addr)
 {
+#ifndef CONFIG_KTSAN
 	if (IS_IMMEDIATE(nr)) {
 		asm volatile(LOCK_PREFIX "andb %1,%0"
 			: CONST_MASK_ADDR(nr, addr)
@@ -113,6 +119,9 @@ clear_bit(long nr, volatile unsigned long *addr)
 		asm volatile(LOCK_PREFIX __ASM_SIZE(btr) " %1,%0"
 			: : RLONG_ADDR(addr), "Ir" (nr) : "memory");
 	}
+#else /* CONFIG_KTSAN */
+	ktsan_atomic_clear_bit((void *)addr, nr, ktsan_memory_order_relaxed);
+#endif /* CONFIG_KTSAN */
 }
 
 /*
@@ -125,8 +134,12 @@ clear_bit(long nr, volatile unsigned long *addr)
  */
 static __always_inline void clear_bit_unlock(long nr, volatile unsigned long *addr)
 {
+#ifndef CONFIG_KTSAN
 	barrier();
 	clear_bit(nr, addr);
+#else /* CONFIG_KTSAN */
+	ktsan_atomic_clear_bit((void *)addr, nr, ktsan_memory_order_release);
+#endif /* CONFIG_KTSAN */
 }
 
 static __always_inline void __clear_bit(long nr, volatile unsigned long *addr)
@@ -158,7 +171,12 @@ static __always_inline bool clear_bit_unlock_is_negative_byte(long nr, volatile
  */
 static __always_inline void __clear_bit_unlock(long nr, volatile unsigned long *addr)
 {
+#ifndef CONFIG_KTSAN
 	__clear_bit(nr, addr);
+#else /* CONFIG_KTSAN */
+	ktsan_atomic64_store((void *)addr, *(unsigned long *)addr & ~(1 << nr),
+			ktsan_memory_order_release);
+#endif /* CONFIG_KTSAN */
 }
 
 /**
@@ -186,6 +204,7 @@ static __always_inline void __change_bit(long nr, volatile unsigned long *addr)
  */
 static __always_inline void change_bit(long nr, volatile unsigned long *addr)
 {
+#ifndef CONFIG_KTSAN
 	if (IS_IMMEDIATE(nr)) {
 		asm volatile(LOCK_PREFIX "xorb %1,%0"
 			: CONST_MASK_ADDR(nr, addr)
@@ -194,6 +213,9 @@ static __always_inline void change_bit(long nr, volatile unsigned long *addr)
 		asm volatile(LOCK_PREFIX __ASM_SIZE(btc) " %1,%0"
 			: : RLONG_ADDR(addr), "Ir" (nr) : "memory");
 	}
+#else /* CONFIG_KTSAN */
+	ktsan_atomic_change_bit((void *)addr, nr, ktsan_memory_order_relaxed);
+#endif /* CONFIG_KTSAN */
 }
 
 /**
@@ -206,7 +228,12 @@ static __always_inline void change_bit(long nr, volatile unsigned long *addr)
  */
 static __always_inline bool test_and_set_bit(long nr, volatile unsigned long *addr)
 {
+#ifndef CONFIG_KTSAN
 	return GEN_BINARY_RMWcc(LOCK_PREFIX __ASM_SIZE(bts), *addr, c, "Ir", nr);
+#else /* CONFIG_KTSAN */
+	return ktsan_atomic_fetch_set_bit((void *)addr, nr,
+			ktsan_memory_order_acq_rel);
+#endif /* CONFIG_KTSAN */
 }
 
 /**
@@ -219,7 +246,12 @@ static __always_inline bool test_and_set_bit(long nr, volatile unsigned long *ad
 static __always_inline bool
 test_and_set_bit_lock(long nr, volatile unsigned long *addr)
 {
+#ifndef CONFIG_KTSAN
 	return test_and_set_bit(nr, addr);
+#else /* CONFIG_KTSAN */
+	return ktsan_atomic_fetch_set_bit((void *)addr, nr,
+			ktsan_memory_order_acquire);
+#endif /* CONFIG_KTSAN */
 }
 
 /**
@@ -252,7 +284,12 @@ static __always_inline bool __test_and_set_bit(long nr, volatile unsigned long *
  */
 static __always_inline bool test_and_clear_bit(long nr, volatile unsigned long *addr)
 {
+#ifndef CONFIG_KTSAN
 	return GEN_BINARY_RMWcc(LOCK_PREFIX __ASM_SIZE(btr), *addr, c, "Ir", nr);
+#else /* CONFIG_KTSAN */
+	return ktsan_atomic_fetch_clear_bit((void *)addr, nr,
+			ktsan_memory_order_acq_rel);
+#endif /* CONFIG_KTSAN */
 }
 
 /**
@@ -305,7 +342,12 @@ static __always_inline bool __test_and_change_bit(long nr, volatile unsigned lon
  */
 static __always_inline bool test_and_change_bit(long nr, volatile unsigned long *addr)
 {
+#ifndef CONFIG_KTSAN
 	return GEN_BINARY_RMWcc(LOCK_PREFIX __ASM_SIZE(btc), *addr, c, "Ir", nr);
+#else /* CONFIG_KTSAN */
+	return ktsan_atomic_fetch_change_bit((void *)addr, nr,
+			ktsan_memory_order_acq_rel);
+#endif /* CONFIG_KTSAN */
 }
 
 static __always_inline bool constant_test_bit(long nr, const volatile unsigned long *addr)
diff --git a/arch/x86/include/asm/cmpxchg.h b/arch/x86/include/asm/cmpxchg.h
index a8bfac131256..80da794431bd 100644
--- a/arch/x86/include/asm/cmpxchg.h
+++ b/arch/x86/include/asm/cmpxchg.h
@@ -2,7 +2,9 @@
 #ifndef ASM_X86_CMPXCHG_H
 #define ASM_X86_CMPXCHG_H
 
+#include <linux/bug.h>
 #include <linux/compiler.h>
+#include <linux/ktsan.h>
 #include <asm/cpufeatures.h>
 #include <asm/alternative.h> /* Provides LOCK_PREFIX */
 
@@ -69,6 +71,7 @@ extern void __add_wrong_size(void)
 		__ret;							\
 	})
 
+#ifndef CONFIG_KTSAN
 /*
  * Note: no "lock" prefix even on SMP: xchg always implies lock anyway.
  * Since this is generally used to protect other memory information, we
@@ -76,6 +79,43 @@ extern void __add_wrong_size(void)
  * information around.
  */
 #define arch_xchg(ptr, v)	__xchg_op((ptr), (v), xchg, "")
+#else /* CONFIG_KTSAN */
+#define arch_xchg(ptr, v)							\
+({									\
+	__typeof__(*(ptr)) ret;						\
+	u64 ret64;							\
+	u32 ret32;							\
+	u16 ret16;							\
+	u8 ret8;							\
+									\
+	__typeof__(*(ptr)) lv = (v);					\
+									\
+	BUILD_BUG_ON(sizeof(*(ptr)) != 8 &&				\
+		     sizeof(*(ptr)) != 4 &&				\
+		     sizeof(*(ptr)) != 2 &&				\
+		     sizeof(*(ptr)) != 1);				\
+									\
+	if (sizeof(*(ptr)) == 8) {					\
+		ret64 = ktsan_atomic64_exchange((void *)(ptr),		\
+			*((u64 *)(&lv)), ktsan_memory_order_acq_rel);	\
+		ret = *((__typeof__(ptr))(&ret64));			\
+	} else if (sizeof(*(ptr)) == 4) {				\
+		ret32 = ktsan_atomic32_exchange((void *)(ptr),		\
+			*((u32 *)(&lv)), ktsan_memory_order_acq_rel);	\
+		ret = *((__typeof__(ptr))(&ret32));			\
+	} else if (sizeof(*(ptr)) == 2) {				\
+		ret16 = ktsan_atomic16_exchange((void *)(ptr),		\
+			*((u16 *)(&lv)), ktsan_memory_order_acq_rel);	\
+		ret = *((__typeof__(ptr))(&ret16));			\
+	} else if (sizeof(*(ptr)) == 1) {				\
+		ret8 = ktsan_atomic8_exchange((void *)(ptr),		\
+			*((u8 *)(&lv)), ktsan_memory_order_acq_rel);	\
+		ret = *((__typeof__(ptr))(&ret8));			\
+	}								\
+									\
+	ret;								\
+})
+#endif /* CONFIG_KTSAN */
 
 /*
  * Atomic compare and exchange.  Compare OLD with MEM, if identical,
@@ -145,12 +185,57 @@ extern void __add_wrong_size(void)
 # include <asm/cmpxchg_64.h>
 #endif
 
+#ifndef CONFIG_KTSAN
 #define arch_cmpxchg(ptr, old, new)					\
 	__cmpxchg(ptr, old, new, sizeof(*(ptr)))
+#else /* CONFIG_KTSAN */
+#define arch_cmpxchg(ptr, old, new)						\
+({									\
+	__typeof__(*(ptr)) _ret;						\
+	u64 ret64;							\
+	u32 ret32;							\
+	u16 ret16;							\
+	u8 ret8;							\
+									\
+	__typeof__(*(ptr)) lo = (old);					\
+	__typeof__(*(ptr)) ln = (new);					\
+									\
+	BUILD_BUG_ON(sizeof(*(ptr)) != 8 &&				\
+		     sizeof(*(ptr)) != 4 &&				\
+		     sizeof(*(ptr)) != 2 &&				\
+		     sizeof(*(ptr)) != 1);				\
+									\
+	if (sizeof(*(ptr)) == 8) {					\
+		ret64 = ktsan_atomic64_compare_exchange((void *)(ptr),	\
+				*((u64 *)(&lo)), *((u64 *)(&ln)),	\
+				ktsan_memory_order_acq_rel);		\
+		_ret = *((__typeof__(ptr))(&ret64));			\
+	} else if (sizeof(*(ptr)) == 4) {				\
+		ret32 = ktsan_atomic32_compare_exchange((void *)(ptr),	\
+				*((u32 *)(&lo)), *((u32 *)(&ln)),	\
+				ktsan_memory_order_acq_rel);		\
+		_ret = *((__typeof__(ptr))(&ret32));			\
+	} else if (sizeof(*(ptr)) == 2) {				\
+		ret16 = ktsan_atomic16_compare_exchange((void *)(ptr),	\
+				*((u16 *)(&lo)), *((u16 *)(&ln)),	\
+				ktsan_memory_order_acq_rel);		\
+		_ret = *((__typeof__(ptr))(&ret16));			\
+	} else if (sizeof(*(ptr)) == 1) {					\
+		ret8 = ktsan_atomic8_compare_exchange((void *)(ptr),	\
+				*((u8 *)(&lo)), *((u8 *)(&ln)),		\
+				ktsan_memory_order_acq_rel);		\
+		_ret = *((__typeof__(ptr))(&ret8));			\
+	}								\
+									\
+	_ret;								\
+})
+#endif /* CONFIG_KTSAN */
 
+/* FIXME(xairy): ktsan? */
 #define arch_sync_cmpxchg(ptr, old, new)				\
 	__sync_cmpxchg(ptr, old, new, sizeof(*(ptr)))
 
+/* FIXME(xairy): ktsan? */
 #define arch_cmpxchg_local(ptr, old, new)				\
 	__cmpxchg_local(ptr, old, new, sizeof(*(ptr)))
 
@@ -230,8 +315,48 @@ extern void __add_wrong_size(void)
  *
  * xadd() is locked when multiple CPUs are online
  */
+
 #define __xadd(ptr, inc, lock)	__xchg_op((ptr), (inc), xadd, lock)
+
+#ifndef CONFIG_KTSAN
 #define xadd(ptr, inc)		__xadd((ptr), (inc), LOCK_PREFIX)
+#else /* CONFIG_KTSAN */
+#define xadd(ptr, inc)							\
+({									\
+	__typeof__(*(ptr)) ret;						\
+	u64 ret64;							\
+	u32 ret32;							\
+	u16 ret16;							\
+	u8 ret8;							\
+									\
+	__typeof__(*(ptr)) li = (inc);					\
+									\
+	BUILD_BUG_ON(sizeof(*(ptr)) != 8 &&				\
+		     sizeof(*(ptr)) != 4 &&				\
+		     sizeof(*(ptr)) != 2 &&				\
+		     sizeof(*(ptr)) != 1);				\
+									\
+	if (sizeof(*(ptr)) == 8) {					\
+		ret64 = ktsan_atomic64_fetch_add((void *)(ptr),		\
+			*((u64 *)(&li)), ktsan_memory_order_acq_rel);	\
+		ret = *((__typeof__(ptr))(&ret64));			\
+	} else if (sizeof(*(ptr)) == 4) {				\
+		ret32 = ktsan_atomic32_fetch_add((void *)(ptr),		\
+			*((u32 *)(&li)), ktsan_memory_order_acq_rel);	\
+		ret = *((__typeof__(ptr))(&ret32));			\
+	} else if (sizeof(*(ptr)) == 2) {				\
+		ret16 = ktsan_atomic16_fetch_add((void *)(ptr),		\
+			*((u16 *)(&li)), ktsan_memory_order_acq_rel);	\
+		ret = *((__typeof__(ptr))(&ret16));			\
+	} else if (sizeof(*(ptr)) == 1) {				\
+		ret8 = ktsan_atomic8_fetch_add((void *)(ptr),		\
+			*((u8 *)(&li)), ktsan_memory_order_acq_rel);	\
+		ret = *((__typeof__(ptr))(&ret8));			\
+	}								\
+									\
+	ret;								\
+})
+#endif /* CONFIG_KTSAN */
 
 #define __cmpxchg_double(pfx, p1, p2, o1, o2, n1, n2)			\
 ({									\
diff --git a/arch/x86/include/asm/ktsan.h b/arch/x86/include/asm/ktsan.h
new file mode 100644
index 000000000000..edc28aab2783
--- /dev/null
+++ b/arch/x86/include/asm/ktsan.h
@@ -0,0 +1,58 @@
+#ifndef _ASM_X86_KTSAN_H
+#define _ASM_X86_KTSAN_H
+
+#ifdef CONFIG_KTSAN
+
+#define KTSAN_PUSH_REGS				\
+	pushq	%rax;				\
+	pushq	%rcx;				\
+	pushq	%rdx;				\
+	pushq	%rdi;				\
+	pushq	%rsi;				\
+	pushq	%r8;				\
+	pushq	%r9;				\
+	pushq	%r10;				\
+	pushq	%r11;				\
+/**/
+
+#define KTSAN_POP_REGS				\
+	popq	%r11;				\
+	popq	%r10;				\
+	popq	%r9;				\
+	popq	%r8;				\
+	popq	%rsi;				\
+	popq	%rdi;				\
+	popq	%rdx;				\
+	popq	%rcx;				\
+	popq	%rax;				\
+/**/
+
+#define KTSAN_INTERRUPT_ENTER			\
+	KTSAN_PUSH_REGS				\
+	call	ktsan_interrupt_enter;		\
+	KTSAN_POP_REGS				\
+/**/
+
+#define KTSAN_INTERRUPT_EXIT			\
+	call	ktsan_interrupt_exit;		\
+/**/
+
+#define KTSAN_SYSCALL_ENTER			\
+	KTSAN_PUSH_REGS				\
+	call	ktsan_syscall_enter;		\
+	KTSAN_POP_REGS				\
+/**/
+
+#define KTSAN_SYSCALL_EXIT			\
+	call	ktsan_syscall_exit;		\
+/**/
+
+#else /* ifdef CONFIG_KTSAN */
+
+#define KTSAN_INTERRUPT_ENTER
+#define KTSAN_INTERRUPT_EXIT
+#define KTSAN_SYSCALL_ENTER
+#define KTSAN_SYSCALL_EXIT
+
+#endif /* ifdef CONFIG_KTSAN */
+#endif /* ifndef _ASM_X86_KTSAN_H */
diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h
index 0bb566315621..66af0a48995f 100644
--- a/arch/x86/include/asm/pgtable_64.h
+++ b/arch/x86/include/asm/pgtable_64.h
@@ -15,6 +15,7 @@
 #include <linux/bitops.h>
 #include <linux/threads.h>
 #include <asm/fixmap.h>
+#include <linux/ktsan.h>
 
 extern p4d_t level4_kernel_pgt[512];
 extern p4d_t level4_ident_pgt[512];
@@ -85,7 +86,15 @@ static inline void native_pmd_clear(pmd_t *pmd)
 static inline pte_t native_ptep_get_and_clear(pte_t *xp)
 {
 #ifdef CONFIG_SMP
-	return native_make_pte(xchg(&xp->pte, 0));
+	/* Make KTSAN ignore xchg.
+	   This significantly reduces the number of sync objects.
+	   This might introduce false positives, but none were observed. */
+	pte_t rv;
+
+	ktsan_thr_event_disable();
+	rv = native_make_pte(xchg(&xp->pte, 0));
+	ktsan_thr_event_enable();
+	return rv;
 #else
 	/* native_local_ptep_get_and_clear,
 	   but duplicated because of cyclic dependency */
diff --git a/arch/x86/include/asm/preempt.h b/arch/x86/include/asm/preempt.h
index 99a7fa9ab0a3..63e2f80a69c0 100644
--- a/arch/x86/include/asm/preempt.h
+++ b/arch/x86/include/asm/preempt.h
@@ -5,6 +5,7 @@
 #include <asm/rmwcc.h>
 #include <asm/percpu.h>
 #include <linux/thread_info.h>
+#include <linux/ktsan.h>
 
 DECLARE_PER_CPU(int, __preempt_count);
 
@@ -77,10 +78,12 @@ static __always_inline bool test_preempt_need_resched(void)
 static __always_inline void __preempt_count_add(int val)
 {
 	raw_cpu_add_4(__preempt_count, val);
+	ktsan_preempt_add(val);
 }
 
 static __always_inline void __preempt_count_sub(int val)
 {
+	ktsan_preempt_sub(val);
 	raw_cpu_add_4(__preempt_count, -val);
 }
 
diff --git a/arch/x86/kernel/Makefile b/arch/x86/kernel/Makefile
index ce1b5cc360a2..07272d1af4fc 100644
--- a/arch/x86/kernel/Makefile
+++ b/arch/x86/kernel/Makefile
@@ -28,6 +28,8 @@ KASAN_SANITIZE_dumpstack_$(BITS).o			:= n
 KASAN_SANITIZE_stacktrace.o				:= n
 KASAN_SANITIZE_paravirt.o				:= n
 
+KTSAN_SANITIZE_nmi.o 					:= n
+
 OBJECT_FILES_NON_STANDARD_relocate_kernel_$(BITS).o	:= y
 OBJECT_FILES_NON_STANDARD_test_nx.o			:= y
 OBJECT_FILES_NON_STANDARD_paravirt_patch_$(BITS).o	:= y
diff --git a/arch/x86/kernel/cpu/Makefile b/arch/x86/kernel/cpu/Makefile
index 5102bf7c8192..1c0f058f7c87 100644
--- a/arch/x86/kernel/cpu/Makefile
+++ b/arch/x86/kernel/cpu/Makefile
@@ -3,6 +3,9 @@
 # Makefile for x86-compatible CPU details, features and quirks
 #
 
+KTSAN_SANITIZE_common.o = n
+KTSAN_SANITIZE_perf_event.o = n
+
 # Don't trace early stages of a secondary CPU boot
 ifdef CONFIG_FUNCTION_TRACER
 CFLAGS_REMOVE_common.o = -pg
diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 75fea0d48c0e..939bb0e5b232 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -26,6 +26,7 @@
 #include <linux/elf-randomize.h>
 #include <trace/events/power.h>
 #include <linux/hw_breakpoint.h>
+#include <linux/ktsan.h>
 #include <asm/cpu.h>
 #include <asm/apic.h>
 #include <asm/syscalls.h>
@@ -96,7 +97,10 @@ EXPORT_PER_CPU_SYMBOL_GPL(__tss_limit_invalid);
  */
 int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
+	/* src is concurrently mutated which causes data races */
+	ktsan_thr_event_disable();
 	memcpy(dst, src, arch_task_struct_size);
+	ktsan_thr_event_enable();
 #ifdef CONFIG_VM86
 	dst->thread.vm86 = NULL;
 #endif
diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c
index 362dd8953f48..f3c85a050f55 100644
--- a/arch/x86/kernel/smpboot.c
+++ b/arch/x86/kernel/smpboot.c
@@ -55,6 +55,7 @@
 #include <linux/gfp.h>
 #include <linux/cpuidle.h>
 #include <linux/numa.h>
+#include <linux/ktsan.h>
 
 #include <asm/acpi.h>
 #include <asm/desc.h>
@@ -225,6 +226,8 @@ static void notrace start_secondary(void *unused)
 #endif
 	load_current_idt();
 	cpu_init();
+	ktsan_cpu_start();
+	ktsan_task_start();
 	x86_cpuinit.early_percpu_clock_init();
 	preempt_disable();
 	smp_callin();
diff --git a/arch/x86/realmode/Makefile b/arch/x86/realmode/Makefile
index 682c895753d9..f84b2639fdda 100644
--- a/arch/x86/realmode/Makefile
+++ b/arch/x86/realmode/Makefile
@@ -7,6 +7,7 @@
 #
 #
 KASAN_SANITIZE			:= n
+KTSAN_SANITIZE			:= n
 OBJECT_FILES_NON_STANDARD	:= y
 
 subdir- := rm
diff --git a/arch/x86/realmode/rm/Makefile b/arch/x86/realmode/rm/Makefile
index f60501a384f9..3cf9823385b5 100644
--- a/arch/x86/realmode/rm/Makefile
+++ b/arch/x86/realmode/rm/Makefile
@@ -12,6 +12,8 @@ OBJECT_FILES_NON_STANDARD	:= y
 # Prevents link failures: __sanitizer_cov_trace_pc() is not linked in.
 KCOV_INSTRUMENT		:= n
 
+KTSAN_SANITIZE := n
+
 always := realmode.bin realmode.relocs
 
 wakeup-objs	:= wakeup_asm.o wakemain.o video-mode.o
diff --git a/drivers/dma-buf/reservation.c b/drivers/dma-buf/reservation.c
index 4d32e2c67862..4966fe08f432 100644
--- a/drivers/dma-buf/reservation.c
+++ b/drivers/dma-buf/reservation.c
@@ -368,10 +368,12 @@ int reservation_object_get_fences_rcu(struct reservation_object *obj,
 				nshared = krealloc(shared, sz, GFP_KERNEL);
 				if (nshared) {
 					shared = nshared;
+					read_seqcount_cancel(&obj->seq);
 					continue;
 				}
 
 				ret = -ENOMEM;
+				read_seqcount_cancel(&obj->seq);
 				break;
 			}
 			shared = nshared;
diff --git a/drivers/firmware/efi/libstub/Makefile b/drivers/firmware/efi/libstub/Makefile
index 0460c7581220..2e9a291bbc99 100644
--- a/drivers/firmware/efi/libstub/Makefile
+++ b/drivers/firmware/efi/libstub/Makefile
@@ -32,6 +32,7 @@ KBUILD_CFLAGS			:= $(cflags-y) -DDISABLE_BRANCH_PROFILING \
 
 GCOV_PROFILE			:= n
 KASAN_SANITIZE			:= n
+KTSAN_SANITIZE			:= n
 UBSAN_SANITIZE			:= n
 OBJECT_FILES_NON_STANDARD	:= y
 
diff --git a/drivers/net/ethernet/intel/e1000/e1000_main.c b/drivers/net/ethernet/intel/e1000/e1000_main.c
index 551de8c2fef2..7f4f3a0a55d7 100644
--- a/drivers/net/ethernet/intel/e1000/e1000_main.c
+++ b/drivers/net/ethernet/intel/e1000/e1000_main.c
@@ -3269,6 +3269,12 @@ static netdev_tx_t e1000_xmit_frame(struct sk_buff *skb,
 
 		if (!netdev_xmit_more() ||
 		    netif_xmit_stopped(netdev_get_tx_queue(netdev, 0))) {
+			/* Teach KTSAN about synchronization through the adapter.
+			 * Here we hand off packets to the adapter,
+			 * e1000_clean_tx_irq will pick them up later and do
+			 * ktsan_sync_acquire(hw).
+			 */
+			ktsan_sync_release(hw);
 			writel(tx_ring->next_to_use, hw->hw_addr + tx_ring->tdt);
 		}
 	} else {
@@ -3838,6 +3844,12 @@ static bool e1000_clean_tx_irq(struct e1000_adapter *adapter,
 	       (count < tx_ring->count)) {
 		bool cleaned = false;
 		dma_rmb();	/* read buffer_info after eop_desc */
+		/* Teach KTSAN about synchronization through the adapter.
+		 * In this case e1000_xmit_frame has wrote next_to_use to
+		 * hardware register and handed off a set of packets to the
+		 * adapter. Here we pick the completed packets up.
+		 */
+		ktsan_sync_acquire(hw);
 		for ( ; !cleaned; count++) {
 			tx_desc = E1000_TX_DESC(*tx_ring, i);
 			buffer_info = &tx_ring->buffer_info[i];
diff --git a/fs/dcache.c b/fs/dcache.c
index c435398f2c81..2710aba2ccbd 100644
--- a/fs/dcache.c
+++ b/fs/dcache.c
@@ -1267,6 +1267,8 @@ static void d_walk(struct dentry *parent, void *data,
 		break;
 	case D_WALK_QUIT:
 	case D_WALK_SKIP:
+		if (!(seq&1))
+			read_seqcount_cancel(&rename_lock.seqcount);
 		goto out_unlock;
 	case D_WALK_NORETRY:
 		retry = false;
@@ -1291,6 +1293,8 @@ static void d_walk(struct dentry *parent, void *data,
 			break;
 		case D_WALK_QUIT:
 			spin_unlock(&dentry->d_lock);
+			if (!(seq&1))
+				read_seqcount_cancel(&rename_lock.seqcount);
 			goto out_unlock;
 		case D_WALK_NORETRY:
 			retry = false;
@@ -1322,7 +1326,7 @@ static void d_walk(struct dentry *parent, void *data,
 		spin_lock(&this_parent->d_lock);
 
 		/* might go back up the wrong parent if we have had a rename. */
-		if (need_seqretry(&rename_lock, seq))
+		if (need_seqretry_check(&rename_lock, seq))
 			goto rename_retry;
 		/* go into the first sibling still alive */
 		do {
@@ -1344,6 +1348,7 @@ static void d_walk(struct dentry *parent, void *data,
 	return;
 
 rename_retry:
+	done_seqretry(&rename_lock, seq);
 	spin_unlock(&this_parent->d_lock);
 	rcu_read_unlock();
 	BUG_ON(seq & 1);
@@ -2195,16 +2200,22 @@ struct dentry *__d_lookup_rcu(const struct dentry *parent,
 		 * we are still guaranteed NUL-termination of ->d_name.name.
 		 */
 		seq = raw_seqcount_begin(&dentry->d_seq);
-		if (dentry->d_parent != parent)
+		if (dentry->d_parent != parent) {
+			read_seqcount_cancel(&dentry->d_seq);
 			continue;
-		if (d_unhashed(dentry))
+		}
+		if (d_unhashed(dentry)) {
+			read_seqcount_cancel(&dentry->d_seq);
 			continue;
+		}
 
 		if (unlikely(parent->d_flags & DCACHE_OP_COMPARE)) {
 			int tlen;
 			const char *tname;
-			if (dentry->d_name.hash != hashlen_hash(hashlen))
+			if (dentry->d_name.hash != hashlen_hash(hashlen)) {
+				read_seqcount_cancel(&dentry->d_seq);
 				continue;
+			}
 			tlen = dentry->d_name.len;
 			tname = dentry->d_name.name;
 			/* we want a consistent (name,len) pair */
@@ -2216,6 +2227,8 @@ struct dentry *__d_lookup_rcu(const struct dentry *parent,
 						    tlen, tname, name) != 0)
 				continue;
 		} else {
+			read_seqcount_cancel(&dentry->d_seq);
+
 			if (dentry->d_name.hash_len != hashlen)
 				continue;
 			if (dentry_cmp(dentry, str, hashlen_len(hashlen)) != 0)
@@ -2246,8 +2259,10 @@ struct dentry *d_lookup(const struct dentry *parent, const struct qstr *name)
 	do {
 		seq = read_seqbegin(&rename_lock);
 		dentry = __d_lookup(parent, name);
-		if (dentry)
+		if (dentry) {
+			read_seqcount_cancel(&rename_lock.seqcount);
 			break;
+		}
 	} while (read_seqretry(&rename_lock, seq));
 	return dentry;
 }
@@ -2467,11 +2482,13 @@ struct dentry *d_alloc_parallel(struct dentry *parent,
 	if (unlikely(dentry)) {
 		if (!lockref_get_not_dead(&dentry->d_lockref)) {
 			rcu_read_unlock();
+			read_seqcancel(&rename_lock);
 			goto retry;
 		}
 		if (read_seqcount_retry(&dentry->d_seq, d_seq)) {
 			rcu_read_unlock();
 			dput(dentry);
+			read_seqcancel(&rename_lock);
 			goto retry;
 		}
 		rcu_read_unlock();
diff --git a/fs/locks.c b/fs/locks.c
index ec1e4a5df629..b721881fa1c2 100644
--- a/fs/locks.c
+++ b/fs/locks.c
@@ -689,8 +689,16 @@ static void locks_delete_global_locks(struct file_lock *fl)
 	 * is done while holding the flc_lock, and new insertions into the list
 	 * also require that it be held.
 	 */
-	if (hlist_unhashed(&fl->fl_link))
+	/*
+	 * This is not safe as other threads modify fl->fl_link.pprev
+	 * concurrently if the node is still in the list.
+	 */
+	ktsan_thr_event_disable();
+	if (hlist_unhashed(&fl->fl_link)) {
+		ktsan_thr_event_enable();
 		return;
+	}
+	ktsan_thr_event_enable();
 
 	fll = per_cpu_ptr(&file_lock_list, fl->fl_link_cpu);
 	spin_lock(&fll->lock);
diff --git a/fs/namei.c b/fs/namei.c
index 20831c2fbb34..17fc21b75f18 100644
--- a/fs/namei.c
+++ b/fs/namei.c
@@ -39,6 +39,7 @@
 #include <linux/bitops.h>
 #include <linux/init_task.h>
 #include <linux/uaccess.h>
+#include <linux/ktsan.h>
 
 #include "internal.h"
 #include "mount.h"
@@ -517,12 +518,22 @@ static void set_nameidata(struct nameidata *p, int dfd, struct filename *name)
 	p->total_link_count = old ? old->total_link_count : 0;
 	p->saved = old;
 	current->nameidata = p;
+
+	/*
+	 * Seqcount manipulation is crazy in this file.
+	 * I've tried to annotate seq-read critical sections, but did fail.
+	 * For now we bulk-disable handling of seqcounts in this region
+	 * and disable handling of memory reads to prevent false positives.
+	 */
+	ktsan_seqcount_ignore_begin();
 }
 
 static void restore_nameidata(void)
 {
 	struct nameidata *now = current->nameidata, *old = now->saved;
 
+	ktsan_seqcount_ignore_end();
+
 	current->nameidata = old;
 	if (old)
 		old->total_link_count = now->total_link_count;
diff --git a/fs/nfs/delegation.c b/fs/nfs/delegation.c
index 0ff3facf81da..92981ad03d78 100644
--- a/fs/nfs/delegation.c
+++ b/fs/nfs/delegation.c
@@ -156,8 +156,12 @@ static int nfs_delegation_claim_opens(struct inode *inode,
 		err = nfs4_open_delegation_recall(ctx, state, stateid, type);
 		if (!err)
 			err = nfs_delegation_claim_locks(state, stateid);
-		if (!err && read_seqcount_retry(&sp->so_reclaim_seqcount, seq))
-			err = -EAGAIN;
+		if (!err) {
+			if (read_seqcount_retry(&sp->so_reclaim_seqcount, seq))
+				err = -EAGAIN;
+		} else {
+			read_seqcount_cancel(&sp->so_reclaim_seqcount);
+		}
 		mutex_unlock(&sp->so_delegreturn_mutex);
 		put_nfs_open_context(ctx);
 		if (err != 0)
diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
index 85b28eb80b11..72fed5686fed 100644
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@ -47,7 +47,11 @@
 #endif
 
 #ifndef read_barrier_depends
+#ifndef CONFIG_KTSAN
 #define read_barrier_depends()		do { } while (0)
+#else /* CONFIG_KTSAN */
+#define read_barrier_depends()		ktsan_thread_fence(ktsan_memory_order_acquire)
+#endif /* CONFIG_KTSAN */
 #endif
 
 #ifndef __smp_mb
diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index 8aaf7cd026b0..2f9e1872e019 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -174,6 +174,10 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 
 #include <uapi/linux/types.h>
 
+#include <linux/ktsan.h>
+
+#ifndef CONFIG_KTSAN
+
 #define __READ_ONCE_SIZE						\
 ({									\
 	switch (size) {							\
@@ -188,6 +192,24 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 	}								\
 })
 
+#else /* CONFIG_KTSAN */
+
+#define __READ_ONCE_SIZE						\
+({									\
+	switch (size) {					\
+	case 1: *(__u8 *)res = ktsan_atomic8_load((void *)p, ktsan_memory_order_relaxed); break;	\
+	case 2: *(__u16 *)res = ktsan_atomic16_load((void *)p, ktsan_memory_order_relaxed); break;	\
+	case 4: *(__u32 *)res = ktsan_atomic32_load((void *)p, ktsan_memory_order_relaxed); break;	\
+	case 8: *(__u64 *)res = ktsan_atomic64_load((void *)p, ktsan_memory_order_relaxed); break;	\
+	default:								\
+		barrier();						\
+		__builtin_memcpy((void *)res, (const void *)p, size);	\
+		barrier();	\
+	}	\
+})
+
+#endif /* CONFIG_KTSAN */
+
 static __always_inline
 void __read_once_size(const volatile void *p, void *res, int size)
 {
@@ -212,6 +234,8 @@ void __read_once_size_nocheck(const volatile void *p, void *res, int size)
 	__READ_ONCE_SIZE;
 }
 
+#ifndef CONFIG_KASAN
+
 static __always_inline void __write_once_size(volatile void *p, void *res, int size)
 {
 	switch (size) {
@@ -226,6 +250,24 @@ static __always_inline void __write_once_size(volatile void *p, void *res, int s
 	}
 }
 
+#else /* CONFIG_KTSAN */
+
+static __always_inline void __write_once_size(volatile void *p, void *res, int size)
+{
+	switch (size) {
+	case 1: ktsan_atomic8_store((void *)p, *(__u8 *)res, ktsan_memory_order_relaxed); break;
+	case 2: ktsan_atomic16_store((void *)p, *(__u16 *)res, ktsan_memory_order_relaxed); break;
+	case 4: ktsan_atomic32_store((void *)p, *(__u32 *)res, ktsan_memory_order_relaxed); break;
+	case 8: ktsan_atomic64_store((void *)p, *(__u64 *)res, ktsan_memory_order_relaxed); break;
+	default:
+		barrier();
+		__builtin_memcpy((void *)p, (const void *)res, size);
+		barrier();
+	}
+}
+
+#endif
+
 /*
  * Prevent the compiler from merging or refetching reads or writes. The
  * compiler is also forbidden from reordering successive instances of
diff --git a/include/linux/cpuset.h b/include/linux/cpuset.h
index 934633a05d20..769e6c6e4868 100644
--- a/include/linux/cpuset.h
+++ b/include/linux/cpuset.h
@@ -147,6 +147,14 @@ static inline bool read_mems_allowed_retry(unsigned int seq)
 	return read_seqcount_retry(&current->mems_allowed_seq, seq);
 }
 
+static inline void read_mems_allowed_cancel(void)
+{
+	if (!static_branch_unlikely(&cpusets_enabled_key))
+		return;
+
+	read_seqcount_cancel(&current->mems_allowed_seq);
+}
+
 static inline void set_mems_allowed(nodemask_t nodemask)
 {
 	unsigned long flags;
@@ -275,6 +283,8 @@ static inline bool read_mems_allowed_retry(unsigned int seq)
 	return false;
 }
 
+static inline void read_mems_allowed_cancel(void) {}
+
 #endif /* !CONFIG_CPUSETS */
 
 #endif /* _LINUX_CPUSET_H */
diff --git a/include/linux/irqflags.h b/include/linux/irqflags.h
index 21619c92c377..d6708f7a02a9 100644
--- a/include/linux/irqflags.h
+++ b/include/linux/irqflags.h
@@ -13,6 +13,7 @@
 #define _LINUX_TRACE_IRQFLAGS_H
 
 #include <linux/typecheck.h>
+#include <linux/ktsan.h>
 #include <asm/irqflags.h>
 
 /* Currently trace_softirqs_on/off is used only by lockdep */
@@ -76,15 +77,25 @@ do {						\
 /*
  * Wrap the arch provided IRQ routines to provide appropriate checks.
  */
-#define raw_local_irq_disable()		arch_local_irq_disable()
-#define raw_local_irq_enable()		arch_local_irq_enable()
+#define raw_local_irq_disable()				\
+	do {						\
+		arch_local_irq_disable();		\
+		ktsan_irq_disable();			\
+	} while (0)
+#define raw_local_irq_enable()				\
+	do {						\
+		ktsan_irq_enable();			\
+		arch_local_irq_enable();		\
+	} while (0)
 #define raw_local_irq_save(flags)			\
 	do {						\
 		typecheck(unsigned long, flags);	\
 		flags = arch_local_irq_save();		\
+		ktsan_irq_save();			\
 	} while (0)
 #define raw_local_irq_restore(flags)			\
 	do {						\
+		ktsan_irq_restore(flags);		\
 		typecheck(unsigned long, flags);	\
 		arch_local_irq_restore(flags);		\
 	} while (0)
diff --git a/include/linux/ktsan.h b/include/linux/ktsan.h
new file mode 100644
index 000000000000..1fa6b6ddf10c
--- /dev/null
+++ b/include/linux/ktsan.h
@@ -0,0 +1,279 @@
+// SPDX-License-Identifier: GPL-2.0
+/* ThreadSanitizer (TSan) is a tool that finds data race bugs. */
+
+#ifndef LINUX_KTSAN_H
+#define LINUX_KTSAN_H
+
+/* We can't include linux/types.h, since it includes linux/compiler.h,
+   which includes linux/ktsan.h. Redeclare some types from linux/types.h. */
+typedef unsigned char u8;
+typedef unsigned short u16;
+typedef unsigned int u32;
+typedef unsigned long long u64;
+typedef _Bool bool;
+typedef unsigned gfp_t;
+struct page;
+
+enum ktsan_memory_order_e {
+	ktsan_memory_order_relaxed,
+	ktsan_memory_order_acquire,
+	ktsan_memory_order_release,
+	ktsan_memory_order_acq_rel
+};
+
+typedef enum ktsan_memory_order_e ktsan_memory_order_t;
+
+enum ktsan_glob_sync_type_e {
+	ktsan_glob_sync_type_rcu_common,
+	ktsan_glob_sync_type_rcu_bh,
+	ktsan_glob_sync_type_rcu_sched,
+	ktsan_glob_sync_type_count,
+};
+
+extern int ktsan_glob_sync[ktsan_glob_sync_type_count];
+
+#ifdef CONFIG_KTSAN
+
+struct kt_task_s;
+
+struct ktsan_task_s {
+	struct kt_task_s *task;
+};
+
+void ktsan_init_early(void);
+void ktsan_init(void);
+
+/* Debugging purposes only. */
+void ktsan_print_diagnostics(void);
+
+void ktsan_cpu_start(void);
+
+void ktsan_task_create(struct ktsan_task_s *new, int pid);
+void ktsan_task_destroy(struct ktsan_task_s *old);
+void ktsan_task_start(void);
+void ktsan_task_stop(void);
+
+void ktsan_thr_event_disable(void);
+void ktsan_thr_event_enable(void);
+void ktsan_thr_report_disable(void);
+void ktsan_thr_report_enable(void);
+
+void ktsan_slab_alloc(void *addr, unsigned long size, unsigned long flags);
+void ktsan_slab_free(void *addr, unsigned long size, unsigned long flags);
+
+void ktsan_sync_acquire(void *addr);
+void ktsan_sync_release(void *addr);
+
+void ktsan_mtx_pre_lock(void *addr, bool write, bool try);
+void ktsan_mtx_post_lock(void *addr, bool write, bool try, bool success);
+void ktsan_mtx_pre_unlock(void *addr, bool write);
+void ktsan_mtx_post_unlock(void *addr, bool write);
+void ktsan_mtx_downgrade(void *addr);
+
+/*
+ * Begin/end of seqcount read critical section.
+ * Disables/enabled handling of memory reads, because reads inside of seqcount
+ * read critical section are inherently racy.
+ */
+void ktsan_seqcount_begin(const void *s);
+void ktsan_seqcount_end(const void *s);
+void ktsan_seqcount_ignore_begin(void);
+void ktsan_seqcount_ignore_end(void);
+
+void ktsan_thread_fence(ktsan_memory_order_t mo);
+
+void ktsan_atomic8_store(void *addr, u8 value, ktsan_memory_order_t mo);
+void ktsan_atomic16_store(void *addr, u16 value, ktsan_memory_order_t mo);
+void ktsan_atomic32_store(void *addr, u32 value, ktsan_memory_order_t mo);
+void ktsan_atomic64_store(void *addr, u64 value, ktsan_memory_order_t mo);
+
+u8 ktsan_atomic8_load(const void *addr, ktsan_memory_order_t mo);
+u16 ktsan_atomic16_load(const void *addr, ktsan_memory_order_t mo);
+u32 ktsan_atomic32_load(const void *addr, ktsan_memory_order_t mo);
+u64 ktsan_atomic64_load(const void *addr, ktsan_memory_order_t mo);
+
+u8 ktsan_atomic8_exchange(void *addr, u8 value, ktsan_memory_order_t mo);
+u16 ktsan_atomic16_exchange(void *addr, u16 value, ktsan_memory_order_t mo);
+u32 ktsan_atomic32_exchange(void *addr, u32 value, ktsan_memory_order_t mo);
+u64 ktsan_atomic64_exchange(void *addr, u64 value, ktsan_memory_order_t mo);
+
+u8 ktsan_atomic8_compare_exchange(void *addr, u8 old, u8 new,
+				  ktsan_memory_order_t mo);
+u16 ktsan_atomic16_compare_exchange(void *addr, u16 old, u16 new,
+				    ktsan_memory_order_t mo);
+u32 ktsan_atomic32_compare_exchange(void *addr, u32 old, u32 new,
+				    ktsan_memory_order_t mo);
+u64 ktsan_atomic64_compare_exchange(void *addr, u64 old, u64 new,
+				    ktsan_memory_order_t mo);
+
+u8 ktsan_atomic8_fetch_add(void *addr, u8 value, ktsan_memory_order_t mo);
+u16 ktsan_atomic16_fetch_add(void *addr, u16 value, ktsan_memory_order_t mo);
+u32 ktsan_atomic32_fetch_add(void *addr, u32 value, ktsan_memory_order_t mo);
+u64 ktsan_atomic64_fetch_add(void *addr, u64 value, ktsan_memory_order_t mo);
+
+void ktsan_atomic_set_bit(void *addr, long nr, ktsan_memory_order_t mo);
+void ktsan_atomic_clear_bit(void *addr, long nr, ktsan_memory_order_t mo);
+void ktsan_atomic_change_bit(void *addr, long nr, ktsan_memory_order_t mo);
+
+int ktsan_atomic_fetch_set_bit(void *addr, long nr, ktsan_memory_order_t mo);
+int ktsan_atomic_fetch_clear_bit(void *addr, long nr, ktsan_memory_order_t mo);
+int ktsan_atomic_fetch_change_bit(void *addr, long nr, ktsan_memory_order_t mo);
+
+void ktsan_preempt_add(int value);
+void ktsan_preempt_sub(int value);
+
+void ktsan_irq_disable(void);
+void ktsan_irq_enable(void);
+void ktsan_irq_save(void);
+void ktsan_irq_restore(unsigned long flags);
+
+void ktsan_percpu_acquire(void *addr);
+
+void ktsan_alloc_page(struct page *page, unsigned int order, gfp_t flags,
+		      int node);
+void ktsan_free_page(struct page *page, unsigned int order);
+void ktsan_split_page(struct page *page, unsigned int order);
+
+void ktsan_syscall_enter(void);
+void ktsan_syscall_exit(void);
+
+#else /* CONFIG_KTSAN */
+
+/* When disabled ktsan is no-op. */
+
+struct ktsan_task_s {
+};
+
+static inline void ktsan_init_early(void)
+{
+}
+static inline void ktsan_init(void)
+{
+}
+
+static inline void ktsan_print_diagnostics(void)
+{
+}
+
+static inline void ktsan_cpu_start(void)
+{
+}
+
+static inline void ktsan_task_create(struct ktsan_task_s *new, int pid)
+{
+}
+static inline void ktsan_task_destroy(struct ktsan_task_s *old)
+{
+}
+static inline void ktsan_task_start(void)
+{
+}
+static inline void ktsan_task_stop(void)
+{
+}
+
+static inline void ktsan_thr_event_disable(void)
+{
+}
+static inline void ktsan_thr_event_enable(void)
+{
+}
+static inline void ktsan_thr_report_disable(void)
+{
+}
+static inline void ktsan_thr_report_enable(void)
+{
+}
+
+static inline void ktsan_memblock_alloc(void *addr, unsigned long size)
+{
+}
+static inline void ktsan_memblock_free(void *addr, unsigned long size)
+{
+}
+
+static inline void ktsan_sync_acquire(void *addr)
+{
+}
+static inline void ktsan_sync_release(void *addr)
+{
+}
+
+static inline void ktsan_mtx_pre_lock(void *addr, bool write, bool try)
+{
+}
+static inline void ktsan_mtx_post_lock(void *addr, bool write, bool try,
+				       bool success)
+{
+}
+static inline void ktsan_mtx_pre_unlock(void *addr, bool write)
+{
+}
+static inline void ktsan_mtx_post_unlock(void *addr, bool write)
+{
+}
+static inline void ktsan_mtx_downgrade(void *addr)
+{
+}
+
+static inline void ktsan_seqcount_begin(const void *s)
+{
+}
+static inline void ktsan_seqcount_end(const void *s)
+{
+}
+static inline void ktsan_seqcount_ignore_begin(void)
+{
+}
+static inline void ktsan_seqcount_ignore_end(void)
+{
+}
+
+/* ktsan_atomic* are not called in non-ktsan build. */
+/* ktsan_bitop* are not called in non-ktsan build. */
+
+static inline void ktsan_preempt_add(int value)
+{
+}
+static inline void ktsan_preempt_sub(int value)
+{
+}
+
+static inline void ktsan_irq_disable(void)
+{
+}
+static inline void ktsan_irq_enable(void)
+{
+}
+static inline void ktsan_irq_save(void)
+{
+}
+static inline void ktsan_irq_restore(unsigned long flags)
+{
+}
+
+static inline void ktsan_percpu_acquire(void *addr)
+{
+}
+
+static inline void ktsan_alloc_page(struct page *page, unsigned int order,
+				    gfp_t flags, int node)
+{
+}
+static inline void ktsan_free_page(struct page *page, unsigned int order)
+{
+}
+static inline void ktsan_split_page(struct page *page, unsigned int order)
+{
+}
+
+static inline void ktsan_syscall_enter(void)
+{
+}
+static inline void ktsan_syscall_exit(void)
+{
+}
+
+#endif /* CONFIG_KTSAN */
+
+#endif /* LINUX_KTSAN_H */
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 8ec38b11b361..d534c21b65e3 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -207,6 +207,10 @@ struct page {
 					   not kmapped, ie. highmem) */
 #endif /* WANT_PAGE_VIRTUAL */
 
+#ifdef CONFIG_KTSAN
+	void *shadow;
+#endif
+
 #ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS
 	int _last_cpupid;
 #endif
diff --git a/include/linux/percpu-defs.h b/include/linux/percpu-defs.h
index a6fabd865211..2824f24b4406 100644
--- a/include/linux/percpu-defs.h
+++ b/include/linux/percpu-defs.h
@@ -17,6 +17,8 @@
 #ifndef _LINUX_PERCPU_DEFS_H
 #define _LINUX_PERCPU_DEFS_H
 
+#include <linux/ktsan.h>
+
 #ifdef CONFIG_SMP
 
 #ifdef MODULE
@@ -239,6 +241,7 @@ do {									\
 
 #define raw_cpu_ptr(ptr)						\
 ({									\
+	ktsan_percpu_acquire(arch_raw_cpu_ptr(ptr));			\
 	__verify_pcpu_ptr(ptr);						\
 	arch_raw_cpu_ptr(ptr);						\
 })
@@ -246,6 +249,7 @@ do {									\
 #ifdef CONFIG_DEBUG_PREEMPT
 #define this_cpu_ptr(ptr)						\
 ({									\
+	ktsan_percpu_acquire(SHIFT_PERCPU_PTR(ptr, my_cpu_offset));	\
 	__verify_pcpu_ptr(ptr);						\
 	SHIFT_PERCPU_PTR(ptr, my_cpu_offset);				\
 })
@@ -318,6 +322,7 @@ static inline void __this_cpu_preempt_check(const char *op) { }
 ({									\
 	typeof(variable) pscr_ret__;					\
 	__verify_pcpu_ptr(&(variable));					\
+	ktsan_percpu_acquire(&(variable));				\
 	switch(sizeof(variable)) {					\
 	case 1: pscr_ret__ = stem##1(variable); break;			\
 	case 2: pscr_ret__ = stem##2(variable); break;			\
@@ -333,6 +338,7 @@ static inline void __this_cpu_preempt_check(const char *op) { }
 ({									\
 	typeof(variable) pscr2_ret__;					\
 	__verify_pcpu_ptr(&(variable));					\
+	ktsan_percpu_acquire(&(variable));				\
 	switch(sizeof(variable)) {					\
 	case 1: pscr2_ret__ = stem##1(variable, __VA_ARGS__); break;	\
 	case 2: pscr2_ret__ = stem##2(variable, __VA_ARGS__); break;	\
@@ -360,6 +366,8 @@ static inline void __this_cpu_preempt_check(const char *op) { }
 	VM_BUG_ON((unsigned long)(&(pcp1)) % (2 * sizeof(pcp1)));	\
 	VM_BUG_ON((unsigned long)(&(pcp2)) !=				\
 		  (unsigned long)(&(pcp1)) + sizeof(pcp1));		\
+	ktsan_percpu_acquire(&(pcp1));					\
+	ktsan_percpu_acquire(&(pcp2));					\
 	switch(sizeof(pcp1)) {						\
 	case 1: pdcrb_ret__ = stem##1(pcp1, pcp2, __VA_ARGS__); break;	\
 	case 2: pdcrb_ret__ = stem##2(pcp1, pcp2, __VA_ARGS__); break;	\
@@ -374,6 +382,7 @@ static inline void __this_cpu_preempt_check(const char *op) { }
 #define __pcpu_size_call(stem, variable, ...)				\
 do {									\
 	__verify_pcpu_ptr(&(variable));					\
+	ktsan_percpu_acquire(&(variable));				\
 	switch(sizeof(variable)) {					\
 		case 1: stem##1(variable, __VA_ARGS__);break;		\
 		case 2: stem##2(variable, __VA_ARGS__);break;		\
diff --git a/include/linux/percpu-rwsem.h b/include/linux/percpu-rwsem.h
index 03cb4b6f842e..fc564a74e555 100644
--- a/include/linux/percpu-rwsem.h
+++ b/include/linux/percpu-rwsem.h
@@ -8,6 +8,7 @@
 #include <linux/rcuwait.h>
 #include <linux/rcu_sync.h>
 #include <linux/lockdep.h>
+#include <linux/ktsan.h>
 
 struct percpu_rw_semaphore {
 	struct rcu_sync		rss;
@@ -33,6 +34,7 @@ static inline void percpu_down_read(struct percpu_rw_semaphore *sem)
 {
 	might_sleep();
 
+	ktsan_mtx_pre_lock(sem, false, false);
 	rwsem_acquire_read(&sem->rw_sem.dep_map, 0, 0, _RET_IP_);
 
 	preempt_disable();
@@ -47,6 +49,7 @@ static inline void percpu_down_read(struct percpu_rw_semaphore *sem)
 	__this_cpu_inc(*sem->read_count);
 	if (unlikely(!rcu_sync_is_idle(&sem->rss)))
 		__percpu_down_read(sem, false); /* Unconditional memory barrier */
+	ktsan_mtx_post_lock(sem, false, false, true);
 	/*
 	 * The preempt_enable() prevents the compiler from
 	 * bleeding the critical section out.
@@ -58,6 +61,7 @@ static inline int percpu_down_read_trylock(struct percpu_rw_semaphore *sem)
 {
 	int ret = 1;
 
+	ktsan_mtx_pre_lock(sem, false, true);
 	preempt_disable();
 	/*
 	 * Same as in percpu_down_read().
@@ -73,6 +77,7 @@ static inline int percpu_down_read_trylock(struct percpu_rw_semaphore *sem)
 
 	if (ret)
 		rwsem_acquire_read(&sem->rw_sem.dep_map, 0, 1, _RET_IP_);
+	ktsan_mtx_post_lock(sem, false, true, ret == 1);
 
 	return ret;
 }
@@ -80,6 +85,7 @@ static inline int percpu_down_read_trylock(struct percpu_rw_semaphore *sem)
 static inline void percpu_up_read(struct percpu_rw_semaphore *sem)
 {
 	preempt_disable();
+	ktsan_mtx_pre_unlock(sem, false);
 	/*
 	 * Same as in percpu_down_read().
 	 */
@@ -90,6 +96,7 @@ static inline void percpu_up_read(struct percpu_rw_semaphore *sem)
 	preempt_enable();
 
 	rwsem_release(&sem->rw_sem.dep_map, 1, _RET_IP_);
+	ktsan_mtx_post_unlock(sem, false);
 }
 
 extern void percpu_down_write(struct percpu_rw_semaphore *);
diff --git a/include/linux/rcupdate.h b/include/linux/rcupdate.h
index b25d20822e75..f0b1f99bafaf 100644
--- a/include/linux/rcupdate.h
+++ b/include/linux/rcupdate.h
@@ -29,6 +29,7 @@
 #include <linux/lockdep.h>
 #include <asm/processor.h>
 #include <linux/cpumask.h>
+#include <linux/ktsan.h>
 
 #define ULONG_CMP_GE(a, b)	(ULONG_MAX / 2 >= (a) - (b))
 #define ULONG_CMP_LT(a, b)	(ULONG_MAX / 2 < (a) - (b))
@@ -640,6 +641,7 @@ static inline void rcu_read_lock(void)
  */
 static inline void rcu_read_unlock(void)
 {
+	ktsan_sync_release(&ktsan_glob_sync[ktsan_glob_sync_type_rcu_common]);
 	RCU_LOCKDEP_WARN(!rcu_is_watching(),
 			 "rcu_read_unlock() used illegally while idle");
 	__release(RCU);
@@ -675,6 +677,7 @@ static inline void rcu_read_lock_bh(void)
  */
 static inline void rcu_read_unlock_bh(void)
 {
+	ktsan_sync_release(&ktsan_glob_sync[ktsan_glob_sync_type_rcu_bh]);
 	RCU_LOCKDEP_WARN(!rcu_is_watching(),
 			 "rcu_read_unlock_bh() used illegally while idle");
 	rcu_lock_release(&rcu_bh_lock_map);
@@ -717,6 +720,7 @@ static inline notrace void rcu_read_lock_sched_notrace(void)
  */
 static inline void rcu_read_unlock_sched(void)
 {
+	ktsan_sync_release(&ktsan_glob_sync[ktsan_glob_sync_type_rcu_sched]);
 	RCU_LOCKDEP_WARN(!rcu_is_watching(),
 			 "rcu_read_unlock_sched() used illegally while idle");
 	rcu_lock_release(&rcu_sched_lock_map);
@@ -727,6 +731,7 @@ static inline void rcu_read_unlock_sched(void)
 /* Used by lockdep and tracing: cannot be traced, cannot call lockdep. */
 static inline notrace void rcu_read_unlock_sched_notrace(void)
 {
+	ktsan_sync_release(&ktsan_glob_sync[ktsan_glob_sync_type_rcu_sched]);
 	__release(RCU_SCHED);
 	preempt_enable_notrace();
 }
diff --git a/include/linux/rwlock_api_smp.h b/include/linux/rwlock_api_smp.h
index 86ebb4bf9c6e..6fcd3f2ef14e 100644
--- a/include/linux/rwlock_api_smp.h
+++ b/include/linux/rwlock_api_smp.h
@@ -15,6 +15,8 @@
  * Released under the General Public License (GPL).
  */
 
+#include <linux/ktsan.h>
+
 void __lockfunc _raw_read_lock(rwlock_t *lock)		__acquires(lock);
 void __lockfunc _raw_write_lock(rwlock_t *lock)		__acquires(lock);
 void __lockfunc _raw_read_lock_bh(rwlock_t *lock)	__acquires(lock);
@@ -117,10 +119,13 @@ _raw_write_unlock_irqrestore(rwlock_t *lock, unsigned long flags)
 static inline int __raw_read_trylock(rwlock_t *lock)
 {
 	preempt_disable();
+	ktsan_mtx_pre_lock(lock, false, true);
 	if (do_raw_read_trylock(lock)) {
 		rwlock_acquire_read(&lock->dep_map, 0, 1, _RET_IP_);
+		ktsan_mtx_post_lock(lock, false, true, true);
 		return 1;
 	}
+	ktsan_mtx_post_lock(lock, false, true, false);
 	preempt_enable();
 	return 0;
 }
@@ -128,10 +133,13 @@ static inline int __raw_read_trylock(rwlock_t *lock)
 static inline int __raw_write_trylock(rwlock_t *lock)
 {
 	preempt_disable();
+	ktsan_mtx_pre_lock(lock, true, true);
 	if (do_raw_write_trylock(lock)) {
 		rwlock_acquire(&lock->dep_map, 0, 1, _RET_IP_);
+		ktsan_mtx_post_lock(lock, true, true, true);
 		return 1;
 	}
+	ktsan_mtx_post_lock(lock, true, true, false);
 	preempt_enable();
 	return 0;
 }
@@ -145,9 +153,11 @@ static inline int __raw_write_trylock(rwlock_t *lock)
 
 static inline void __raw_read_lock(rwlock_t *lock)
 {
+	ktsan_mtx_pre_lock(lock, false, false);
 	preempt_disable();
 	rwlock_acquire_read(&lock->dep_map, 0, 0, _RET_IP_);
 	LOCK_CONTENDED(lock, do_raw_read_trylock, do_raw_read_lock);
+	ktsan_mtx_post_lock(lock, false, false, true);
 }
 
 static inline unsigned long __raw_read_lock_irqsave(rwlock_t *lock)
@@ -156,9 +166,11 @@ static inline unsigned long __raw_read_lock_irqsave(rwlock_t *lock)
 
 	local_irq_save(flags);
 	preempt_disable();
+	ktsan_mtx_pre_lock(lock, false, false);
 	rwlock_acquire_read(&lock->dep_map, 0, 0, _RET_IP_);
 	LOCK_CONTENDED_FLAGS(lock, do_raw_read_trylock, do_raw_read_lock,
 			     do_raw_read_lock_flags, &flags);
+	ktsan_mtx_post_lock(lock, false, false, true);
 	return flags;
 }
 
@@ -166,15 +178,19 @@ static inline void __raw_read_lock_irq(rwlock_t *lock)
 {
 	local_irq_disable();
 	preempt_disable();
+	ktsan_mtx_pre_lock(lock, false, false);
 	rwlock_acquire_read(&lock->dep_map, 0, 0, _RET_IP_);
 	LOCK_CONTENDED(lock, do_raw_read_trylock, do_raw_read_lock);
+	ktsan_mtx_post_lock(lock, false, false, true);
 }
 
 static inline void __raw_read_lock_bh(rwlock_t *lock)
 {
 	__local_bh_disable_ip(_RET_IP_, SOFTIRQ_LOCK_OFFSET);
+	ktsan_mtx_pre_lock(lock, false, false);
 	rwlock_acquire_read(&lock->dep_map, 0, 0, _RET_IP_);
 	LOCK_CONTENDED(lock, do_raw_read_trylock, do_raw_read_lock);
+	ktsan_mtx_post_lock(lock, false, false, true);
 }
 
 static inline unsigned long __raw_write_lock_irqsave(rwlock_t *lock)
@@ -183,9 +199,11 @@ static inline unsigned long __raw_write_lock_irqsave(rwlock_t *lock)
 
 	local_irq_save(flags);
 	preempt_disable();
+	ktsan_mtx_pre_lock(lock, true, false);
 	rwlock_acquire(&lock->dep_map, 0, 0, _RET_IP_);
 	LOCK_CONTENDED_FLAGS(lock, do_raw_write_trylock, do_raw_write_lock,
 			     do_raw_write_lock_flags, &flags);
+	ktsan_mtx_post_lock(lock, true, false, true);
 	return flags;
 }
 
@@ -193,85 +211,107 @@ static inline void __raw_write_lock_irq(rwlock_t *lock)
 {
 	local_irq_disable();
 	preempt_disable();
+	ktsan_mtx_pre_lock(lock, true, false);
 	rwlock_acquire(&lock->dep_map, 0, 0, _RET_IP_);
 	LOCK_CONTENDED(lock, do_raw_write_trylock, do_raw_write_lock);
+	ktsan_mtx_post_lock(lock, true, false, true);
 }
 
 static inline void __raw_write_lock_bh(rwlock_t *lock)
 {
 	__local_bh_disable_ip(_RET_IP_, SOFTIRQ_LOCK_OFFSET);
+	ktsan_mtx_pre_lock(lock, true, false);
 	rwlock_acquire(&lock->dep_map, 0, 0, _RET_IP_);
 	LOCK_CONTENDED(lock, do_raw_write_trylock, do_raw_write_lock);
+	ktsan_mtx_post_lock(lock, true, false, true);
 }
 
 static inline void __raw_write_lock(rwlock_t *lock)
 {
 	preempt_disable();
+	ktsan_mtx_pre_lock(lock, true, false);
 	rwlock_acquire(&lock->dep_map, 0, 0, _RET_IP_);
 	LOCK_CONTENDED(lock, do_raw_write_trylock, do_raw_write_lock);
+	ktsan_mtx_post_lock(lock, true, false, true);
 }
 
 #endif /* !CONFIG_GENERIC_LOCKBREAK || CONFIG_DEBUG_LOCK_ALLOC */
 
 static inline void __raw_write_unlock(rwlock_t *lock)
 {
+	ktsan_mtx_pre_unlock(lock, true);
 	rwlock_release(&lock->dep_map, 1, _RET_IP_);
 	do_raw_write_unlock(lock);
+	ktsan_mtx_post_unlock(lock, true);
 	preempt_enable();
 }
 
 static inline void __raw_read_unlock(rwlock_t *lock)
 {
+	ktsan_mtx_pre_unlock(lock, false);
 	rwlock_release(&lock->dep_map, 1, _RET_IP_);
 	do_raw_read_unlock(lock);
+	ktsan_mtx_post_unlock(lock, false);
 	preempt_enable();
 }
 
 static inline void
 __raw_read_unlock_irqrestore(rwlock_t *lock, unsigned long flags)
 {
+	ktsan_mtx_pre_unlock(lock, false);
 	rwlock_release(&lock->dep_map, 1, _RET_IP_);
 	do_raw_read_unlock(lock);
+	ktsan_mtx_post_unlock(lock, false);
 	local_irq_restore(flags);
 	preempt_enable();
 }
 
 static inline void __raw_read_unlock_irq(rwlock_t *lock)
 {
+	ktsan_mtx_pre_unlock(lock, false);
 	rwlock_release(&lock->dep_map, 1, _RET_IP_);
 	do_raw_read_unlock(lock);
+	ktsan_mtx_post_unlock(lock, false);
 	local_irq_enable();
 	preempt_enable();
 }
 
 static inline void __raw_read_unlock_bh(rwlock_t *lock)
 {
+	ktsan_mtx_pre_unlock(lock, false);
 	rwlock_release(&lock->dep_map, 1, _RET_IP_);
 	do_raw_read_unlock(lock);
+	ktsan_mtx_post_unlock(lock, false);
 	__local_bh_enable_ip(_RET_IP_, SOFTIRQ_LOCK_OFFSET);
 }
 
 static inline void __raw_write_unlock_irqrestore(rwlock_t *lock,
 					     unsigned long flags)
 {
+	ktsan_mtx_pre_unlock(lock, true);
 	rwlock_release(&lock->dep_map, 1, _RET_IP_);
 	do_raw_write_unlock(lock);
+	ktsan_mtx_post_unlock(lock, true);
 	local_irq_restore(flags);
 	preempt_enable();
 }
 
 static inline void __raw_write_unlock_irq(rwlock_t *lock)
 {
+	ktsan_mtx_pre_unlock(lock, true);
 	rwlock_release(&lock->dep_map, 1, _RET_IP_);
 	do_raw_write_unlock(lock);
+	ktsan_mtx_post_unlock(lock, true);
 	local_irq_enable();
 	preempt_enable();
 }
 
 static inline void __raw_write_unlock_bh(rwlock_t *lock)
 {
+	ktsan_mtx_pre_unlock(lock, true);
 	rwlock_release(&lock->dep_map, 1, _RET_IP_);
 	do_raw_write_unlock(lock);
+	ktsan_mtx_post_unlock(lock, true);
 	__local_bh_enable_ip(_RET_IP_, SOFTIRQ_LOCK_OFFSET);
 }
 
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 11837410690f..657b4be2da9d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -29,6 +29,7 @@
 #include <linux/mm_types_task.h>
 #include <linux/task_io_accounting.h>
 #include <linux/rseq.h>
+#include <linux/ktsan.h>
 
 /* task_struct member predeclarations (sorted alphabetically): */
 struct audit_context;
@@ -1195,6 +1196,9 @@ struct task_struct {
 	void				*security;
 #endif
 
+	/* KTSAN per-task state. Empty in non-KTSAN build. */
+	struct ktsan_task_s ktsan;
+
 #ifdef CONFIG_GCC_PLUGIN_STACKLEAK
 	unsigned long			lowest_stack;
 	unsigned long			prev_lowest_stack;
diff --git a/include/linux/seqlock.h b/include/linux/seqlock.h
index bcf4cf26b8c8..fe8af3881f7b 100644
--- a/include/linux/seqlock.h
+++ b/include/linux/seqlock.h
@@ -37,6 +37,7 @@
 #include <linux/preempt.h>
 #include <linux/lockdep.h>
 #include <linux/compiler.h>
+#include <linux/ktsan.h>
 #include <asm/processor.h>
 
 /*
@@ -109,6 +110,7 @@ static inline unsigned __read_seqcount_begin(const seqcount_t *s)
 {
 	unsigned ret;
 
+	ktsan_seqcount_begin(s);
 repeat:
 	ret = READ_ONCE(s->sequence);
 	if (unlikely(ret & 1)) {
@@ -129,7 +131,26 @@ static inline unsigned __read_seqcount_begin(const seqcount_t *s)
  */
 static inline unsigned raw_read_seqcount(const seqcount_t *s)
 {
-	unsigned ret = READ_ONCE(s->sequence);
+	unsigned ret;
+
+	ktsan_seqcount_begin(s);
+	ret = READ_ONCE(s->sequence);
+	smp_rmb();
+	return ret;
+}
+
+/**
+ * raw_read_seqcount_nocritical - Read the raw seqcount
+ * @s: pointer to seqcount_t
+ * Returns: seq count
+ *
+ * unlike raw_read_seqcount this function does not open a read critical section.
+ */
+static inline unsigned raw_read_seqcount_nocritical(const seqcount_t *s)
+{
+	unsigned ret;
+
+	ret = READ_ONCE(s->sequence);
 	smp_rmb();
 	return ret;
 }
@@ -181,7 +202,10 @@ static inline unsigned read_seqcount_begin(const seqcount_t *s)
  */
 static inline unsigned raw_seqcount_begin(const seqcount_t *s)
 {
-	unsigned ret = READ_ONCE(s->sequence);
+	unsigned ret;
+
+	ktsan_seqcount_begin(s);
+	ret = READ_ONCE(s->sequence);
 	smp_rmb();
 	return ret & ~1;
 }
@@ -202,7 +226,11 @@ static inline unsigned raw_seqcount_begin(const seqcount_t *s)
  */
 static inline int __read_seqcount_retry(const seqcount_t *s, unsigned start)
 {
-	return unlikely(s->sequence != start);
+	int ret;
+
+	ret = unlikely(s->sequence != start);
+	ktsan_seqcount_end(s);
+	return ret;
 }
 
 /**
@@ -221,10 +249,23 @@ static inline int read_seqcount_retry(const seqcount_t *s, unsigned start)
 	return __read_seqcount_retry(s, start);
 }
 
+/**
+ * read_seqcount_cancel - cancel a seq-read critical section
+ * @s: pointer to seqcount_t
+ *
+ * This is a no-op except for ktsan, it needs to know scopes of seq-read
+ * critical sections. The sections are denoted either by begin->retry or
+ * by begin->cancel.
+ */
+static inline void read_seqcount_cancel(const seqcount_t *s)
+{
+	ktsan_seqcount_end(s);
+}
 
 
 static inline void raw_write_seqcount_begin(seqcount_t *s)
 {
+	ktsan_seqcount_begin(s);
 	s->sequence++;
 	smp_wmb();
 }
@@ -233,6 +274,7 @@ static inline void raw_write_seqcount_end(seqcount_t *s)
 {
 	smp_wmb();
 	s->sequence++;
+	ktsan_seqcount_end(s);
 }
 
 /**
@@ -278,8 +320,10 @@ static inline void raw_write_seqcount_barrier(seqcount_t *s)
 
 static inline int raw_read_seqcount_latch(seqcount_t *s)
 {
+	int seq;
+	ktsan_seqcount_begin(s);
 	/* Pairs with the first smp_wmb() in raw_write_seqcount_latch() */
-	int seq = READ_ONCE(s->sequence); /* ^^^ */
+	seq = READ_ONCE(s->sequence); /* ^^^ */
 	return seq;
 }
 
@@ -438,6 +482,11 @@ static inline unsigned read_seqretry(const seqlock_t *sl, unsigned start)
 	return read_seqcount_retry(&sl->seqcount, start);
 }
 
+static inline void read_seqcancel(const seqlock_t *sl)
+{
+	read_seqcount_cancel(&sl->seqcount);
+}
+
 /*
  * Lock out other writers and update the count.
  * Acts like a normal spin_lock/unlock.
@@ -536,6 +585,20 @@ static inline int need_seqretry(seqlock_t *lock, int seq)
 	return !(seq & 1) && read_seqretry(lock, seq);
 }
 
+/**
+ * Same as need_seqretry, but does not end critical section,
+ * if the check is successful.
+ */
+static inline int need_seqretry_check(seqlock_t *lock, int seq)
+{
+	if (seq & 1)
+		return 0;
+	if (read_seqretry(lock, seq))
+		return 1;
+	ktsan_seqcount_begin(&lock->seqcount);
+	return 0;
+}
+
 static inline void done_seqretry(seqlock_t *lock, int seq)
 {
 	if (seq & 1)
diff --git a/include/linux/spinlock_api_smp.h b/include/linux/spinlock_api_smp.h
index 42dfab89e740..92f3e471d96f 100644
--- a/include/linux/spinlock_api_smp.h
+++ b/include/linux/spinlock_api_smp.h
@@ -5,6 +5,8 @@
 # error "please don't include this file directly"
 #endif
 
+#include <linux/ktsan.h>
+
 /*
  * include/linux/spinlock_api_smp.h
  *
@@ -86,10 +88,13 @@ _raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)
 static inline int __raw_spin_trylock(raw_spinlock_t *lock)
 {
 	preempt_disable();
+	ktsan_mtx_pre_lock(lock, true, true);
 	if (do_raw_spin_trylock(lock)) {
 		spin_acquire(&lock->dep_map, 0, 1, _RET_IP_);
+		ktsan_mtx_post_lock(lock, true, true, true);
 		return 1;
 	}
+	ktsan_mtx_post_lock(lock, true, true, false);
 	preempt_enable();
 	return 0;
 }
@@ -107,6 +112,7 @@ static inline unsigned long __raw_spin_lock_irqsave(raw_spinlock_t *lock)
 
 	local_irq_save(flags);
 	preempt_disable();
+	ktsan_mtx_pre_lock(lock, true, false);
 	spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);
 	/*
 	 * On lockdep we dont want the hand-coded irq-enable of
@@ -118,6 +124,7 @@ static inline unsigned long __raw_spin_lock_irqsave(raw_spinlock_t *lock)
 #else
 	do_raw_spin_lock_flags(lock, &flags);
 #endif
+	ktsan_mtx_post_lock(lock, true, false, true);
 	return flags;
 }
 
@@ -125,64 +132,81 @@ static inline void __raw_spin_lock_irq(raw_spinlock_t *lock)
 {
 	local_irq_disable();
 	preempt_disable();
+	ktsan_mtx_pre_lock(lock, true, false);
 	spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);
 	LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);
+	ktsan_mtx_post_lock(lock, true, false, true);
 }
 
 static inline void __raw_spin_lock_bh(raw_spinlock_t *lock)
 {
 	__local_bh_disable_ip(_RET_IP_, SOFTIRQ_LOCK_OFFSET);
+	ktsan_mtx_pre_lock(lock, true, false);
 	spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);
 	LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);
+	ktsan_mtx_post_lock(lock, true, false, true);
 }
 
 static inline void __raw_spin_lock(raw_spinlock_t *lock)
 {
 	preempt_disable();
+	ktsan_mtx_pre_lock(lock, true, false);
 	spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);
 	LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);
+	ktsan_mtx_post_lock(lock, true, false, true);
 }
 
 #endif /* !CONFIG_GENERIC_LOCKBREAK || CONFIG_DEBUG_LOCK_ALLOC */
 
 static inline void __raw_spin_unlock(raw_spinlock_t *lock)
 {
+	ktsan_mtx_pre_unlock(lock, true);
 	spin_release(&lock->dep_map, 1, _RET_IP_);
 	do_raw_spin_unlock(lock);
+	ktsan_mtx_post_unlock(lock, true);
 	preempt_enable();
 }
 
 static inline void __raw_spin_unlock_irqrestore(raw_spinlock_t *lock,
 					    unsigned long flags)
 {
+	ktsan_mtx_pre_unlock(lock, true);
 	spin_release(&lock->dep_map, 1, _RET_IP_);
 	do_raw_spin_unlock(lock);
+	ktsan_mtx_post_unlock(lock, true);
 	local_irq_restore(flags);
 	preempt_enable();
 }
 
 static inline void __raw_spin_unlock_irq(raw_spinlock_t *lock)
 {
+	ktsan_mtx_pre_unlock(lock, true);
 	spin_release(&lock->dep_map, 1, _RET_IP_);
 	do_raw_spin_unlock(lock);
+	ktsan_mtx_post_unlock(lock, true);
 	local_irq_enable();
 	preempt_enable();
 }
 
 static inline void __raw_spin_unlock_bh(raw_spinlock_t *lock)
 {
+	ktsan_mtx_pre_unlock(lock, true);
 	spin_release(&lock->dep_map, 1, _RET_IP_);
 	do_raw_spin_unlock(lock);
+	ktsan_mtx_post_unlock(lock, true);
 	__local_bh_enable_ip(_RET_IP_, SOFTIRQ_LOCK_OFFSET);
 }
 
 static inline int __raw_spin_trylock_bh(raw_spinlock_t *lock)
 {
 	__local_bh_disable_ip(_RET_IP_, SOFTIRQ_LOCK_OFFSET);
+	ktsan_mtx_pre_lock(lock, true, true);
 	if (do_raw_spin_trylock(lock)) {
 		spin_acquire(&lock->dep_map, 0, 1, _RET_IP_);
+		ktsan_mtx_post_lock(lock, true, true, true);
 		return 1;
 	}
+	ktsan_mtx_post_lock(lock, true, true, false);
 	__local_bh_enable_ip(_RET_IP_, SOFTIRQ_LOCK_OFFSET);
 	return 0;
 }
diff --git a/include/linux/thread_info.h b/include/linux/thread_info.h
index 8d8821b3689a..65dc88ea12c1 100644
--- a/include/linux/thread_info.h
+++ b/include/linux/thread_info.h
@@ -43,7 +43,11 @@ enum {
 #define THREAD_ALIGN	THREAD_SIZE
 #endif
 
+#ifndef CONFIG_KTSAN
 #define THREADINFO_GFP		(GFP_KERNEL_ACCOUNT | __GFP_ZERO)
+#else /* CONFIG_KTSAN */
+# define THREADINFO_GFP		(GFP_KERNEL)
+#endif /* CONFIG_KTSAN */
 
 /*
  * flag set/clear/test wrappers
diff --git a/include/linux/wait_bit.h b/include/linux/wait_bit.h
index 7dec36aecbd9..6eb36ee1fd1d 100644
--- a/include/linux/wait_bit.h
+++ b/include/linux/wait_bit.h
@@ -70,12 +70,23 @@ extern int bit_wait_io_timeout(struct wait_bit_key *key, int mode);
 static inline int
 wait_on_bit(unsigned long *word, int bit, unsigned mode)
 {
+	int ret = 0;
+
 	might_sleep();
-	if (!test_bit(bit, word))
-		return 0;
-	return out_of_line_wait_on_bit(word, bit,
-				       bit_wait,
-				       mode);
+	if (test_bit(bit, word))
+		ret = out_of_line_wait_on_bit(word, bit,
+					      bit_wait,
+					      mode);
+	/* test_bit does not imply a memory barrier, nonetheless wait_on_bit is
+	 * a synchronization operation. Inter-thread ordering is ensured by
+	 * control dependency on test_bit result. Ktsan does not understand
+	 * this ordering, so we add an explicit acquire annotation. Reset of the
+	 * bit should be done with clear_bit_unlock, which is the pairing
+	 * release operation.
+	 */
+	if (ret == 0)
+		ktsan_sync_acquire(word);
+	return ret;
 }
 
 /**
@@ -95,12 +106,17 @@ wait_on_bit(unsigned long *word, int bit, unsigned mode)
 static inline int
 wait_on_bit_io(unsigned long *word, int bit, unsigned mode)
 {
+	int ret = 0;
+
 	might_sleep();
-	if (!test_bit(bit, word))
-		return 0;
-	return out_of_line_wait_on_bit(word, bit,
-				       bit_wait_io,
-				       mode);
+	if (test_bit(bit, word))
+		ret = out_of_line_wait_on_bit(word, bit,
+					      bit_wait_io,
+					      mode);
+	if (ret == 0)
+		ktsan_sync_acquire(word);
+	return ret;
+
 }
 
 /**
@@ -122,12 +138,16 @@ static inline int
 wait_on_bit_timeout(unsigned long *word, int bit, unsigned mode,
 		    unsigned long timeout)
 {
+	int ret = 0;
+
 	might_sleep();
-	if (!test_bit(bit, word))
-		return 0;
-	return out_of_line_wait_on_bit_timeout(word, bit,
-					       bit_wait_timeout,
-					       mode, timeout);
+	if (test_bit(bit, word))
+		ret = out_of_line_wait_on_bit_timeout(word, bit,
+						      bit_wait_timeout,
+						      mode, timeout);
+	if (ret == 0)
+		ktsan_sync_acquire(word);
+	return ret;
 }
 
 /**
@@ -150,10 +170,14 @@ static inline int
 wait_on_bit_action(unsigned long *word, int bit, wait_bit_action_f *action,
 		   unsigned mode)
 {
+	int ret = 0;
+
 	might_sleep();
-	if (!test_bit(bit, word))
-		return 0;
-	return out_of_line_wait_on_bit(word, bit, action, mode);
+	if (test_bit(bit, word))
+		ret = out_of_line_wait_on_bit(word, bit, action, mode);
+	if (ret == 0)
+		ktsan_sync_acquire(word);
+	return ret;
 }
 
 /**
diff --git a/include/net/sch_generic.h b/include/net/sch_generic.h
index 21f434f3ac9e..e4ed705e6760 100644
--- a/include/net/sch_generic.h
+++ b/include/net/sch_generic.h
@@ -140,7 +140,7 @@ static inline bool qdisc_is_running(struct Qdisc *qdisc)
 {
 	if (qdisc->flags & TCQ_F_NOLOCK)
 		return spin_is_locked(&qdisc->seqlock);
-	return (raw_read_seqcount(&qdisc->running) & 1) ? true : false;
+	return (raw_read_seqcount_nocritical(&qdisc->running) & 1) ? true : false;
 }
 
 static inline bool qdisc_is_percpu_stats(const struct Qdisc *q)
diff --git a/include/net/sock.h b/include/net/sock.h
index 6cbc16136357..0347051b497c 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -883,7 +883,7 @@ static inline int sk_stream_min_wspace(const struct sock *sk)
 
 static inline int sk_stream_wspace(const struct sock *sk)
 {
-	return sk->sk_sndbuf - sk->sk_wmem_queued;
+	return sk->sk_sndbuf - READ_ONCE(sk->sk_wmem_queued);
 }
 
 void sk_stream_write_space(struct sock *sk);
@@ -1467,7 +1467,7 @@ DECLARE_STATIC_KEY_FALSE(tcp_tx_skb_cache_key);
 static inline void sk_wmem_free_skb(struct sock *sk, struct sk_buff *skb)
 {
 	sock_set_flag(sk, SOCK_QUEUE_SHRUNK);
-	sk->sk_wmem_queued -= skb->truesize;
+	WRITE_ONCE(sk->sk_wmem_queued, sk->sk_wmem_queued - skb->truesize);
 	sk_mem_uncharge(sk, skb->truesize);
 	if (static_branch_unlikely(&tcp_tx_skb_cache_key) &&
 	    !sk->sk_tx_skb_cache && !skb_cloned(skb)) {
diff --git a/init/Kconfig b/init/Kconfig
index 0e2344389501..22a1d79b2fe5 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -1701,6 +1701,7 @@ config SLAB
 
 config SLUB
 	bool "SLUB (Unqueued Allocator)"
+	depends on !KTSAN
 	select HAVE_HARDENED_USERCOPY_ALLOCATOR
 	help
 	   SLUB is a slab allocator that minimizes cache line usage
@@ -1711,7 +1712,7 @@ config SLUB
 	   a slab allocator.
 
 config SLOB
-	depends on EXPERT
+	depends on EXPERT && !KTSAN
 	bool "SLOB (Simple Allocator)"
 	help
 	   SLOB replaces the stock allocator with a drastically simpler
diff --git a/init/main.c b/init/main.c
index 66a196c5e4c3..5ab8c7deaf79 100644
--- a/init/main.c
+++ b/init/main.c
@@ -93,6 +93,7 @@
 #include <linux/rodata_test.h>
 #include <linux/jump_label.h>
 #include <linux/mem_encrypt.h>
+#include <linux/ktsan.h>
 
 #include <asm/io.h>
 #include <asm/bugs.h>
@@ -596,6 +597,7 @@ asmlinkage __visible void __init start_kernel(void)
 	 * These use large bootmem allocations and must precede
 	 * kmem_cache_init()
 	 */
+	ktsan_init_early();
 	setup_log_buf(0);
 	vfs_caches_init_early();
 	sort_main_extable();
@@ -757,6 +759,10 @@ asmlinkage __visible void __init start_kernel(void)
 	arch_post_acpi_subsys_init();
 	sfi_init_late();
 
+	ktsan_init();
+	ktsan_cpu_start();
+	ktsan_task_start();
+
 	/* Do the rest non-__init'ed, we're now alive */
 	arch_call_rest_init();
 }
diff --git a/kernel/Makefile b/kernel/Makefile
index a8d923b5481b..29568e5ebe02 100644
--- a/kernel/Makefile
+++ b/kernel/Makefile
@@ -3,6 +3,9 @@
 # Makefile for the linux kernel.
 #
 
+# XXX: messes up trace.
+KTSAN_SANITIZE_softirq.o = n
+
 obj-y     = fork.o exec_domain.o panic.o \
 	    cpu.o exit.o softirq.o resource.o \
 	    sysctl.o sysctl_binary.o capability.o ptrace.o user.o \
@@ -30,6 +33,7 @@ KCOV_INSTRUMENT_extable.o := n
 # Don't self-instrument.
 KCOV_INSTRUMENT_kcov.o := n
 KASAN_SANITIZE_kcov.o := n
+KTSAN_SANITIZE_kcov.o := n
 CFLAGS_kcov.o := $(call cc-option, -fno-conserve-stack -fno-stack-protector)
 
 # cond_syscall is currently not LTO compatible
diff --git a/kernel/fork.c b/kernel/fork.c
index fe83343da24b..e23916ab740b 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -94,6 +94,7 @@
 #include <linux/livepatch.h>
 #include <linux/thread_info.h>
 #include <linux/stackleak.h>
+#include <linux/ktsan.h>
 
 #include <asm/pgtable.h>
 #include <asm/pgalloc.h>
@@ -927,6 +928,11 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 #ifdef CONFIG_MEMCG
 	tsk->active_memcg = NULL;
 #endif
+
+#ifdef CONFIG_KTSAN
+	tsk->ktsan.task = NULL;
+#endif
+
 	return tsk;
 
 free_stack:
@@ -2300,6 +2306,7 @@ struct task_struct *fork_idle(int cpu)
 	task = copy_process(CLONE_VM, 0, 0, NULL, NULL, &init_struct_pid, 0, 0,
 			    cpu_to_node(cpu));
 	if (!IS_ERR(task)) {
+		ktsan_task_create(&task->ktsan, task->pid);
 		init_idle_pids(task);
 		init_idle(task, cpu);
 	}
@@ -2356,6 +2363,8 @@ long _do_fork(unsigned long clone_flags,
 	if (IS_ERR(p))
 		return PTR_ERR(p);
 
+	ktsan_task_create(&p->ktsan, p->pid);
+
 	/*
 	 * Do this prior waking up the new thread - the thread pointer
 	 * might get invalid after that point, if the thread exits quickly.
diff --git a/kernel/locking/mutex.c b/kernel/locking/mutex.c
index 0c601ae072b3..7a791c02c85e 100644
--- a/kernel/locking/mutex.c
+++ b/kernel/locking/mutex.c
@@ -29,6 +29,7 @@
 #include <linux/interrupt.h>
 #include <linux/debug_locks.h>
 #include <linux/osq_lock.h>
+#include <linux/ktsan.h>
 
 #ifdef CONFIG_DEBUG_MUTEXES
 # include "mutex-debug.h"
@@ -39,6 +40,7 @@
 void
 __mutex_init(struct mutex *lock, const char *name, struct lock_class_key *key)
 {
+	ktsan_thr_event_disable();
 	atomic_long_set(&lock->owner, 0);
 	spin_lock_init(&lock->wait_lock);
 	INIT_LIST_HEAD(&lock->wait_list);
@@ -47,6 +49,7 @@ __mutex_init(struct mutex *lock, const char *name, struct lock_class_key *key)
 #endif
 
 	debug_mutex_init(lock, name, key);
+	ktsan_thr_event_enable();
 }
 EXPORT_SYMBOL(__mutex_init);
 
@@ -254,8 +257,10 @@ void __sched mutex_lock(struct mutex *lock)
 {
 	might_sleep();
 
+	ktsan_mtx_pre_lock(lock, true, false);
 	if (!__mutex_trylock_fast(lock))
 		__mutex_lock_slowpath(lock);
+	ktsan_mtx_post_lock(lock, true, false, true);
 }
 EXPORT_SYMBOL(mutex_lock);
 #endif
@@ -678,7 +683,9 @@ mutex_optimistic_spin(struct mutex *lock, struct ww_acquire_ctx *ww_ctx,
 		 * we do not, make it so, otherwise we might get stuck.
 		 */
 		__set_current_state(TASK_RUNNING);
+		ktsan_mtx_post_lock(lock, true, false, false);
 		schedule_preempt_disabled();
+		ktsan_mtx_pre_lock(lock, true, false);
 	}
 
 	return false;
@@ -707,11 +714,16 @@ static noinline void __sched __mutex_unlock_slowpath(struct mutex *lock, unsigne
  */
 void __sched mutex_unlock(struct mutex *lock)
 {
+	ktsan_mtx_pre_unlock(lock, true);
+
 #ifndef CONFIG_DEBUG_LOCK_ALLOC
-	if (__mutex_unlock_fast(lock))
+	if (__mutex_unlock_fast(lock)) {
+		ktsan_mtx_post_unlock(lock, true);
 		return;
+	}
 #endif
 	__mutex_unlock_slowpath(lock, _RET_IP_);
+	ktsan_mtx_post_unlock(lock, true);
 }
 EXPORT_SYMBOL(mutex_unlock);
 
@@ -906,7 +918,9 @@ __mutex_lock_common(struct mutex *lock, long state, unsigned int subclass,
 	struct ww_mutex *ww;
 	int ret;
 
+	ktsan_mtx_post_lock(lock, true, false, false);
 	might_sleep();
+	ktsan_mtx_pre_lock(lock, true, false);
 
 	ww = container_of(lock, struct ww_mutex, base);
 	if (use_ww_ctx && ww_ctx) {
@@ -1000,7 +1014,9 @@ __mutex_lock_common(struct mutex *lock, long state, unsigned int subclass,
 		}
 
 		spin_unlock(&lock->wait_lock);
+		ktsan_mtx_post_lock(lock, true, false, false);
 		schedule_preempt_disabled();
+		ktsan_mtx_pre_lock(lock, true, false);
 
 		/*
 		 * ww_mutex needs to always recheck its position since its waiter
@@ -1275,12 +1291,18 @@ __mutex_lock_interruptible_slowpath(struct mutex *lock);
  */
 int __sched mutex_lock_interruptible(struct mutex *lock)
 {
+	int ret;
+
 	might_sleep();
+	ktsan_mtx_pre_lock(lock, true, false);
 
-	if (__mutex_trylock_fast(lock))
+	if (__mutex_trylock_fast(lock)) {
+		ktsan_mtx_post_lock(lock, true, false, true);
 		return 0;
-
-	return __mutex_lock_interruptible_slowpath(lock);
+	}
+	ret = __mutex_lock_interruptible_slowpath(lock);
+	ktsan_mtx_post_lock(lock, true, false, !ret);
+	return ret;
 }
 
 EXPORT_SYMBOL(mutex_lock_interruptible);
@@ -1299,12 +1321,19 @@ EXPORT_SYMBOL(mutex_lock_interruptible);
  */
 int __sched mutex_lock_killable(struct mutex *lock)
 {
+	int ret;
+
 	might_sleep();
+	ktsan_mtx_pre_lock(lock, true, false);
 
-	if (__mutex_trylock_fast(lock))
+	if (__mutex_trylock_fast(lock)) {
+		ktsan_mtx_post_lock(lock, true, false, true);
 		return 0;
+	}
 
-	return __mutex_lock_killable_slowpath(lock);
+	ret = __mutex_lock_killable_slowpath(lock);
+	ktsan_mtx_post_lock(lock, true, false, !ret);
+	return ret;
 }
 EXPORT_SYMBOL(mutex_lock_killable);
 
@@ -1379,10 +1408,13 @@ __ww_mutex_lock_interruptible_slowpath(struct ww_mutex *lock,
  */
 int __sched mutex_trylock(struct mutex *lock)
 {
-	bool locked = __mutex_trylock(lock);
+	bool locked;
 
+	ktsan_mtx_pre_lock(lock, true, true);
+	locked = __mutex_trylock(lock);
 	if (locked)
 		mutex_acquire(&lock->dep_map, 0, 1, _RET_IP_);
+	ktsan_mtx_post_lock(lock, true, true, locked == 1);
 
 	return locked;
 }
diff --git a/kernel/locking/rwsem-xadd.c b/kernel/locking/rwsem-xadd.c
index 0b1f77957240..f1ab8ff5030e 100644
--- a/kernel/locking/rwsem-xadd.c
+++ b/kernel/locking/rwsem-xadd.c
@@ -76,6 +76,7 @@
 void __init_rwsem(struct rw_semaphore *sem, const char *name,
 		  struct lock_class_key *key)
 {
+	ktsan_thr_event_disable();
 #ifdef CONFIG_DEBUG_LOCK_ALLOC
 	/*
 	 * Make sure we are not reinitializing a held semaphore:
@@ -90,6 +91,7 @@ void __init_rwsem(struct rw_semaphore *sem, const char *name,
 	sem->owner = NULL;
 	osq_lock_init(&sem->osq);
 #endif
+	ktsan_thr_event_enable();
 }
 
 EXPORT_SYMBOL(__init_rwsem);
@@ -440,12 +442,13 @@ __rwsem_down_read_failed_common(struct rw_semaphore *sem, int state)
 {
 	long count, adjustment = -RWSEM_ACTIVE_READ_BIAS;
 	struct rwsem_waiter waiter;
+	unsigned long flags;
 	DEFINE_WAKE_Q(wake_q);
 
 	waiter.task = current;
 	waiter.type = RWSEM_WAITING_FOR_READ;
 
-	raw_spin_lock_irq(&sem->wait_lock);
+	raw_spin_lock_irqsave(&sem->wait_lock, flags);
 	if (list_empty(&sem->wait_list)) {
 		/*
 		 * In case the wait queue is empty and the lock isn't owned
@@ -454,7 +457,7 @@ __rwsem_down_read_failed_common(struct rw_semaphore *sem, int state)
 		 * been set in the count.
 		 */
 		if (atomic_long_read(&sem->count) >= 0) {
-			raw_spin_unlock_irq(&sem->wait_lock);
+			raw_spin_unlock_irqrestore(&sem->wait_lock, flags);
 			rwsem_set_reader_owned(sem);
 			lockevent_inc(rwsem_rlock_fast);
 			return sem;
@@ -477,7 +480,7 @@ __rwsem_down_read_failed_common(struct rw_semaphore *sem, int state)
 	     adjustment != -RWSEM_ACTIVE_READ_BIAS))
 		__rwsem_mark_wake(sem, RWSEM_WAKE_ANY, &wake_q);
 
-	raw_spin_unlock_irq(&sem->wait_lock);
+	raw_spin_unlock_irqrestore(&sem->wait_lock, flags);
 	wake_up_q(&wake_q);
 
 	/* wait to be given the lock */
@@ -486,13 +489,15 @@ __rwsem_down_read_failed_common(struct rw_semaphore *sem, int state)
 		if (!waiter.task)
 			break;
 		if (signal_pending_state(state, current)) {
-			raw_spin_lock_irq(&sem->wait_lock);
+			raw_spin_lock_irqsave(&sem->wait_lock, flags);
 			if (waiter.task)
 				goto out_nolock;
-			raw_spin_unlock_irq(&sem->wait_lock);
+			raw_spin_unlock_irqrestore(&sem->wait_lock, flags);
 			break;
 		}
+		ktsan_mtx_post_lock(sem, false, false, false);
 		schedule();
+		ktsan_mtx_pre_lock(sem, false, false);
 		lockevent_inc(rwsem_sleep_reader);
 	}
 
@@ -503,7 +508,7 @@ __rwsem_down_read_failed_common(struct rw_semaphore *sem, int state)
 	list_del(&waiter.list);
 	if (list_empty(&sem->wait_list))
 		atomic_long_add(-RWSEM_WAITING_BIAS, &sem->count);
-	raw_spin_unlock_irq(&sem->wait_lock);
+	raw_spin_unlock_irqrestore(&sem->wait_lock, flags);
 	__set_current_state(TASK_RUNNING);
 	lockevent_inc(rwsem_rlock_fail);
 	return ERR_PTR(-EINTR);
@@ -533,6 +538,7 @@ __rwsem_down_write_failed_common(struct rw_semaphore *sem, int state)
 	bool waiting = true; /* any queued threads before us */
 	struct rwsem_waiter waiter;
 	struct rw_semaphore *ret = sem;
+	unsigned long flags;
 	DEFINE_WAKE_Q(wake_q);
 
 	/* undo write bias from down_write operation, stop active locking */
@@ -549,7 +555,7 @@ __rwsem_down_write_failed_common(struct rw_semaphore *sem, int state)
 	waiter.task = current;
 	waiter.type = RWSEM_WAITING_FOR_WRITE;
 
-	raw_spin_lock_irq(&sem->wait_lock);
+	raw_spin_lock_irqsave(&sem->wait_lock, flags);
 
 	/* account for this before adding a new element to the list */
 	if (list_empty(&sem->wait_list))
@@ -591,19 +597,21 @@ __rwsem_down_write_failed_common(struct rw_semaphore *sem, int state)
 	while (true) {
 		if (rwsem_try_write_lock(count, sem))
 			break;
-		raw_spin_unlock_irq(&sem->wait_lock);
+		raw_spin_unlock_irqrestore(&sem->wait_lock, flags);
 
 		/* Block until there are no active lockers. */
 		do {
 			if (signal_pending_state(state, current))
 				goto out_nolock;
 
+			ktsan_mtx_post_lock(sem, true, false, false);
 			schedule();
+			ktsan_mtx_pre_lock(sem, true, false);
 			lockevent_inc(rwsem_sleep_writer);
 			set_current_state(state);
 		} while ((count = atomic_long_read(&sem->count)) & RWSEM_ACTIVE_MASK);
 
-		raw_spin_lock_irq(&sem->wait_lock);
+		raw_spin_lock_irqsave(&sem->wait_lock, flags);
 	}
 	__set_current_state(TASK_RUNNING);
 	list_del(&waiter.list);
@@ -614,13 +622,13 @@ __rwsem_down_write_failed_common(struct rw_semaphore *sem, int state)
 
 out_nolock:
 	__set_current_state(TASK_RUNNING);
-	raw_spin_lock_irq(&sem->wait_lock);
+	raw_spin_lock_irqsave(&sem->wait_lock, flags);
 	list_del(&waiter.list);
 	if (list_empty(&sem->wait_list))
 		atomic_long_add(-RWSEM_WAITING_BIAS, &sem->count);
 	else
 		__rwsem_mark_wake(sem, RWSEM_WAKE_ANY, &wake_q);
-	raw_spin_unlock_irq(&sem->wait_lock);
+	raw_spin_unlock_irqrestore(&sem->wait_lock, flags);
 	wake_up_q(&wake_q);
 	lockevent_inc(rwsem_wlock_fail);
 
diff --git a/kernel/locking/rwsem.c b/kernel/locking/rwsem.c
index ccbf18f560ff..af6a23597236 100644
--- a/kernel/locking/rwsem.c
+++ b/kernel/locking/rwsem.c
@@ -11,6 +11,8 @@
 #include <linux/sched/debug.h>
 #include <linux/export.h>
 #include <linux/rwsem.h>
+#include <linux/ktsan.h>
+
 #include <linux/atomic.h>
 
 #include "rwsem.h"
@@ -21,9 +23,11 @@
 void __sched down_read(struct rw_semaphore *sem)
 {
 	might_sleep();
+	ktsan_mtx_pre_lock(sem, false, false);
 	rwsem_acquire_read(&sem->dep_map, 0, 0, _RET_IP_);
 
 	LOCK_CONTENDED(sem, __down_read_trylock, __down_read);
+	ktsan_mtx_post_lock(sem, false, false, true);
 }
 
 EXPORT_SYMBOL(down_read);
@@ -31,13 +35,16 @@ EXPORT_SYMBOL(down_read);
 int __sched down_read_killable(struct rw_semaphore *sem)
 {
 	might_sleep();
+	ktsan_mtx_pre_lock(sem, false, false);
 	rwsem_acquire_read(&sem->dep_map, 0, 0, _RET_IP_);
 
 	if (LOCK_CONTENDED_RETURN(sem, __down_read_trylock, __down_read_killable)) {
 		rwsem_release(&sem->dep_map, 1, _RET_IP_);
+		ktsan_mtx_post_lock(sem, false, false, false);
 		return -EINTR;
 	}
 
+	ktsan_mtx_post_lock(sem, false, false, true);
 	return 0;
 }
 
@@ -48,10 +55,14 @@ EXPORT_SYMBOL(down_read_killable);
  */
 int down_read_trylock(struct rw_semaphore *sem)
 {
-	int ret = __down_read_trylock(sem);
+	int ret;
+
+	ktsan_mtx_pre_lock(sem, false, true);
+	ret = __down_read_trylock(sem);
 
 	if (ret == 1)
 		rwsem_acquire_read(&sem->dep_map, 0, 1, _RET_IP_);
+	ktsan_mtx_post_lock(sem, false, true, ret == 1);
 	return ret;
 }
 
@@ -63,9 +74,11 @@ EXPORT_SYMBOL(down_read_trylock);
 void __sched down_write(struct rw_semaphore *sem)
 {
 	might_sleep();
+	ktsan_mtx_pre_lock(sem, true, false);
 	rwsem_acquire(&sem->dep_map, 0, 0, _RET_IP_);
 
 	LOCK_CONTENDED(sem, __down_write_trylock, __down_write);
+	ktsan_mtx_post_lock(sem, true, false, true);
 }
 
 EXPORT_SYMBOL(down_write);
@@ -76,13 +89,16 @@ EXPORT_SYMBOL(down_write);
 int __sched down_write_killable(struct rw_semaphore *sem)
 {
 	might_sleep();
+	ktsan_mtx_pre_lock(sem, true, false);
 	rwsem_acquire(&sem->dep_map, 0, 0, _RET_IP_);
 
 	if (LOCK_CONTENDED_RETURN(sem, __down_write_trylock, __down_write_killable)) {
 		rwsem_release(&sem->dep_map, 1, _RET_IP_);
+		ktsan_mtx_post_lock(sem, true, false, false);
 		return -EINTR;
 	}
 
+	ktsan_mtx_post_lock(sem, true, false, true);
 	return 0;
 }
 
@@ -93,10 +109,14 @@ EXPORT_SYMBOL(down_write_killable);
  */
 int down_write_trylock(struct rw_semaphore *sem)
 {
-	int ret = __down_write_trylock(sem);
+	int ret;
+
+	ktsan_mtx_pre_lock(sem, true, true);
+	ret = __down_write_trylock(sem);
 
 	if (ret == 1)
 		rwsem_acquire(&sem->dep_map, 0, 1, _RET_IP_);
+	ktsan_mtx_post_lock(sem, true, true, ret == 1);
 
 	return ret;
 }
@@ -108,9 +128,11 @@ EXPORT_SYMBOL(down_write_trylock);
  */
 void up_read(struct rw_semaphore *sem)
 {
+	ktsan_mtx_pre_unlock(sem, false);
 	rwsem_release(&sem->dep_map, 1, _RET_IP_);
 
 	__up_read(sem);
+	ktsan_mtx_post_unlock(sem, false);
 }
 
 EXPORT_SYMBOL(up_read);
@@ -120,9 +142,11 @@ EXPORT_SYMBOL(up_read);
  */
 void up_write(struct rw_semaphore *sem)
 {
+	ktsan_mtx_pre_unlock(sem, true);
 	rwsem_release(&sem->dep_map, 1, _RET_IP_);
 
 	__up_write(sem);
+	ktsan_mtx_post_unlock(sem, true);
 }
 
 EXPORT_SYMBOL(up_write);
@@ -132,6 +156,7 @@ EXPORT_SYMBOL(up_write);
  */
 void downgrade_write(struct rw_semaphore *sem)
 {
+	ktsan_mtx_downgrade(sem);
 	lock_downgrade(&sem->dep_map, _RET_IP_);
 
 	__downgrade_write(sem);
@@ -144,9 +169,11 @@ EXPORT_SYMBOL(downgrade_write);
 void down_read_nested(struct rw_semaphore *sem, int subclass)
 {
 	might_sleep();
+	ktsan_mtx_pre_lock(sem, false, false);
 	rwsem_acquire_read(&sem->dep_map, subclass, 0, _RET_IP_);
 
 	LOCK_CONTENDED(sem, __down_read_trylock, __down_read);
+	ktsan_mtx_post_lock(sem, false, false, true);
 }
 
 EXPORT_SYMBOL(down_read_nested);
@@ -154,9 +181,11 @@ EXPORT_SYMBOL(down_read_nested);
 void _down_write_nest_lock(struct rw_semaphore *sem, struct lockdep_map *nest)
 {
 	might_sleep();
+	ktsan_mtx_pre_lock(sem, true, false);
 	rwsem_acquire_nest(&sem->dep_map, 0, 0, nest, _RET_IP_);
 
 	LOCK_CONTENDED(sem, __down_write_trylock, __down_write);
+	ktsan_mtx_post_lock(sem, true, false, true);
 }
 
 EXPORT_SYMBOL(_down_write_nest_lock);
@@ -165,8 +194,10 @@ void down_read_non_owner(struct rw_semaphore *sem)
 {
 	might_sleep();
 
+	ktsan_mtx_pre_lock(sem, false, false);
 	__down_read(sem);
 	__rwsem_set_reader_owned(sem, NULL);
+	ktsan_mtx_post_lock(sem, false, false, true);
 }
 
 EXPORT_SYMBOL(down_read_non_owner);
@@ -174,9 +205,11 @@ EXPORT_SYMBOL(down_read_non_owner);
 void down_write_nested(struct rw_semaphore *sem, int subclass)
 {
 	might_sleep();
+	ktsan_mtx_pre_lock(sem, true, false);
 	rwsem_acquire(&sem->dep_map, subclass, 0, _RET_IP_);
 
 	LOCK_CONTENDED(sem, __down_write_trylock, __down_write);
+	ktsan_mtx_post_lock(sem, true, false, true);
 }
 
 EXPORT_SYMBOL(down_write_nested);
@@ -184,13 +217,16 @@ EXPORT_SYMBOL(down_write_nested);
 int __sched down_write_killable_nested(struct rw_semaphore *sem, int subclass)
 {
 	might_sleep();
+	ktsan_mtx_pre_lock(sem, true, false);
 	rwsem_acquire(&sem->dep_map, subclass, 0, _RET_IP_);
 
 	if (LOCK_CONTENDED_RETURN(sem, __down_write_trylock, __down_write_killable)) {
 		rwsem_release(&sem->dep_map, 1, _RET_IP_);
+		ktsan_mtx_post_lock(sem, true, false, false);
 		return -EINTR;
 	}
 
+	ktsan_mtx_post_lock(sem, true, false, true);
 	return 0;
 }
 
@@ -200,7 +236,9 @@ void up_read_non_owner(struct rw_semaphore *sem)
 {
 	DEBUG_RWSEMS_WARN_ON(!((unsigned long)sem->owner & RWSEM_READER_OWNED),
 				sem);
+	ktsan_mtx_pre_unlock(sem, false);
 	__up_read(sem);
+	ktsan_mtx_post_unlock(sem, false);
 }
 
 EXPORT_SYMBOL(up_read_non_owner);
diff --git a/kernel/locking/semaphore.c b/kernel/locking/semaphore.c
index d9dd94defc0a..246f2f33c967 100644
--- a/kernel/locking/semaphore.c
+++ b/kernel/locking/semaphore.c
@@ -54,12 +54,17 @@ void down(struct semaphore *sem)
 {
 	unsigned long flags;
 
+	// Are semaphores used for critical section guarding?
+	// It seems semaphores are not owned by a single thread. Any thread
+	// can up/down it and KTSAN seems confused by it.
+	// ktsan_mtx_pre_lock(sem, true, false);
 	raw_spin_lock_irqsave(&sem->lock, flags);
 	if (likely(sem->count > 0))
 		sem->count--;
 	else
 		__down(sem);
 	raw_spin_unlock_irqrestore(&sem->lock, flags);
+	// ktsan_mtx_post_lock(sem, true, false, true);
 }
 EXPORT_SYMBOL(down);
 
@@ -77,12 +82,14 @@ int down_interruptible(struct semaphore *sem)
 	unsigned long flags;
 	int result = 0;
 
+	// ktsan_mtx_pre_lock(sem, true, false);
 	raw_spin_lock_irqsave(&sem->lock, flags);
 	if (likely(sem->count > 0))
 		sem->count--;
 	else
 		result = __down_interruptible(sem);
 	raw_spin_unlock_irqrestore(&sem->lock, flags);
+	// ktsan_mtx_post_lock(sem, true, false, true);
 
 	return result;
 }
@@ -103,12 +110,14 @@ int down_killable(struct semaphore *sem)
 	unsigned long flags;
 	int result = 0;
 
+	// ktsan_mtx_pre_lock(sem, true, false);
 	raw_spin_lock_irqsave(&sem->lock, flags);
 	if (likely(sem->count > 0))
 		sem->count--;
 	else
 		result = __down_killable(sem);
 	raw_spin_unlock_irqrestore(&sem->lock, flags);
+	// ktsan_mtx_post_lock(sem, true, false, true);
 
 	return result;
 }
@@ -132,11 +141,13 @@ int down_trylock(struct semaphore *sem)
 	unsigned long flags;
 	int count;
 
+	// ktsan_mtx_pre_lock(sem, true, false);
 	raw_spin_lock_irqsave(&sem->lock, flags);
 	count = sem->count - 1;
 	if (likely(count >= 0))
 		sem->count = count;
 	raw_spin_unlock_irqrestore(&sem->lock, flags);
+	// ktsan_mtx_post_lock(sem, true, false, count >= 0);
 
 	return (count < 0);
 }
@@ -157,12 +168,14 @@ int down_timeout(struct semaphore *sem, long timeout)
 	unsigned long flags;
 	int result = 0;
 
+	// ktsan_mtx_pre_lock(sem, true, false);
 	raw_spin_lock_irqsave(&sem->lock, flags);
 	if (likely(sem->count > 0))
 		sem->count--;
 	else
 		result = __down_timeout(sem, timeout);
 	raw_spin_unlock_irqrestore(&sem->lock, flags);
+	// ktsan_mtx_post_lock(sem, true, false, result == 0);
 
 	return result;
 }
@@ -179,12 +192,14 @@ void up(struct semaphore *sem)
 {
 	unsigned long flags;
 
+	// ktsan_mtx_pre_unlock(sem, true);
 	raw_spin_lock_irqsave(&sem->lock, flags);
 	if (likely(list_empty(&sem->wait_list)))
 		sem->count++;
 	else
 		__up(sem);
 	raw_spin_unlock_irqrestore(&sem->lock, flags);
+	// ktsan_mtx_post_unlock(sem, true);
 }
 EXPORT_SYMBOL(up);
 
@@ -217,7 +232,9 @@ static inline int __sched __down_common(struct semaphore *sem, long state,
 			goto timed_out;
 		__set_current_state(state);
 		raw_spin_unlock_irq(&sem->lock);
+		// ktsan_mtx_post_lock(sem, true, false, false);
 		timeout = schedule_timeout(timeout);
+		// ktsan_mtx_pre_lock(sem, true, false);
 		raw_spin_lock_irq(&sem->lock);
 		if (waiter.up)
 			return 0;
diff --git a/kernel/pid.c b/kernel/pid.c
index e5cad0c7d5dd..cb277540d3a5 100644
--- a/kernel/pid.c
+++ b/kernel/pid.c
@@ -108,6 +108,11 @@ void put_pid(struct pid *pid)
 	ns = pid->numbers[pid->level].ns;
 	if ((atomic_read(&pid->count) == 1) ||
 	     atomic_dec_and_test(&pid->count)) {
+#ifndef CONFIG_KTSAN
+		/* Turn atomic_read into load acquire since ktsan
+		   doesn't understand control dependency. */
+		rmb();
+#endif /* CONFIG_KTSAN */
 		kmem_cache_free(ns->pid_cachep, pid);
 		put_pid_ns(ns);
 	}
diff --git a/kernel/rcu/tree.c b/kernel/rcu/tree.c
index 980ca3ca643f..429380cfd443 100644
--- a/kernel/rcu/tree.c
+++ b/kernel/rcu/tree.c
@@ -51,6 +51,7 @@
 #include <linux/tick.h>
 #include <linux/sysrq.h>
 #include <linux/kprobes.h>
+#include <linux/ktsan.h>
 
 #include "tree.h"
 #include "rcu.h"
@@ -2089,6 +2090,14 @@ static void rcu_do_batch(struct rcu_data *rdp)
 	rhp = rcu_cblist_dequeue(&rcl);
 	for (; rhp; rhp = rcu_cblist_dequeue(&rcl)) {
 		debug_rcu_head_unqueue(rhp);
+
+		/* call_rcu is defined to be call_rcu_sched
+		   in the current kernel configuration. */
+		ktsan_sync_acquire(&ktsan_glob_sync[
+			ktsan_glob_sync_type_rcu_common]);
+		ktsan_sync_acquire(&ktsan_glob_sync[
+			ktsan_glob_sync_type_rcu_sched]);
+
 		if (__rcu_reclaim(rcu_state.name, rhp))
 			rcu_cblist_dequeued_lazy(&rcl);
 		/*
@@ -2565,6 +2574,7 @@ void synchronize_rcu(void)
 		synchronize_rcu_expedited();
 	else
 		wait_rcu_gp(call_rcu);
+	ktsan_sync_acquire(&ktsan_glob_sync[ktsan_glob_sync_type_rcu_common]);
 }
 EXPORT_SYMBOL_GPL(synchronize_rcu);
 
diff --git a/kernel/rcu/tree_plugin.h b/kernel/rcu/tree_plugin.h
index 1102765f91fd..4050c05881a1 100644
--- a/kernel/rcu/tree_plugin.h
+++ b/kernel/rcu/tree_plugin.h
@@ -13,6 +13,7 @@
 
 #include <linux/delay.h>
 #include <linux/gfp.h>
+#include <linux/ktsan.h>
 #include <linux/oom.h>
 #include <linux/sched/debug.h>
 #include <linux/smpboot.h>
diff --git a/kernel/sched/Makefile b/kernel/sched/Makefile
index 21fb5a5662b5..2585fff4a1ad 100644
--- a/kernel/sched/Makefile
+++ b/kernel/sched/Makefile
@@ -1,4 +1,7 @@
 # SPDX-License-Identifier: GPL-2.0
+
+KTSAN_SANITIZE := n
+
 ifdef CONFIG_FUNCTION_TRACER
 CFLAGS_REMOVE_clock.o = $(CC_FLAGS_FTRACE)
 endif
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 874c427742a9..ae373acf9805 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -11,6 +11,7 @@
 #include <linux/nospec.h>
 
 #include <linux/kcov.h>
+#include <linux/ktsan.h>
 
 #include <asm/switch_to.h>
 #include <asm/tlb.h>
@@ -2696,9 +2697,18 @@ static struct rq *finish_task_switch(struct task_struct *prev)
 		 */
 		kprobe_flush_task(prev);
 
+		/* Synchronize to the previous thread since it might
+		   have accessed prev struct, see comment in __schedule.
+		   Since ktsan_sync_acquire is ignored when called from
+		   scheduler, use fake start/stop to prevent this. */
+		ktsan_task_start();
+		ktsan_sync_acquire(prev);
+		ktsan_task_stop();
+
+		ktsan_task_destroy(&prev->ktsan);
+
 		/* Task is done with its stack. */
 		put_task_stack(prev);
-
 		put_task_struct(prev);
 	}
 
@@ -2762,9 +2772,15 @@ asmlinkage __visible void schedule_tail(struct task_struct *prev)
 	 */
 
 	rq = finish_task_switch(prev);
+
 	balance_callback(rq);
 	preempt_enable();
 
+	/* Must be before put_user call because the latter can cause
+	   page fault, which might enter scheduler, which results in
+	   two consequent ktsan_task_start calls. */
+	ktsan_task_start();
+
 	if (current->set_child_tid)
 		put_user(task_pid_vnr(current), current->set_child_tid);
 
@@ -2779,7 +2795,6 @@ context_switch(struct rq *rq, struct task_struct *prev,
 	       struct task_struct *next, struct rq_flags *rf)
 {
 	struct mm_struct *mm, *oldmm;
-
 	prepare_task_switch(rq, prev, next);
 
 	mm = next->mm;
@@ -3378,6 +3393,12 @@ static void __sched notrace __schedule(bool preempt)
 	rq = cpu_rq(cpu);
 	prev = rq->curr;
 
+	/* The next thread might access prev structure while deleting it,
+	   but the synchronization that come from scheduler is not seen by
+	   ktsan, so we manually synchronize the next and prev threads. */
+	ktsan_sync_release(prev);
+	ktsan_task_stop();
+
 	schedule_debug(prev);
 
 	if (sched_feat(HRTICK))
@@ -3449,6 +3470,7 @@ static void __sched notrace __schedule(bool preempt)
 	}
 
 	balance_callback(rq);
+	ktsan_task_start();
 }
 
 void __noreturn do_task_dead(void)
diff --git a/kernel/time/hrtimer.c b/kernel/time/hrtimer.c
index 41dfff23c1f9..901ab8e90ad7 100644
--- a/kernel/time/hrtimer.c
+++ b/kernel/time/hrtimer.c
@@ -1318,8 +1318,10 @@ bool hrtimer_active(const struct hrtimer *timer)
 		seq = raw_read_seqcount_begin(&base->seq);
 
 		if (timer->state != HRTIMER_STATE_INACTIVE ||
-		    base->running == timer)
+		    base->running == timer) {
+			read_seqcount_cancel(&base->seq);
 			return true;
+		}
 
 	} while (read_seqcount_retry(&base->seq, seq) ||
 		 base != READ_ONCE(timer->base));
diff --git a/kernel/workqueue.c b/kernel/workqueue.c
index 95aea04ff722..a09d34ca2baa 100644
--- a/kernel/workqueue.c
+++ b/kernel/workqueue.c
@@ -50,6 +50,7 @@
 #include <linux/uaccess.h>
 #include <linux/sched/isolation.h>
 #include <linux/nmi.h>
+#include <linux/ktsan.h>
 
 #include "workqueue_internal.h"
 
@@ -1941,6 +1942,9 @@ static struct worker *create_worker(struct worker_pool *pool)
 	spin_lock_irq(&pool->lock);
 	worker->pool->nr_workers++;
 	worker_enter_idle(worker);
+
+	ktsan_sync_release(worker);
+
 	wake_up_process(worker->task);
 	spin_unlock_irq(&pool->lock);
 
@@ -2359,6 +2363,8 @@ static int worker_thread(void *__worker)
 	struct worker *worker = __worker;
 	struct worker_pool *pool = worker->pool;
 
+	ktsan_sync_acquire(worker);
+
 	/* tell the scheduler that this is a workqueue worker */
 	set_pf_worker(true);
 woke_up:
diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
index cbdfae379896..09f1d0a0dc7f 100644
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -775,6 +775,7 @@ config DEBUG_STACKOVERFLOW
 	  If in doubt, say "N".
 
 source "lib/Kconfig.kasan"
+source "lib/Kconfig.ktsan"
 
 endmenu # "Memory Debugging"
 
diff --git a/lib/Kconfig.ktsan b/lib/Kconfig.ktsan
new file mode 100644
index 000000000000..cd58db25e930
--- /dev/null
+++ b/lib/Kconfig.ktsan
@@ -0,0 +1,10 @@
+config HAVE_ARCH_KTSAN
+	bool
+
+if HAVE_ARCH_KTSAN
+
+config KTSAN
+	bool "ThreadSanitizer: dynamic data race error detector"
+	default n
+
+endif
diff --git a/mm/Makefile b/mm/Makefile
index ac5e5ba78874..5968fe71d341 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -104,3 +104,4 @@ obj-$(CONFIG_HARDENED_USERCOPY) += usercopy.o
 obj-$(CONFIG_PERCPU_STATS) += percpu-stats.o
 obj-$(CONFIG_HMM) += hmm.o
 obj-$(CONFIG_MEMFD_CREATE) += memfd.o
+obj-$(CONFIG_KTSAN)		+= ktsan/
diff --git a/mm/filemap.c b/mm/filemap.c
index 6dd9a2274c80..f143dc3a7e6d 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -944,6 +944,8 @@ struct page *__page_cache_alloc(gfp_t gfp)
 			n = cpuset_mem_spread_node();
 			page = __alloc_pages_node(n, gfp, 0);
 		} while (!page && read_mems_allowed_retry(cpuset_mems_cookie));
+		if (page)
+			read_mems_allowed_cancel();
 
 		return page;
 	}
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index ede7e7f5d1ab..1907b323329a 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -917,8 +917,10 @@ static struct page *dequeue_huge_page_nodemask(struct hstate *h, gfp_t gfp_mask,
 		node = zone_to_nid(zone);
 
 		page = dequeue_huge_page_node_exact(h, node);
-		if (page)
+		if (page) {
+			read_mems_allowed_cancel();
 			return page;
+		}
 	}
 	if (unlikely(read_mems_allowed_retry(cpuset_mems_cookie)))
 		goto retry_cpuset;
diff --git a/mm/ktsan/Makefile b/mm/ktsan/Makefile
new file mode 100644
index 000000000000..50b03ab3ad2c
--- /dev/null
+++ b/mm/ktsan/Makefile
@@ -0,0 +1,85 @@
+# SPDX-License-Identifier: GPL-2.0
+obj-y := access.o 		\
+	 cache.o 		\
+	 clk.o 			\
+	 func.o 		\
+	 ktsan.o 		\
+	 memblock.o 		\
+	 mutexset.o 		\
+	 report.o 		\
+	 shadow.o 		\
+	 spinlock.o		\
+	 stack.o 		\
+	 stack_depot.o 		\
+	 stat.o 		\
+	 supp.o			\
+	 sync_atomic.o 		\
+	 sync_atomic_no_ktsan.o \
+	 sync_mtx.o 		\
+	 sync_percpu.o 		\
+	 sync_seqlock.o 	\
+	 sync.o 		\
+	 tab.o 			\
+	 tests_inst.o 		\
+	 tests_noinst.o 	\
+	 tests.o 		\
+	 thr.o 			\
+	 trace.o 		\
+
+CFLAGS_REMOVE_access.o			= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_cache.o			= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_clk.o			= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_func.o			= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_ktsan.o			= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_memblock.o		= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_mutexset.o		= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_report.o			= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_shadow.o			= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_spinlock.o                = -pg -O0 -O1 -O2
+CFLAGS_REMOVE_stack.o			= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_stack_depot.o		= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_stat.o			= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_supp.o			= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_sync_atomic.o		= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_sync_atomic_no_ktsan.o	= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_sync_mtx.o		= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_sync_percpu.o		= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_sync_seqlock.o		= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_sync.o			= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_tab.o			= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_tests_inst.o		= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_tests_noinst.o		= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_tests.o			= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_thr.o			= -pg -O0 -O1 -O2
+CFLAGS_REMOVE_trace.o			= -pg -O0 -O1 -O2
+
+CFLAGS_access.o			= -O3 -fomit-frame-pointer
+CFLAGS_cache.o			= -O3 -fomit-frame-pointer
+CFLAGS_clk.o			= -O3 -fomit-frame-pointer
+CFLAGS_func.o			= -O3 -fomit-frame-pointer
+CFLAGS_ktsan.o			= -O3 -fomit-frame-pointer
+CFLAGS_memblock.o		= -O3 -fomit-frame-pointer
+CFLAGS_mutexset.o		= -O3 -fomit-frame-pointer
+CFLAGS_report.o			= -O3 -fomit-frame-pointer
+CFLAGS_shadow.o			= -O3 -fomit-frame-pointer
+CFLAGS_spinlock.o               = -O3 -fomit-frame-pointer
+CFLAGS_stack.o			= -O3 -fomit-frame-pointer
+CFLAGS_stack_depot.o		= -O3 -fomit-frame-pointer
+CFLAGS_stat.o			= -O3 -fomit-frame-pointer
+CFLAGS_supp.o			= -O3 -fomit-frame-pointer
+CFLAGS_sync_atomic.o		= -O3 -fomit-frame-pointer
+CFLAGS_sync_atomic_no_ktsan.o	= -O3 -fomit-frame-pointer
+CFLAGS_sync_mtx.o		= -O3 -fomit-frame-pointer
+CFLAGS_sync_percpu.o		= -O3 -fomit-frame-pointer
+CFLAGS_sync_seqlock.o		= -O3 -fomit-frame-pointer
+CFLAGS_sync.o			= -O3 -fomit-frame-pointer
+CFLAGS_tab.o			= -O3 -fomit-frame-pointer
+CFLAGS_tests_inst.o		= -O3 -fomit-frame-pointer
+CFLAGS_tests_noinst.o		= -O3 -fomit-frame-pointer
+CFLAGS_tests.o			= -O3 -fomit-frame-pointer
+CFLAGS_thr.o			= -O3 -fomit-frame-pointer
+CFLAGS_trace.o			= -O3 -fomit-frame-pointer
+
+KTSAN_SANITIZE := n
+KTSAN_SANITIZE_tests_inst.o = y
+KCOV_INSTRUMENT := n
diff --git a/mm/ktsan/access.c b/mm/ktsan/access.c
new file mode 100644
index 000000000000..9b9665d11dab
--- /dev/null
+++ b/mm/ktsan/access.c
@@ -0,0 +1,218 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+#include <linux/kernel.h>
+#include <linux/mm_types.h>
+#include <linux/mm.h>
+#include <linux/printk.h>
+#include <linux/sched.h>
+
+static __always_inline
+bool ranges_intersect(int first_offset, int first_size, int second_offset,
+	int second_size)
+{
+	if (first_offset + first_size <= second_offset)
+		return false;
+
+	if (second_offset + second_size <= first_offset)
+		return false;
+
+	return true;
+}
+
+static __always_inline
+bool update_one_shadow_slot(kt_thr_t *thr, uptr_t addr, kt_shadow_t *slot,
+	kt_shadow_t value, bool stored)
+{
+	kt_race_info_t info;
+	kt_shadow_t old;
+	u64 raw;
+
+	raw = kt_atomic64_load_no_ktsan(slot);
+	if (raw == 0) {
+		if (!stored)
+			kt_atomic64_store_no_ktsan(slot,
+				KT_SHADOW_TO_LONG(value));
+		return true;
+	}
+	old = *(kt_shadow_t *)&raw;
+
+	/* Is the memory access equal to the previous? */
+	if (value.offset == old.offset && value.size == old.size) {
+		/* Same thread? */
+		if (likely(value.tid == old.tid)) {
+			/* TODO. */
+			return false;
+		}
+
+		/* Happens-before? */
+		if (likely(kt_clk_get(&thr->clk, old.tid) >= old.clock)) {
+			if (!stored)
+				kt_atomic64_store_no_ktsan(slot,
+					KT_SHADOW_TO_LONG(value));
+			return true;
+		}
+
+		if (likely(old.read && value.read))
+			return false;
+		if (likely(old.atomic && value.atomic))
+			return false;
+
+		goto report_race;
+	}
+
+	/* Do the memory accesses intersect? */
+	if (unlikely(ranges_intersect(old.offset, (1 << old.size),
+			     value.offset, (1 << value.size)))) {
+		if (old.tid == value.tid)
+			return false;
+		if (old.read && value.read)
+			return false;
+		if (kt_clk_get(&thr->clk, old.tid) >= old.clock)
+			return false;
+		if (likely(old.atomic && value.atomic))
+			return false;
+
+report_race:
+		info.addr = addr;
+		info.old = old;
+		info.new = value;
+		kt_report_race(thr, &info);
+
+		return true;
+	}
+
+	return false;
+}
+
+static __always_inline
+void kt_access_impl(kt_thr_t *thr, kt_shadow_t *slots, kt_time_t current_clock,
+		uptr_t addr, size_t size, bool read, bool atomic)
+{
+	kt_shadow_t value;
+	int i;
+	bool stored;
+
+	kt_stat_inc(read ? kt_stat_access_read : kt_stat_access_write);
+	kt_stat_inc(kt_stat_access_size1 + size);
+
+	value.tid = thr->id;
+	value.clock = current_clock;
+	value.offset = addr & (KT_GRAIN - 1);
+	value.size = size;
+	value.read = read;
+	value.atomic = atomic;
+
+	stored = false;
+	for (i = 0; i < KT_SHADOW_SLOTS; i++)
+		stored |= update_one_shadow_slot(thr, addr, &slots[i],
+						 value, stored);
+
+	/*pr_err("thread: %d, addr: %lx, size: %u, read: %d, stored: %d\n",
+		 (int)thr->id, addr, (int)size, (int)read, stored);*/
+
+	if (!stored) {
+		/* Evict random shadow slot. */
+		kt_atomic64_store_no_ktsan(
+			&slots[current_clock % KT_SHADOW_SLOTS],
+			KT_SHADOW_TO_LONG(value));
+	}
+}
+
+/*
+   Size might be 0, 1, 2 or 3 and equals to the binary logarithm
+   of the actual access size.
+*/
+void kt_access(kt_thr_t *thr, uptr_t pc, uptr_t addr,
+		size_t size, bool read, bool atomic)
+{
+	kt_time_t current_clock;
+	kt_shadow_t *slots;
+
+	if (read && thr->read_disable_depth)
+		return;
+
+	slots = kt_shadow_get(addr);
+	if (unlikely(!slots))
+		return; /* FIXME? */
+
+	kt_trace_add_event(thr, kt_event_mop, kt_compress(pc));
+	current_clock = kt_clk_get(&thr->clk, thr->id);
+
+	kt_access_impl(thr, slots, current_clock, addr, size, read, atomic);
+}
+
+void kt_access_range(kt_thr_t *thr, uptr_t pc, uptr_t addr,
+			size_t size, bool read)
+{
+	kt_time_t current_clock;
+	kt_shadow_t *slots;
+
+	BUG_ON(size == 0);
+	if (read && thr->read_disable_depth)
+		return;
+
+	slots = kt_shadow_get(addr);
+	if (unlikely(!slots))
+		return; /* FIXME? */
+
+	kt_trace_add_event(thr, kt_event_mop, kt_compress(pc));
+	current_clock = kt_clk_get(&thr->clk, thr->id);
+
+	/* Handle unaligned beginning, if any. */
+	if (addr & (KT_GRAIN - 1)) {
+		for (; (addr & (KT_GRAIN - 1)) && size; addr++, size--)
+			kt_access_impl(thr, slots, current_clock, addr,
+				KT_ACCESS_SIZE_1, read, false);
+		slots += KT_SHADOW_SLOTS;
+	}
+
+	/* Handle middle part, if any. */
+	for (; size >= KT_GRAIN; addr += KT_GRAIN, size -= KT_GRAIN) {
+		kt_access_impl(thr, slots, current_clock, addr,
+			KT_ACCESS_SIZE_8, read, false);
+		slots += KT_SHADOW_SLOTS;
+	}
+
+	/* Handle ending, if any. */
+	for (; size; addr++, size--)
+		kt_access_impl(thr, slots, current_clock, addr,
+			KT_ACCESS_SIZE_1, read, false);
+}
+
+void kt_access_range_imitate(kt_thr_t *thr, uptr_t pc, uptr_t addr,
+				size_t size, bool read)
+{
+	kt_time_t current_clock;
+	kt_shadow_t value;
+	kt_shadow_t *slots;
+	int i;
+
+	/* Currently it is called only from kt_memblock_alloc, so the address
+	 * and size must be multiple of KT_GRAIN. */
+	BUG_ON((addr & (KT_GRAIN - 1)) != 0);
+	BUG_ON((size & (KT_GRAIN - 1)) != 0);
+
+	slots = kt_shadow_get(addr);
+	if (!slots)
+		return; /* FIXME? */
+
+	kt_trace_add_event(thr, kt_event_mop, kt_compress(pc));
+	current_clock = kt_clk_get(&thr->clk, thr->id);
+
+	/* Below we assume that access size 8 covers whole grain. */
+	BUG_ON(KT_GRAIN != (1 << KT_ACCESS_SIZE_8));
+
+	value.tid = thr->id;
+	value.clock = current_clock;
+	value.offset = 0;
+	value.size = KT_ACCESS_SIZE_8;
+	value.read = read;
+	value.atomic = false;
+
+	for (; size; size -= KT_GRAIN) {
+		for (i = 0; i < KT_SHADOW_SLOTS; i++, slots++)
+			kt_atomic64_store_no_ktsan(slots,
+				i ? 0 : KT_SHADOW_TO_LONG(value));
+	}
+}
diff --git a/mm/ktsan/cache.c b/mm/ktsan/cache.c
new file mode 100644
index 000000000000..af734e0dd228
--- /dev/null
+++ b/mm/ktsan/cache.c
@@ -0,0 +1,64 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+#include <linux/gfp.h>
+#include <linux/memblock.h>
+#include <linux/mm.h>
+#include <linux/mm_types.h>
+#include <linux/spinlock.h>
+
+/* Only available during early boot. */
+void __init kt_cache_init(kt_cache_t *cache, size_t obj_size,
+			  size_t obj_max_num)
+{
+	phys_addr_t paddr;
+	uptr_t obj;
+	int i;
+
+	BUG_ON(obj_size == 0);
+	BUG_ON(obj_max_num == 0);
+
+	obj_size = round_up(obj_size, sizeof(unsigned long));
+	cache->mem_size = obj_size * obj_max_num;
+
+	paddr = memblock_phys_alloc(cache->mem_size, PAGE_SIZE);
+	BUG_ON(paddr == 0);
+	cache->base = (uptr_t)(phys_to_virt(paddr));
+
+	for (i = 0, obj = cache->base; i < obj_max_num-1; i++, obj += obj_size)
+		*(void **)obj = (void *)(obj + obj_size);
+	*(void **)obj = NULL;
+
+	cache->head = (void *)cache->base;
+	kt_spin_init(&cache->lock);
+}
+
+/* Only available during early boot. */
+void __init kt_cache_destroy(kt_cache_t *cache)
+{
+	int rv;
+
+	rv = memblock_free(cache->base, cache->mem_size);
+	BUG_ON(rv == 0);
+}
+
+void *kt_cache_alloc(kt_cache_t *cache)
+{
+	void *obj;
+
+	kt_spin_lock(&cache->lock);
+	obj = cache->head;
+	if (obj)
+		cache->head = *(void **)obj;
+	kt_spin_unlock(&cache->lock);
+
+	return obj;
+}
+
+void kt_cache_free(kt_cache_t *cache, void *obj)
+{
+	kt_spin_lock(&cache->lock);
+	*(void **)obj = cache->head;
+	cache->head = obj;
+	kt_spin_unlock(&cache->lock);
+}
diff --git a/mm/ktsan/clk.c b/mm/ktsan/clk.c
new file mode 100644
index 000000000000..773a74a02f7e
--- /dev/null
+++ b/mm/ktsan/clk.c
@@ -0,0 +1,25 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+#include <linux/kernel.h>
+
+void kt_clk_init(kt_clk_t *clk)
+{
+	memset(clk, 0, sizeof(*clk));
+}
+
+void kt_clk_acquire(kt_clk_t *dst, kt_clk_t *src)
+{
+	int i;
+
+	for (i = 0; i < KT_MAX_THREAD_COUNT; i++)
+		dst->time[i] = max(dst->time[i], src->time[i]);
+}
+
+void kt_clk_set(kt_clk_t *dst, kt_clk_t *src)
+{
+	int i;
+
+	for (i = 0; i < KT_MAX_THREAD_COUNT; i++)
+		dst->time[i] = src->time[i];
+}
diff --git a/mm/ktsan/func.c b/mm/ktsan/func.c
new file mode 100644
index 000000000000..df650cec3194
--- /dev/null
+++ b/mm/ktsan/func.c
@@ -0,0 +1,35 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+#include <linux/preempt.h>
+
+static void kt_tame_acquire_and_release_clocks(kt_thr_t *thr)
+{
+	if (thr->acquire_active)
+		thr->acquire_active--;
+
+	if (thr->release_active)
+		thr->release_active--;
+}
+
+void kt_func_entry(kt_thr_t *thr, uptr_t pc)
+{
+	kt_stat_inc(kt_stat_func_entry);
+
+	kt_trace_add_event(thr, kt_event_func_enter, kt_compress(pc));
+
+	kt_stack_push(&thr->stack, pc);
+
+	kt_tame_acquire_and_release_clocks(thr);
+}
+
+void kt_func_exit(kt_thr_t *thr)
+{
+	kt_stat_inc(kt_stat_func_exit);
+
+	kt_trace_add_event(thr, kt_event_func_exit, 0);
+
+	kt_stack_pop(&thr->stack);
+
+	kt_tame_acquire_and_release_clocks(thr);
+}
diff --git a/mm/ktsan/ktsan.c b/mm/ktsan/ktsan.c
new file mode 100644
index 000000000000..a8e759654f34
--- /dev/null
+++ b/mm/ktsan/ktsan.c
@@ -0,0 +1,1132 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+#include <linux/atomic.h>
+#include <linux/irqflags.h>
+#include <linux/kernel.h>
+#include <linux/nmi.h>
+#include <linux/preempt.h>
+#include <linux/printk.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+
+int ktsan_glob_sync[ktsan_glob_sync_type_count];
+EXPORT_SYMBOL(ktsan_glob_sync);
+
+kt_ctx_t kt_ctx;
+
+static inline kt_cpu_t *kt_current_cpu(void)
+{
+	return this_cpu_ptr(kt_ctx.cpus);
+}
+
+static inline kt_task_t *kt_current_task(void)
+{
+	KT_BUG_ON(!current);
+	return current->ktsan.task;
+}
+
+#define DISABLE_INTERRUPTS(flags)	\
+	preempt_disable();		\
+	flags = arch_local_irq_save();	\
+	stop_nmi()			\
+/**/
+
+#define ENABLE_INTERRUPTS(flags)	\
+	restart_nmi();			\
+	arch_local_irq_restore(flags);	\
+	preempt_enable()		\
+/**/
+
+#define IN_INTERRUPT()			\
+	(in_nmi())			\
+/**/
+
+/* Nothing special. */
+#define KT_ENTER_NORMAL		0
+/* Handle events that come from the scheduler internals. */
+#define KT_ENTER_SCHED		(1<<0)
+/* Handle events even if events were disabled with ktsan_disable(). */
+#define KT_ENTER_DISABLED	(1<<1)
+
+#define ENTER(enter_flags)						\
+	kt_task_t *task;						\
+	kt_thr_t *thr;							\
+	uptr_t pc;							\
+	unsigned long kt_flags;						\
+	bool event_handled;						\
+									\
+	thr = NULL;							\
+	event_handled = false;						\
+									\
+	DISABLE_INTERRUPTS(kt_flags);					\
+									\
+	if (unlikely(!kt_ctx.enabled))					\
+		goto exit;						\
+									\
+	/* Ignore NMIs for now. */					\
+	if (unlikely(IN_INTERRUPT()))					\
+		goto exit;						\
+									\
+	task = kt_current_task();					\
+	if (unlikely(!task))						\
+		goto exit;						\
+									\
+	if (unlikely(!task->running &&					\
+			!((enter_flags) & KT_ENTER_SCHED))) 		\
+		goto exit;						\
+									\
+	thr = task->thr;						\
+	KT_BUG_ON(!thr);						\
+									\
+	if (unlikely(thr->event_disable_depth != 0 &&			\
+			!((enter_flags) & KT_ENTER_DISABLED)))		\
+		goto exit;						\
+									\
+	if (unlikely(__test_and_set_bit(0, &thr->inside)))		\
+		goto exit;						\
+									\
+	pc = (uptr_t)_RET_IP_;						\
+	event_handled = true;						\
+/**/
+
+#define LEAVE()								\
+	KT_BUG_ON(task != current->ktsan.task);				\
+	if (unlikely(!__test_and_clear_bit(0, &thr->inside)))		\
+		KT_BUG_ON(1);						\
+									\
+exit:									\
+	KT_BUG_ON(thr && event_handled && thr->inside);			\
+	ENABLE_INTERRUPTS(kt_flags);					\
+/**/
+
+void __init ktsan_init_early(void)
+{
+	kt_ctx_t *ctx = &kt_ctx;
+
+	memset(ctx, 0, sizeof(*ctx));
+	kt_tab_init(&ctx->sync_tab, KT_SYNC_TAB_SIZE,
+		    sizeof(kt_tab_sync_t), KT_MAX_SYNC_COUNT);
+	kt_tab_init(&ctx->memblock_tab, KT_MEMBLOCK_TAB_SIZE,
+		    sizeof(kt_tab_memblock_t), KT_MAX_MEMBLOCK_COUNT);
+	kt_tab_init(&ctx->test_tab, 13, sizeof(kt_tab_test_t), 20);
+
+	kt_cache_init(&ctx->percpu_sync_cache,
+		      sizeof(kt_percpu_sync_t), KT_MAX_PERCPU_SYNC_COUNT);
+	kt_cache_init(&ctx->task_cache, sizeof(kt_task_t), KT_MAX_TASK_COUNT);
+
+	kt_thr_pool_init();
+
+	kt_stack_depot_init(&ctx->stack_depot);
+}
+
+static void ktsan_report_memory_usage(void)
+{
+	u64 sync_tab_mem = KT_SYNC_TAB_SIZE * sizeof(kt_tab_part_t);
+	u64 sync_cache_mem = KT_MAX_SYNC_COUNT * sizeof(kt_tab_sync_t);
+	u64 sync_total_mem = sync_tab_mem + sync_cache_mem;
+
+	u64 memblock_tab_mem = KT_MEMBLOCK_TAB_SIZE * sizeof(kt_tab_part_t);
+	u64 memblock_cache_mem = KT_MAX_MEMBLOCK_COUNT *
+					sizeof(kt_tab_memblock_t);
+	u64 memblock_total_mem = memblock_tab_mem + memblock_cache_mem;
+
+	u64 percpu_sync_cache_mem = KT_MAX_PERCPU_SYNC_COUNT *
+					sizeof(kt_percpu_sync_t);
+	u64 task_cache_mem = KT_MAX_TASK_COUNT * sizeof(kt_task_t);
+
+	u64 thr_cache_mem = KT_MAX_THREAD_COUNT * sizeof(kt_thr_t);
+
+	u64 depot_objs_mem = KT_STACK_DEPOT_MEMORY_LIMIT;
+
+	u64 total_mem = sync_total_mem + memblock_total_mem + task_cache_mem +
+		percpu_sync_cache_mem + thr_cache_mem + depot_objs_mem;
+
+	pr_err("ktsan memory usage: %llu GB + shadow.\n", total_mem >> 30);
+	pr_err("             syncs: %llu MB \n", sync_total_mem >> 20);
+	pr_err("          memblock: %llu MB \n", memblock_total_mem >> 20);
+	pr_err("      percpu syncs: %llu MB\n", percpu_sync_cache_mem >> 20);
+	pr_err("             tasks: %llu MB\n", task_cache_mem >> 20);
+	pr_err("      thrs (trace): %llu MB\n", thr_cache_mem >> 20);
+	pr_err("       stack depot: %llu MB\n", depot_objs_mem >> 20);
+}
+
+void ktsan_init(void)
+{
+	kt_ctx_t *ctx;
+	kt_cpu_t *cpu;
+	kt_task_t *task;
+	kt_thr_t *thr;
+	int inside, i;
+
+	ctx = &kt_ctx;
+
+	ctx->cpus = alloc_percpu(kt_cpu_t);
+	for_each_possible_cpu(i) {
+		cpu = per_cpu_ptr(ctx->cpus, i);
+		memset(cpu, 0, sizeof(*cpu));
+	}
+
+	thr = kt_thr_create(NULL, current->pid);
+	task = kt_cache_alloc(&kt_ctx.task_cache);
+	task->thr = thr;
+	current->ktsan.task = task;
+
+	BUG_ON(ctx->enabled);
+	inside = __test_and_set_bit(0, &thr->inside);
+	BUG_ON(inside != 0);
+
+	kt_stat_init();
+	kt_supp_init();
+	kt_tests_init();
+
+	inside = __test_and_clear_bit(0, &thr->inside);
+	BUG_ON(!inside);
+	ctx->enabled = 1;
+
+	pr_err("KTSAN: enabled\n");
+	ktsan_report_memory_usage();
+}
+
+void ktsan_print_diagnostics(void)
+{
+	ENTER(KT_ENTER_DISABLED);
+	LEAVE();
+
+	pr_err("# # # # # # # # # # ktsan diagnostics # # # # # # # # # #\n");
+	pr_err("\n");
+
+#if KT_DEBUG_TRACE
+	if (thr != NULL) {
+		kt_time_t time;
+
+		time = kt_clk_get(&thr->clk, thr->id);
+		pr_err("Trace:\n");
+		kt_trace_dump(&thr->trace, (time - 512) % KT_TRACE_SIZE, time);
+		pr_err("\n");
+	}
+#endif /* KT_DEBUG_TRACE */
+
+	pr_err("Runtime:\n");
+	pr_err(" runtime active:                %s\n", event_handled ? "+" : "-");
+	if (!event_handled) {
+		pr_err(" kt_ctx.enabled:                %s\n",
+			(kt_ctx.enabled) ? "+" : "-");
+		pr_err(" !IN_INTERRUPT():               %s\n",
+			(!IN_INTERRUPT()) ? "+" : "-");
+		pr_err(" current:                       %s\n",
+			(current) ? "+" : "-");
+		pr_err(" current->ktsan.task:           %s\n",
+			(current->ktsan.task) ? "+" : "-");
+		if (thr != NULL) {
+			pr_err(" thr->event_disable_depth == 0: %s\n",
+				(thr->event_disable_depth == 0) ? "+" : "-");
+			pr_err(" thr->cpu != NULL:              %s\n",
+				(thr->cpu != NULL) ? "+" : "-");
+		}
+	}
+
+	pr_err("\n");
+
+	if (thr != NULL) {
+		pr_err("Thread:\n");
+		pr_err(" thr->id:                    %d\n", thr->id);
+		pr_err(" thr->pid:                   %d\n", thr->pid);
+		pr_err(" thr->inside:                %d\n",
+			kt_atomic32_load_no_ktsan((void *)&thr->inside));
+		pr_err(" thr->report_disable_depth:  %d\n",
+			thr->report_disable_depth);
+		pr_err(" thr->preempt_disable_depth: %d\n",
+			thr->preempt_disable_depth);
+		pr_err(" thr->event_disable_depth:   %d\n",
+			thr->event_disable_depth);
+		pr_err(" thr->irqs_disabled:         %s\n",
+			thr->irqs_disabled ? "+" : "-");
+		pr_err("\n");
+	}
+	pr_err("\n");
+
+#if KT_DEBUG
+	if (thr != NULL) {
+		kt_trace_state_t state;
+
+		pr_err("Last event disable:\n");
+		kt_trace_restore_state(thr,
+				thr->last_event_disable_time, &state);
+		kt_stack_print(&state.stack, 0);
+		pr_err("\n");
+
+		pr_err("Last event enable:\n");
+		kt_trace_restore_state(thr,
+				thr->last_event_enable_time, &state);
+		kt_stack_print(&state.stack, 0);
+		pr_err("\n");
+	}
+#endif /* KT_DEBUG */
+
+	pr_err("# # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n");
+}
+
+/* FIXME(xairy): not sure if this is the best place for this
+   function, but it requires access to ENTER and LEAVE. */
+void kt_tests_run(void)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_tests_run_noinst();
+	LEAVE();
+	kt_tests_run_inst();
+}
+
+void ktsan_interrupt_enter(void)
+{
+	/* Switch to a dedicated stack for interrupts. This helps to keep stack
+	 * depot memory consumption sane. If we would glue interrupted thread
+	 * and interrupt stacks together, we will have to constantly save new
+	 * stacks in stack depot.
+	 */
+	ENTER(KT_ENTER_NORMAL);
+	if (thr->interrupt_depth++ == 0)
+		kt_thr_interrupt(thr, pc, &thr->cpu->interrupted);
+	LEAVE();
+}
+
+void ktsan_interrupt_exit(void)
+{
+	ENTER(KT_ENTER_NORMAL);
+	if (--thr->interrupt_depth == 0)
+		kt_thr_resume(thr, pc, &thr->cpu->interrupted);
+	BUG_ON(thr->interrupt_depth < 0);
+	LEAVE();
+}
+
+void ktsan_syscall_enter(void)
+{
+	/* Does nothing for now. */
+}
+
+void ktsan_syscall_exit(void)
+{
+	/* Does nothing for now. */
+}
+
+void ktsan_cpu_start(void)
+{
+	/* Does nothing for now. */
+}
+
+void ktsan_task_create(struct ktsan_task_s *new, int pid)
+{
+	ENTER(KT_ENTER_SCHED | KT_ENTER_DISABLED);
+	new->task = kt_cache_alloc(&kt_ctx.task_cache);
+	new->task->thr = kt_thr_create(thr, pid);
+	new->task->running = false;
+	LEAVE();
+}
+
+void ktsan_task_destroy(struct ktsan_task_s *old)
+{
+	ENTER(KT_ENTER_SCHED | KT_ENTER_DISABLED);
+	kt_thr_destroy(thr, old->task->thr);
+	old->task->thr = NULL;
+	kt_cache_free(&kt_ctx.task_cache, old->task);
+	LEAVE();
+}
+
+void ktsan_task_start(void)
+{
+	ENTER(KT_ENTER_SCHED | KT_ENTER_DISABLED);
+	BUG_ON(task->running);
+	task->running = true;
+	kt_thr_start(thr, pc);
+	LEAVE();
+}
+
+void ktsan_task_stop(void)
+{
+	ENTER(KT_ENTER_SCHED | KT_ENTER_DISABLED);
+	BUG_ON(thr->interrupt_depth); /* Context switch during an interrupt? */
+	BUG_ON(!task->running);
+	task->running = false;
+	kt_thr_stop(thr, pc);
+	LEAVE();
+}
+
+void ktsan_thr_event_disable(void)
+{
+	ENTER(KT_ENTER_DISABLED);
+	kt_thr_event_disable(thr, pc, &kt_flags);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_thr_event_disable);
+
+void ktsan_thr_event_enable(void)
+{
+	ENTER(KT_ENTER_DISABLED);
+	kt_thr_event_enable(thr, pc, &kt_flags);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_thr_event_enable);
+
+void ktsan_thr_report_disable(void)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_thr_report_disable(thr);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_thr_report_disable);
+
+void ktsan_thr_report_enable(void)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_thr_report_enable(thr);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_thr_report_enable);
+
+void ktsan_sync_acquire(void *addr)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_sync_acquire(thr, pc, (uptr_t)addr);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_sync_acquire);
+
+void ktsan_sync_release(void *addr)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_sync_release(thr, pc, (uptr_t)addr);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_sync_release);
+
+static
+void ktsan_memblock_alloc(void *addr, unsigned long size, bool write_to_shadow)
+{
+	ENTER(KT_ENTER_DISABLED);
+	BUG_ON(thr->event_disable_depth != 0);
+	kt_memblock_alloc(thr, pc, (uptr_t)addr, (size_t)size, write_to_shadow);
+	LEAVE();
+}
+
+void ktsan_memblock_free(void *addr, unsigned long size, bool write_to_shadow)
+{
+	ENTER(KT_ENTER_DISABLED);
+	BUG_ON(thr->event_disable_depth != 0);
+	kt_memblock_free(thr, pc, (uptr_t)addr, (size_t)size, write_to_shadow);
+	LEAVE();
+}
+
+void ktsan_slab_alloc(void *addr, unsigned long size, unsigned long flags)
+{
+	ktsan_memblock_alloc(addr, size, !(flags & SLAB_TYPESAFE_BY_RCU));
+}
+
+void ktsan_slab_free(void *addr, unsigned long size, unsigned long flags)
+{
+	ktsan_memblock_free(addr, size, !(flags & SLAB_TYPESAFE_BY_RCU));
+}
+
+void ktsan_mtx_pre_lock(void *addr, bool write, bool try)
+{
+	ENTER(KT_ENTER_DISABLED);
+
+	if (kt_thr_event_disable(thr, pc, &kt_flags))
+		kt_mtx_pre_lock(thr, pc, (uptr_t)addr, write, try);
+
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_mtx_pre_lock);
+
+void ktsan_mtx_post_lock(void *addr, bool write, bool try, bool success)
+{
+	ENTER(KT_ENTER_DISABLED);
+
+	if (kt_thr_event_enable(thr, pc, &kt_flags))
+		kt_mtx_post_lock(thr, pc, (uptr_t)addr, write, try, success);
+
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_mtx_post_lock);
+
+void ktsan_mtx_pre_unlock(void *addr, bool write)
+{
+	ENTER(KT_ENTER_DISABLED);
+
+	if (kt_thr_event_disable(thr, pc, &kt_flags))
+		kt_mtx_pre_unlock(thr, pc, (uptr_t)addr, write);
+
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_mtx_pre_unlock);
+
+void ktsan_mtx_post_unlock(void *addr, bool write)
+{
+	ENTER(KT_ENTER_DISABLED);
+
+	if (kt_thr_event_enable(thr, pc, &kt_flags))
+		kt_mtx_post_unlock(thr, pc, (uptr_t)addr, write);
+
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_mtx_post_unlock);
+
+void ktsan_mtx_downgrade(void *addr)
+{
+	ENTER(KT_ENTER_DISABLED);
+	kt_mtx_downgrade(thr, pc, (uptr_t)addr);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_mtx_downgrade);
+
+void ktsan_seqcount_begin(const void *s)
+{
+	ENTER(KT_ENTER_DISABLED);
+	kt_seqcount_begin(thr, pc, (uptr_t)s);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_seqcount_begin);
+
+void ktsan_seqcount_end(const void *s)
+{
+	ENTER(KT_ENTER_DISABLED);
+	kt_seqcount_end(thr, pc, (uptr_t)s);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_seqcount_end);
+
+
+void ktsan_seqcount_ignore_begin(void)
+{
+	ENTER(KT_ENTER_DISABLED);
+	kt_seqcount_ignore_begin(thr, pc);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_seqcount_ignore_begin);
+
+void ktsan_seqcount_ignore_end(void)
+{
+	ENTER(KT_ENTER_DISABLED);
+	kt_seqcount_ignore_end(thr, pc);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_seqcount_ignore_end);
+
+void ktsan_thread_fence(ktsan_memory_order_t mo)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_thread_fence(thr, pc, mo);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_thread_fence);
+
+void ktsan_atomic8_store(void *addr, u8 value, ktsan_memory_order_t mo)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_atomic8_store(thr, pc, addr, value, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic8_store_no_ktsan(addr, value);
+}
+EXPORT_SYMBOL(ktsan_atomic8_store);
+
+void ktsan_atomic16_store(void *addr, u16 value, ktsan_memory_order_t mo)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_atomic16_store(thr, pc, addr, value, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic16_store_no_ktsan(addr, value);
+}
+EXPORT_SYMBOL(ktsan_atomic16_store);
+
+void ktsan_atomic32_store(void *addr, u32 value, ktsan_memory_order_t mo)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_atomic32_store(thr, pc, addr, value, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic32_store_no_ktsan(addr, value);
+}
+EXPORT_SYMBOL(ktsan_atomic32_store);
+
+void ktsan_atomic64_store(void *addr, u64 value, ktsan_memory_order_t mo)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_atomic64_store(thr, pc, addr, value, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic64_store_no_ktsan(addr, value);
+}
+EXPORT_SYMBOL(ktsan_atomic64_store);
+
+u8 ktsan_atomic8_load(const void *addr, ktsan_memory_order_t mo)
+{
+	u8 rv = 0;
+
+	ENTER(KT_ENTER_NORMAL);
+	rv = kt_atomic8_load(thr, pc, addr, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic8_load_no_ktsan(addr);
+	return rv;
+}
+EXPORT_SYMBOL(ktsan_atomic8_load);
+
+u16 ktsan_atomic16_load(const void *addr, ktsan_memory_order_t mo)
+{
+	u16 rv = 0;
+
+	ENTER(KT_ENTER_NORMAL);
+	rv = kt_atomic16_load(thr, pc, addr, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic16_load_no_ktsan(addr);
+	return rv;
+}
+EXPORT_SYMBOL(ktsan_atomic16_load);
+
+u32 ktsan_atomic32_load(const void *addr, ktsan_memory_order_t mo)
+{
+	u32 rv = 0;
+
+	ENTER(KT_ENTER_NORMAL);
+	rv = kt_atomic32_load(thr, pc, addr, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic32_load_no_ktsan(addr);
+	return rv;
+}
+EXPORT_SYMBOL(ktsan_atomic32_load);
+
+u64 ktsan_atomic64_load(const void *addr, ktsan_memory_order_t mo)
+{
+	u64 rv = 0;
+
+	ENTER(KT_ENTER_NORMAL);
+	rv = kt_atomic64_load(thr, pc, addr, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic64_load_no_ktsan(addr);
+	return rv;
+}
+EXPORT_SYMBOL(ktsan_atomic64_load);
+
+u8 ktsan_atomic8_exchange(void *addr, u8 value, ktsan_memory_order_t mo)
+{
+	u8 rv = 0;
+
+	ENTER(KT_ENTER_NORMAL);
+	rv = kt_atomic8_exchange(thr, pc, addr, value, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic8_exchange_no_ktsan(addr, value);
+	return rv;
+}
+EXPORT_SYMBOL(ktsan_atomic8_exchange);
+
+u16 ktsan_atomic16_exchange(void *addr, u16 value, ktsan_memory_order_t mo)
+{
+	u16 rv = 0;
+
+	ENTER(KT_ENTER_NORMAL);
+	rv = kt_atomic16_exchange(thr, pc, addr, value, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic16_exchange_no_ktsan(addr, value);
+	return rv;
+}
+EXPORT_SYMBOL(ktsan_atomic16_exchange);
+
+u32 ktsan_atomic32_exchange(void *addr, u32 value, ktsan_memory_order_t mo)
+{
+	u32 rv = 0;
+
+	ENTER(KT_ENTER_NORMAL);
+	rv = kt_atomic32_exchange(thr, pc, addr, value, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic32_exchange_no_ktsan(addr, value);
+	return rv;
+}
+EXPORT_SYMBOL(ktsan_atomic32_exchange);
+
+u64 ktsan_atomic64_exchange(void *addr, u64 value, ktsan_memory_order_t mo)
+{
+	u64 rv = 0;
+
+	ENTER(KT_ENTER_NORMAL);
+	rv = kt_atomic64_exchange(thr, pc, addr, value, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic64_exchange_no_ktsan(addr, value);
+	return rv;
+}
+EXPORT_SYMBOL(ktsan_atomic64_exchange);
+
+u8 ktsan_atomic8_compare_exchange(void *addr, u8 old, u8 new,
+					ktsan_memory_order_t mo)
+{
+	u8 rv = 0;
+
+	ENTER(KT_ENTER_NORMAL);
+	rv = kt_atomic8_compare_exchange(thr, pc, addr, old, new, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic8_compare_exchange_no_ktsan(addr, old, new);
+	return rv;
+}
+EXPORT_SYMBOL(ktsan_atomic8_compare_exchange);
+
+u16 ktsan_atomic16_compare_exchange(void *addr, u16 old, u16 new,
+					ktsan_memory_order_t mo)
+{
+	u16 rv = 0;
+
+	ENTER(KT_ENTER_NORMAL);
+	rv = kt_atomic16_compare_exchange(thr, pc, addr, old, new, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic16_compare_exchange_no_ktsan(addr, old, new);
+	return rv;
+}
+EXPORT_SYMBOL(ktsan_atomic16_compare_exchange);
+
+u32 ktsan_atomic32_compare_exchange(void *addr, u32 old, u32 new,
+					ktsan_memory_order_t mo)
+{
+	u32 rv = 0;
+
+	ENTER(KT_ENTER_NORMAL);
+	rv = kt_atomic32_compare_exchange(thr, pc, addr, old, new, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic32_compare_exchange_no_ktsan(addr, old, new);
+	return rv;
+}
+EXPORT_SYMBOL(ktsan_atomic32_compare_exchange);
+
+u64 ktsan_atomic64_compare_exchange(void *addr, u64 old, u64 new,
+					ktsan_memory_order_t mo)
+{
+	u64 rv = 0;
+
+	ENTER(KT_ENTER_NORMAL);
+	rv = kt_atomic64_compare_exchange(thr, pc, addr, old, new, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic64_compare_exchange_no_ktsan(addr, old, new);
+	return rv;
+}
+EXPORT_SYMBOL(ktsan_atomic64_compare_exchange);
+
+u8 ktsan_atomic8_fetch_add(void *addr, u8 value, ktsan_memory_order_t mo)
+{
+	u8 rv = 0;
+
+	ENTER(KT_ENTER_NORMAL);
+	rv = kt_atomic8_fetch_add(thr, pc, addr, value, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic8_fetch_add_no_ktsan(addr, value);
+	return rv;
+}
+EXPORT_SYMBOL(ktsan_atomic8_fetch_add);
+
+u16 ktsan_atomic16_fetch_add(void *addr, u16 value, ktsan_memory_order_t mo)
+{
+	u16 rv = 0;
+
+	ENTER(KT_ENTER_NORMAL);
+	rv = kt_atomic16_fetch_add(thr, pc, addr, value, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic16_fetch_add_no_ktsan(addr, value);
+	return rv;
+}
+EXPORT_SYMBOL(ktsan_atomic16_fetch_add);
+
+u32 ktsan_atomic32_fetch_add(void *addr, u32 value, ktsan_memory_order_t mo)
+{
+	u32 rv = 0;
+
+	ENTER(KT_ENTER_NORMAL);
+	rv = kt_atomic32_fetch_add(thr, pc, addr, value, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic32_fetch_add_no_ktsan(addr, value);
+	return rv;
+}
+EXPORT_SYMBOL(ktsan_atomic32_fetch_add);
+
+u64 ktsan_atomic64_fetch_add(void *addr, u64 value, ktsan_memory_order_t mo)
+{
+	u64 rv = 0;
+
+	ENTER(KT_ENTER_NORMAL);
+	rv = kt_atomic64_fetch_add(thr, pc, addr, value, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic64_fetch_add_no_ktsan(addr, value);
+	return rv;
+}
+EXPORT_SYMBOL(ktsan_atomic64_fetch_add);
+
+void ktsan_atomic_set_bit(void *addr, long nr, ktsan_memory_order_t mo)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_atomic_set_bit(thr, pc, addr, nr, mo);
+	LEAVE();
+
+	if (!event_handled)
+		kt_atomic_set_bit_no_ktsan(addr, nr);
+}
+EXPORT_SYMBOL(ktsan_atomic_set_bit);
+
+void ktsan_atomic_clear_bit(void *addr, long nr, ktsan_memory_order_t mo)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_atomic_clear_bit(thr, pc, addr, nr, mo);
+	LEAVE();
+
+	if (!event_handled)
+		kt_atomic_clear_bit_no_ktsan(addr, nr);
+}
+EXPORT_SYMBOL(ktsan_atomic_clear_bit);
+
+void ktsan_atomic_change_bit(void *addr, long nr, ktsan_memory_order_t mo)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_atomic_change_bit(thr, pc, addr, nr, mo);
+	LEAVE();
+
+	if (!event_handled)
+		kt_atomic_change_bit_no_ktsan(addr, nr);
+}
+EXPORT_SYMBOL(ktsan_atomic_change_bit);
+
+int ktsan_atomic_fetch_set_bit(void *addr, long nr, ktsan_memory_order_t mo)
+{
+	int rv = 0;
+
+	ENTER(KT_ENTER_NORMAL);
+	rv = kt_atomic_fetch_set_bit(thr, pc, addr, nr, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic_fetch_set_bit_no_ktsan(addr, nr);
+	return rv;
+}
+EXPORT_SYMBOL(ktsan_atomic_fetch_set_bit);
+
+int ktsan_atomic_fetch_clear_bit(void *addr, long nr, ktsan_memory_order_t mo)
+{
+	int rv = 0;
+
+	ENTER(KT_ENTER_NORMAL);
+	rv = kt_atomic_fetch_clear_bit(thr, pc, addr, nr, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic_fetch_clear_bit_no_ktsan(addr, nr);
+	return rv;
+}
+EXPORT_SYMBOL(ktsan_atomic_fetch_clear_bit);
+
+int ktsan_atomic_fetch_change_bit(void *addr, long nr, ktsan_memory_order_t mo)
+{
+	int rv = 0;
+
+	ENTER(KT_ENTER_NORMAL);
+	rv = kt_atomic_fetch_change_bit(thr, pc, addr, nr, mo);
+	LEAVE();
+
+	if (!event_handled)
+		return kt_atomic_fetch_change_bit_no_ktsan(addr, nr);
+	return rv;
+}
+EXPORT_SYMBOL(ktsan_atomic_fetch_change_bit);
+
+void ktsan_preempt_add(int value)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_preempt_add(thr, pc, value);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_preempt_add);
+
+void ktsan_preempt_sub(int value)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_preempt_sub(thr, pc, value);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_preempt_sub);
+
+void ktsan_irq_disable(void)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_irq_disable(thr, pc);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_irq_disable);
+
+void ktsan_irq_enable(void)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_irq_enable(thr, pc);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_irq_enable);
+
+void ktsan_irq_save(void)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_irq_save(thr, pc);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_irq_save);
+
+void ktsan_irq_restore(unsigned long flags)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_irq_restore(thr, pc, flags);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_irq_restore);
+
+void ktsan_percpu_acquire(void *addr)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_percpu_acquire(thr, pc, (uptr_t)addr);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_percpu_acquire);
+
+void ktsan_read1(void *addr)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_access(thr, pc, (uptr_t)addr, 0, true, false);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_read1);
+
+void ktsan_read2(void *addr)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_access(thr, pc, (uptr_t)addr, 1, true, false);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_read2);
+
+void ktsan_read4(void *addr)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_access(thr, pc, (uptr_t)addr, 2, true, false);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_read4);
+
+void ktsan_read8(void *addr)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_access(thr, pc, (uptr_t)addr, 3, true, false);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_read8);
+
+void ktsan_read16(void *addr)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_access(thr, pc, (uptr_t)addr, 3, true, false);
+	kt_access(thr, pc, (uptr_t)addr + 8, 3, true, false);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_read16);
+
+void ktsan_read_range(void *addr, size_t sz)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_access_range(thr, pc, (uptr_t)addr, sz, true);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_read_range);
+
+void ktsan_write1(void *addr)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_access(thr, pc, (uptr_t)addr, 0, false, false);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_write1);
+
+void ktsan_write2(void *addr)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_access(thr, pc, (uptr_t)addr, 1, false, false);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_write2);
+
+void ktsan_write4(void *addr)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_access(thr, pc, (uptr_t)addr, 2, false, false);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_write4);
+
+void ktsan_write8(void *addr)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_access(thr, pc, (uptr_t)addr, 3, false, false);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_write8);
+
+void ktsan_write16(void *addr)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_access(thr, pc, (uptr_t)addr, 3, false, false);
+	kt_access(thr, pc, (uptr_t)addr + 8, 3, false, false);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_write16);
+
+void ktsan_write_range(void *addr, size_t sz)
+{
+	ENTER(KT_ENTER_NORMAL);
+	kt_access_range(thr, pc, (uptr_t)addr, sz, false);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_write_range);
+
+void ktsan_func_entry(void *call_pc)
+{
+#if KT_DEBUG
+	/* mutex_lock calls mutex_lock_slowpath, and it might be useful
+	   to see these frames in trace when debugging. Same in func_exit. */
+	ENTER(KT_ENTER_DISABLED);
+#else
+	ENTER(KT_ENTER_NORMAL);
+#endif
+	kt_func_entry(thr, (uptr_t)call_pc);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_func_entry);
+
+void ktsan_func_exit(void)
+{
+#if KT_DEBUG
+	ENTER(KT_ENTER_DISABLED);
+#else
+	ENTER(KT_ENTER_NORMAL);
+#endif
+	kt_func_exit(thr);
+	LEAVE();
+}
+EXPORT_SYMBOL(ktsan_func_exit);
+
+void __tsan_read1(void *) __attribute__ ((alias("ktsan_read1")));
+EXPORT_SYMBOL(__tsan_read1);
+
+void __tsan_read2(void *) __attribute__ ((alias("ktsan_read2")));
+EXPORT_SYMBOL(__tsan_read2);
+
+void __tsan_read4(void *) __attribute__ ((alias("ktsan_read4")));
+EXPORT_SYMBOL(__tsan_read4);
+
+void __tsan_read8(void *) __attribute__ ((alias("ktsan_read8")));
+EXPORT_SYMBOL(__tsan_read8);
+
+void __tsan_read16(void *) __attribute__ ((alias("ktsan_read16")));
+EXPORT_SYMBOL(__tsan_read16);
+
+void __tsan_read_range(void *, unsigned long size)
+	__attribute__ ((alias("ktsan_read_range")));
+EXPORT_SYMBOL(__tsan_read_range);
+
+void __tsan_unaligned_read1(void *) __attribute__ ((alias("ktsan_read1")));
+EXPORT_SYMBOL(__tsan_unaligned_read1);
+
+void __tsan_unaligned_read2(void *) __attribute__ ((alias("ktsan_read2")));
+EXPORT_SYMBOL(__tsan_unaligned_read2);
+
+void __tsan_unaligned_read4(void *) __attribute__ ((alias("ktsan_read4")));
+EXPORT_SYMBOL(__tsan_unaligned_read4);
+
+void __tsan_unaligned_read8(void *) __attribute__ ((alias("ktsan_read8")));
+EXPORT_SYMBOL(__tsan_unaligned_read8);
+
+void __tsan_unaligned_read16(void *) __attribute__ ((alias("ktsan_read16")));
+EXPORT_SYMBOL(__tsan_unaligned_read16);
+
+void __tsan_unaligned_read_range(void *, unsigned long size)
+	__attribute__ ((alias("ktsan_read_range")));
+EXPORT_SYMBOL(__tsan_unaligned_read_range);
+
+void __tsan_write1(void *) __attribute__ ((alias("ktsan_write1")));
+EXPORT_SYMBOL(__tsan_write1);
+
+void __tsan_write2(void *) __attribute__ ((alias("ktsan_write2")));
+EXPORT_SYMBOL(__tsan_write2);
+
+void __tsan_write4(void *) __attribute__ ((alias("ktsan_write4")));
+EXPORT_SYMBOL(__tsan_write4);
+
+void __tsan_write8(void *) __attribute__ ((alias("ktsan_write8")));
+EXPORT_SYMBOL(__tsan_write8);
+
+void __tsan_write16(void *) __attribute__ ((alias("ktsan_write16")));
+EXPORT_SYMBOL(__tsan_write16);
+
+void __tsan_write_range(void *, unsigned long size)
+	__attribute__ ((alias("ktsan_write_range")));
+EXPORT_SYMBOL(__tsan_write_range);
+
+void __tsan_unaligned_write1(void *) __attribute__ ((alias("ktsan_write1")));
+EXPORT_SYMBOL(__tsan_unaligned_write1);
+
+void __tsan_unaligned_write2(void *) __attribute__ ((alias("ktsan_write2")));
+EXPORT_SYMBOL(__tsan_unaligned_write2);
+
+void __tsan_unaligned_write4(void *) __attribute__ ((alias("ktsan_write4")));
+EXPORT_SYMBOL(__tsan_unaligned_write4);
+
+void __tsan_unaligned_write8(void *) __attribute__ ((alias("ktsan_write8")));
+EXPORT_SYMBOL(__tsan_unaligned_write8);
+
+void __tsan_unaligned_write16(void *) __attribute__ ((alias("ktsan_write16")));
+EXPORT_SYMBOL(__tsan_unaligned_write16);
+
+void __tsan_unaligned_write_range(void *, unsigned long size)
+	__attribute__ ((alias("ktsan_write_range")));
+EXPORT_SYMBOL(__tsan_unaligned_write_range);
+
+void __tsan_func_entry(void *) __attribute__ ((alias("ktsan_func_entry")));
+EXPORT_SYMBOL(__tsan_func_entry);
+
+void __tsan_func_exit(void) __attribute__ ((alias("ktsan_func_exit")));
+EXPORT_SYMBOL(__tsan_func_exit);
+
+void __tsan_init(void) __attribute__ ((alias("ktsan_init")));
+EXPORT_SYMBOL(__tsan_init);
diff --git a/mm/ktsan/ktsan.h b/mm/ktsan/ktsan.h
new file mode 100644
index 000000000000..e6f8a249ec66
--- /dev/null
+++ b/mm/ktsan/ktsan.h
@@ -0,0 +1,855 @@
+// SPDX-License-Identifier: GPL-2.0
+#ifndef __X86_MM_KTSAN_KTSAN_H
+#define __X86_MM_KTSAN_KTSAN_H
+
+#include <linux/ktsan.h>
+#include <linux/list.h>
+#include <linux/percpu.h>
+#include <linux/types.h>
+#include <linux/gfp.h>
+#include <linux/kernel.h>
+#include <linux/mm_types.h>
+#include <linux/mm.h>
+
+#define KT_DEBUG 0
+#define KT_DEBUG_TRACE 0
+#define KT_ENABLE_STATS 0
+
+#define KT_GRAIN 8
+#define KT_SHADOW_SLOTS_LOG 2
+#define KT_SHADOW_SLOTS (1 << KT_SHADOW_SLOTS_LOG)
+#define KT_SHADOW_TO_LONG(shadow) (*(long *)(&shadow))
+
+/* Logarithms of access sizes, used in shadow encoding. */
+#define KT_ACCESS_SIZE_1 0
+#define KT_ACCESS_SIZE_2 1
+#define KT_ACCESS_SIZE_4 2
+#define KT_ACCESS_SIZE_8 3
+
+#define KT_THREAD_ID_BITS 12
+#define KT_CLOCK_BITS 42
+
+#define KT_SYNC_TAB_SIZE 196613
+#define KT_MEMBLOCK_TAB_SIZE 196613
+
+#define KT_MAX_SYNC_COUNT (1700 * 1000)
+#define KT_MAX_MEMBLOCK_COUNT (200 * 1000)
+#define KT_MAX_PERCPU_SYNC_COUNT (30 * 1000)
+
+#define KT_MAX_TASK_COUNT 1024
+#define KT_MAX_THREAD_COUNT KT_MAX_TASK_COUNT
+
+#define KT_MAX_STACK_FRAMES 96
+#define KT_TAME_COUNTER_LIMIT 3
+#define KT_MAX_LOCKED_MTX 32
+
+#define KT_STACK_DEPOT_PARTS 196613
+/* Can't be more than 16 GB because offset divided by 4 is stored in uint32 */
+#define KT_STACK_DEPOT_MEMORY_LIMIT (512 << 20)
+
+#define KT_TRACE_PARTS 8
+#define KT_TRACE_PART_SIZE (8 * 1024)
+#define KT_TRACE_SIZE (KT_TRACE_PARTS * KT_TRACE_PART_SIZE)
+
+/* For use on performance-critical paths. */
+#if KT_DEBUG
+#define KT_BUG_ON(x) BUG_ON(x)
+#else
+#define KT_BUG_ON(x)                                                           \
+	{                                                                      \
+	}
+#endif
+
+typedef unsigned long uptr_t;
+typedef unsigned long kt_time_t;
+
+typedef struct kt_thr_s kt_thr_t;
+typedef struct kt_clk_s kt_clk_t;
+typedef struct kt_tab_s kt_tab_t;
+typedef struct kt_tab_obj_s kt_tab_obj_t;
+typedef struct kt_tab_part_s kt_tab_part_t;
+typedef struct kt_tab_sync_s kt_tab_sync_t;
+typedef struct kt_tab_memblock_s kt_tab_memblock_t;
+typedef struct kt_tab_lock_s kt_tab_lock_t;
+typedef struct kt_tab_test_s kt_tab_test_t;
+typedef struct kt_ctx_s kt_ctx_t;
+typedef enum kt_stat_e kt_stat_t;
+typedef struct kt_stats_s kt_stats_t;
+typedef struct kt_cpu_s kt_cpu_t;
+typedef struct kt_task_s kt_task_t;
+typedef struct kt_race_info_s kt_race_info_t;
+typedef struct kt_cache_s kt_cache_t;
+typedef struct kt_stack_s kt_stack_t;
+typedef u32 kt_stack_handle_t;
+typedef struct kt_stack_depot_s kt_stack_depot_t;
+typedef struct kt_stack_depot_obj_s kt_stack_depot_obj_t;
+typedef enum kt_event_type_e kt_event_type_t;
+typedef struct kt_event_s kt_event_t;
+typedef struct kt_locked_mutex_s kt_locked_mutex_t;
+typedef struct kt_mutexset_s kt_mutexset_t;
+typedef struct kt_trace_state_s kt_trace_state_t;
+typedef struct kt_trace_part_header_s kt_trace_part_header_t;
+typedef struct kt_trace_s kt_trace_t;
+typedef struct kt_id_manager_s kt_id_manager_t;
+typedef struct kt_thr_pool_s kt_thr_pool_t;
+typedef struct kt_shadow_s kt_shadow_t;
+typedef struct kt_percpu_sync_s kt_percpu_sync_t;
+typedef struct kt_spinlock_s kt_spinlock_t;
+typedef struct kt_interrupted_s kt_interrupted_t;
+
+/* Ktsan runtime internal, non-instrumented spinlock. */
+
+struct kt_spinlock_s {
+	u8 state;
+};
+
+/* Internal allocator. */
+
+struct kt_cache_s {
+	unsigned long base;
+	unsigned long mem_size;
+	void *head;
+	kt_spinlock_t lock;
+};
+
+/* Stack. */
+
+struct kt_stack_s {
+	s32 size;
+	u32 pc[KT_MAX_STACK_FRAMES];
+};
+
+/* Stack depot. */
+
+struct kt_stack_depot_obj_s {
+	kt_stack_depot_obj_t *link;
+	u32 hash;
+	s32 stack_size;
+	u32 stack_pc[0];
+};
+
+struct kt_stack_depot_s {
+	kt_cache_t stack_cache;
+	u64 stack_offset;
+	unsigned long nstacks;
+	kt_stack_depot_obj_t *parts[KT_STACK_DEPOT_PARTS];
+	kt_spinlock_t lock;
+};
+
+/* Trace. */
+
+enum kt_event_type_e {
+	kt_event_nop,
+	kt_event_mop, /* memory operation */
+	kt_event_func_enter,
+	kt_event_func_exit,
+	kt_event_thr_start,
+	kt_event_thr_stop,
+	kt_event_lock,
+	kt_event_rlock,
+	kt_event_unlock,
+	kt_event_runlock,
+	kt_event_interrupt,
+	kt_event_downgrade,
+#if KT_DEBUG
+	kt_event_acquire,
+	kt_event_release,
+	kt_event_nonmat_acquire,
+	kt_event_nonmat_release,
+	kt_event_membar_acquire,
+	kt_event_membar_release,
+	kt_event_preempt_enable,
+	kt_event_preempt_disable,
+	kt_event_irq_enable,
+	kt_event_irq_disable,
+	kt_event_event_disable,
+	kt_event_event_enable,
+#endif /* KT_DEBUG */
+};
+
+struct kt_event_s {
+	u64 type : 16;
+	u64 data : 48;
+	/* The data field is
+	   cpu id for thread start and stop events,
+	   sync uid for lock and unlock events,
+	   and pc for other kinds of event. */
+};
+
+struct kt_locked_mutex_s {
+	u64 uid;
+	kt_stack_handle_t stack;
+	bool write;
+};
+
+struct kt_mutexset_s {
+	kt_locked_mutex_t mtx[KT_MAX_LOCKED_MTX];
+	int size;
+};
+
+struct kt_trace_state_s {
+	kt_stack_t stack;
+	kt_mutexset_t mutexset;
+	int pid;
+	int cpu_id;
+};
+
+struct kt_trace_part_header_s {
+	kt_trace_state_t state;
+	kt_time_t clock;
+};
+
+struct kt_trace_s {
+	kt_trace_part_header_t headers[KT_TRACE_PARTS];
+	kt_event_t events[KT_TRACE_SIZE];
+	kt_spinlock_t lock;
+};
+
+/* Clocks. */
+
+struct kt_clk_s {
+	kt_time_t time[KT_MAX_THREAD_COUNT];
+};
+
+/* Shadow. */
+
+struct kt_shadow_s {
+	unsigned long tid : KT_THREAD_ID_BITS;
+	unsigned long clock : KT_CLOCK_BITS;
+	unsigned long offset : 3;
+	unsigned long size : 2;
+	unsigned long read : 1;
+	unsigned long atomic : 1;
+};
+
+/* Reports. */
+
+struct kt_race_info_s {
+	unsigned long addr;
+	kt_shadow_t old;
+	kt_shadow_t new;
+};
+
+/* Hash table. */
+
+struct kt_tab_obj_s {
+	kt_spinlock_t lock;
+	kt_tab_obj_t *link;
+	uptr_t key;
+};
+
+struct kt_tab_part_s {
+	kt_spinlock_t lock;
+	kt_tab_obj_t *head;
+};
+
+struct kt_tab_s {
+	unsigned size;
+	unsigned objsize;
+	kt_tab_part_t *parts;
+	kt_cache_t obj_cache;
+	kt_cache_t parts_cache;
+};
+
+struct kt_tab_sync_s {
+	kt_tab_obj_t tab;
+	u64 uid;
+	kt_clk_t clk;
+	int lock_tid; /* id of thread that locked mutex */
+	struct list_head list;
+	uptr_t pc;
+	kt_time_t last_lock_time;
+	kt_time_t last_unlock_time;
+};
+
+struct kt_tab_lock_s {
+	kt_tab_obj_t tab;
+	kt_spinlock_t lock;
+	struct list_head list;
+};
+
+struct kt_tab_memblock_s {
+	kt_tab_obj_t tab;
+	struct list_head sync_list;
+	struct list_head lock_list;
+};
+
+struct kt_tab_test_s {
+	kt_tab_obj_t tab;
+	unsigned long data[4];
+};
+
+/* Threads. */
+
+struct kt_thr_s {
+	int id;
+	int pid;
+	unsigned long inside; /* already inside of ktsan runtime */
+	kt_cpu_t *cpu;
+	kt_clk_t clk;
+	kt_stack_t stack;
+	kt_mutexset_t mutexset;
+	kt_clk_t acquire_clk;
+	int acquire_active;
+	kt_clk_t release_clk;
+	int release_active;
+	kt_trace_t trace;
+	int read_disable_depth;
+	int event_disable_depth;
+	int report_disable_depth;
+	int preempt_disable_depth;
+	bool irqs_disabled;
+	unsigned long irq_flags_before_disable;
+	struct list_head quarantine_list; /* list entry */
+	struct list_head percpu_list; /* list head */
+	/* List of currently "acquired" for reading seqcounts. */
+	uptr_t seqcount[6];
+	/* Where the seqcounts were acquired (for debugging). */
+	uptr_t seqcount_pc[6];
+	/* Ignore of all seqcount-related events. */
+	int seqcount_ignore;
+	int interrupt_depth;
+#if KT_DEBUG
+	kt_time_t last_event_disable_time;
+	kt_time_t last_event_enable_time;
+#endif
+};
+
+/* Holds state of an interrupted thread while it executes interrupts.
+ * Essentially a subset of kt_thr_t state.
+ */
+struct kt_interrupted_s {
+	kt_thr_t *thr;
+	kt_stack_t stack;
+	kt_mutexset_t mutexset;
+	kt_clk_t acquire_clk;
+	int acquire_active;
+	kt_clk_t release_clk;
+	int release_active;
+	int read_disable_depth;
+	int report_disable_depth;
+	int preempt_disable_depth;
+	struct list_head percpu_list;
+	uptr_t seqcount[6];
+	uptr_t seqcount_pc[6];
+	int seqcount_ignore;
+};
+
+struct kt_thr_pool_s {
+	kt_cache_t cache;
+	kt_thr_t *thrs[KT_MAX_THREAD_COUNT];
+	int new_id;
+	int new_pid;
+	struct list_head quarantine;
+	int quarantine_size;
+	kt_spinlock_t lock;
+};
+
+/* Per-cpu synchronization. */
+
+struct kt_percpu_sync_s {
+	uptr_t addr;
+	struct list_head list;
+};
+
+/* Statistics. */
+
+enum kt_stat_e {
+	kt_stat_reports,
+	kt_stat_access_read,
+	kt_stat_access_write,
+	kt_stat_access_size1,
+	kt_stat_access_size2,
+	kt_stat_access_size4,
+	kt_stat_access_size8,
+	kt_stat_sync_objects,
+	kt_stat_sync_alloc,
+	kt_stat_sync_free,
+	kt_stat_memblock_objects,
+	kt_stat_memblock_alloc,
+	kt_stat_memblock_free,
+	kt_stat_threads,
+	kt_stat_thread_create,
+	kt_stat_thread_destroy,
+	kt_stat_acquire,
+	kt_stat_release,
+	kt_stat_func_entry,
+	kt_stat_func_exit,
+	kt_stat_trace_event,
+	kt_stat_count,
+};
+
+struct kt_stats_s {
+	unsigned long stat[kt_stat_count];
+};
+
+/* KTSAN per-cpu state. */
+
+struct kt_cpu_s {
+	/* Thread that currently runs on the CPU or NULL. */
+	kt_thr_t *thr;
+	kt_stats_t stat;
+	u64 sync_uid_pos;
+	u64 sync_uid_end;
+	kt_interrupted_t interrupted;
+};
+
+/* KTSAN per-task state. */
+
+struct kt_task_s {
+	/* Thread that is associated with this task. Never NULL. */
+	kt_thr_t *thr;
+	/* Shows whether this task is being executed. */
+	bool running;
+};
+
+/* Global. */
+
+struct kt_ctx_s {
+	int enabled;
+	kt_cpu_t __percpu *cpus;
+	kt_cache_t task_cache;
+	kt_tab_t sync_tab; /* sync addr -> sync object */
+	kt_tab_t memblock_tab; /* memory block -> sync objects */
+	kt_tab_t test_tab;
+	kt_cache_t percpu_sync_cache;
+	kt_thr_pool_t thr_pool;
+	kt_stack_depot_t stack_depot;
+	u64 sync_uid_gen;
+};
+
+extern kt_ctx_t kt_ctx;
+
+/* Statistics. Enabled only when KT_ENABLE_STATS = 1. */
+
+void kt_stat_init(void);
+
+static inline void kt_stat_add(kt_stat_t what, unsigned long x)
+{
+#if KT_ENABLE_STATS
+	this_cpu_ptr(kt_ctx.cpus)->stat.stat[what] += x;
+#endif
+}
+
+static inline void kt_stat_inc(kt_stat_t what)
+{
+	kt_stat_add(what, 1);
+}
+
+static inline void kt_stat_dec(kt_stat_t what)
+{
+	kt_stat_add(what, -1);
+}
+
+/* Stack. */
+
+/* All kernel addresses have 0xffff in high 2 bytes (on x86_64). */
+#define KT_ADDR_MASK 0xffff000000000000ull
+#define KT_PC_MASK 0xffffffff00000000ull
+
+static inline u64 kt_compress(u64 addr)
+{
+	KT_BUG_ON((addr | KT_ADDR_MASK) != addr);
+	return addr & ~KT_ADDR_MASK;
+}
+
+static inline u64 kt_decompress(u64 addr)
+{
+	KT_BUG_ON((addr & KT_ADDR_MASK) != 0);
+	if (addr & KT_PC_MASK)
+		return addr | KT_ADDR_MASK;
+	/* This must be a PC with cut off high part. */
+	return addr | KT_PC_MASK;
+}
+
+static __always_inline void kt_stack_init(kt_stack_t *stack)
+{
+	stack->size = 0;
+}
+
+static __always_inline void kt_stack_push(kt_stack_t *stack, u32 pc)
+{
+	BUG_ON(stack->size + 1 >= KT_MAX_STACK_FRAMES);
+	stack->pc[stack->size++] = pc;
+}
+
+static __always_inline u32 kt_stack_pop(kt_stack_t *stack)
+{
+	BUG_ON(stack->size <= 0);
+	return stack->pc[--stack->size];
+}
+
+void kt_stack_copy(kt_stack_t *dst, kt_stack_t *src);
+void kt_stack_print(kt_stack_t *stack, uptr_t top_pc);
+
+#if KT_DEBUG
+void kt_stack_print_current(unsigned long strip_addr);
+void kt_stack_save_current(kt_stack_t *stack, unsigned long strip_addr);
+#endif
+
+/* Stack depot. */
+
+void kt_stack_depot_init(kt_stack_depot_t *depot);
+kt_stack_handle_t kt_stack_depot_save(kt_stack_depot_t *depot,
+				      kt_stack_t *stack);
+kt_stack_t *kt_stack_depot_get(kt_stack_depot_t *depot,
+			       kt_stack_handle_t handle);
+void kt_stack_depot_stats(kt_stack_depot_t *depot, unsigned long *nstacks,
+			  unsigned long *memory);
+
+/* Clocks. */
+
+void kt_clk_init(kt_clk_t *clk);
+void kt_clk_acquire(kt_clk_t *dst, kt_clk_t *src);
+void kt_clk_set(kt_clk_t *dst, kt_clk_t *src);
+
+static __always_inline kt_time_t kt_clk_get(kt_clk_t *clk, int tid)
+{
+	KT_BUG_ON(tid >= KT_MAX_THREAD_COUNT);
+	return clk->time[tid];
+}
+
+static __always_inline void kt_clk_tick(kt_clk_t *clk, int tid)
+{
+	KT_BUG_ON(tid >= KT_MAX_THREAD_COUNT);
+	clk->time[tid]++;
+}
+
+/* Trace. */
+
+void kt_trace_init(kt_trace_t *trace);
+void kt_trace_switch(kt_thr_t *thr);
+void kt_trace_restore_state(kt_thr_t *thr, kt_time_t clock,
+			    kt_trace_state_t *state);
+void kt_trace_dump(kt_trace_t *trace, unsigned long beg, unsigned long end);
+u64 kt_trace_last_data(kt_thr_t *thr);
+
+/* Adds the event to the thread trace.
+ * Only 48 low bits of data are saved, use kt_compress if you need to save
+ * addresses or pcs.
+ */
+static inline void kt_trace_add_event(kt_thr_t *thr, kt_event_type_t type,
+				      u64 data)
+{
+	kt_trace_t *trace;
+	kt_time_t clock;
+	kt_event_t event;
+	unsigned pos;
+
+	kt_stat_inc(kt_stat_trace_event);
+
+	kt_clk_tick(&thr->clk, thr->id);
+
+	trace = &thr->trace;
+	clock = kt_clk_get(&thr->clk, thr->id);
+	pos = clock % KT_TRACE_SIZE;
+
+	if ((pos % KT_TRACE_PART_SIZE) == 0)
+		kt_trace_switch(thr);
+
+	event.type = type;
+	event.data = data;
+	BUG_ON(event.data != data);
+	trace->events[pos] = event;
+}
+
+/* Same as kt_trace_add_event but saves 2 data items to trace.
+ * Data is still stripped to 48 bits, but data2 is saved entirely.
+ * The function ensures that the two words do not cross part boundary.
+ * It is responsibility of kt_trace_follow to deal with both data items.
+ */
+void kt_trace_add_event2(kt_thr_t *thr, kt_event_type_t type, u64 data,
+			 u64 data2);
+
+/* Spinlock. */
+
+void kt_spin_init(kt_spinlock_t *l);
+void kt_spin_lock(kt_spinlock_t *l);
+void kt_spin_unlock(kt_spinlock_t *l);
+int kt_spin_is_locked(kt_spinlock_t *l);
+
+/* Shadow. */
+
+static __always_inline void *kt_shadow_get(uptr_t addr)
+{
+	struct page *page;
+	unsigned long aligned_addr;
+	unsigned long shadow_offset;
+
+	if (unlikely(addr < (unsigned long)(__va(0)) ||
+		     addr >= (unsigned long)(__va(max_pfn << PAGE_SHIFT))))
+		return NULL;
+
+	/* XXX: kmemcheck checks something about pte here. */
+
+	page = virt_to_page(addr);
+	if (unlikely(!page->shadow))
+		return NULL;
+
+	aligned_addr = round_down(addr, KT_GRAIN);
+	shadow_offset = (aligned_addr & (PAGE_SIZE - 1)) * KT_SHADOW_SLOTS;
+	return page->shadow + shadow_offset;
+}
+
+void kt_shadow_clear(uptr_t addr, size_t size);
+
+extern unsigned long kt_shadow_pages;
+
+/* Threads. */
+
+void kt_thr_pool_init(void);
+
+kt_thr_t *kt_thr_create(kt_thr_t *thr, int pid);
+void kt_thr_destroy(kt_thr_t *thr, kt_thr_t *old);
+kt_thr_t *kt_thr_get(int id);
+
+void kt_thr_start(kt_thr_t *thr, uptr_t pc);
+void kt_thr_stop(kt_thr_t *thr, uptr_t pc);
+
+void kt_thr_interrupt(kt_thr_t *thr, uptr_t pc, kt_interrupted_t *state);
+void kt_thr_resume(kt_thr_t *thr, uptr_t pc, kt_interrupted_t *state);
+
+bool kt_thr_event_disable(kt_thr_t *thr, uptr_t pc, unsigned long *flags);
+bool kt_thr_event_enable(kt_thr_t *thr, uptr_t pc, unsigned long *flags);
+void kt_thr_report_disable(kt_thr_t *thr);
+void kt_thr_report_enable(kt_thr_t *thr);
+
+/* Synchronization. */
+
+kt_tab_sync_t *kt_sync_ensure_created(kt_thr_t *thr, uptr_t pc, uptr_t addr);
+void kt_sync_free(kt_thr_t *thr, uptr_t addr);
+
+void kt_sync_acquire(kt_thr_t *thr, uptr_t pc, uptr_t addr);
+void kt_sync_release(kt_thr_t *thr, uptr_t pc, uptr_t addr);
+
+void kt_acquire(kt_thr_t *thr, uptr_t pc, kt_tab_sync_t *sync);
+void kt_release(kt_thr_t *thr, uptr_t pc, kt_tab_sync_t *sync);
+
+void kt_mtx_pre_lock(kt_thr_t *thr, uptr_t pc, uptr_t addr, bool wr, bool try);
+void kt_mtx_post_lock(kt_thr_t *thr, uptr_t pc, uptr_t addr, bool wr, bool try,
+		      bool success);
+void kt_mtx_pre_unlock(kt_thr_t *thr, uptr_t pc, uptr_t addr, bool wr);
+void kt_mtx_post_unlock(kt_thr_t *thr, uptr_t pc, uptr_t addr, bool wr);
+void kt_mtx_downgrade(kt_thr_t *thr, uptr_t pc, uptr_t addr);
+
+void kt_mutexset_init(kt_mutexset_t *s);
+void kt_mutexset_lock(kt_mutexset_t *s, u64 uid, kt_stack_handle_t h, bool wr);
+void kt_mutexset_unlock(kt_mutexset_t *set, u64 uid, bool wr);
+void kt_mutexset_downgrade(kt_mutexset_t *set, u64 uid);
+
+void kt_mutex_lock(kt_thr_t *thr, uptr_t pc, u64 sync_uid, bool write);
+void kt_mutex_unlock(kt_thr_t *thr, u64 sync_uid, bool write);
+void kt_mutex_downgrade(kt_thr_t *thr, u64 sync_uid);
+
+void kt_seqcount_begin(kt_thr_t *thr, uptr_t pc, uptr_t addr);
+void kt_seqcount_end(kt_thr_t *thr, uptr_t pc, uptr_t addr);
+void kt_seqcount_ignore_begin(kt_thr_t *thr, uptr_t pc);
+void kt_seqcount_ignore_end(kt_thr_t *thr, uptr_t pc);
+void kt_seqcount_bug(kt_thr_t *thr, uptr_t addr, const char *what);
+
+void kt_thread_fence(kt_thr_t *thr, uptr_t pc, ktsan_memory_order_t mo);
+
+void kt_atomic8_store(kt_thr_t *thr, uptr_t pc, void *addr, u8 value,
+		      ktsan_memory_order_t mo);
+void kt_atomic16_store(kt_thr_t *thr, uptr_t pc, void *addr, u16 value,
+		       ktsan_memory_order_t mo);
+void kt_atomic32_store(kt_thr_t *thr, uptr_t pc, void *addr, u32 value,
+		       ktsan_memory_order_t mo);
+void kt_atomic64_store(kt_thr_t *thr, uptr_t pc, void *addr, u64 value,
+		       ktsan_memory_order_t mo);
+
+u8 kt_atomic8_load(kt_thr_t *thr, uptr_t pc, const void *addr,
+		   ktsan_memory_order_t mo);
+u16 kt_atomic16_load(kt_thr_t *thr, uptr_t pc, const void *addr,
+		     ktsan_memory_order_t mo);
+u32 kt_atomic32_load(kt_thr_t *thr, uptr_t pc, const void *addr,
+		     ktsan_memory_order_t mo);
+u64 kt_atomic64_load(kt_thr_t *thr, uptr_t pc, const void *addr,
+		     ktsan_memory_order_t mo);
+
+u8 kt_atomic8_exchange(kt_thr_t *thr, uptr_t pc, void *addr, u8 value,
+		       ktsan_memory_order_t mo);
+u16 kt_atomic16_exchange(kt_thr_t *thr, uptr_t pc, void *addr, u16 value,
+			 ktsan_memory_order_t mo);
+u32 kt_atomic32_exchange(kt_thr_t *thr, uptr_t pc, void *addr, u32 value,
+			 ktsan_memory_order_t mo);
+u64 kt_atomic64_exchange(kt_thr_t *thr, uptr_t pc, void *addr, u64 value,
+			 ktsan_memory_order_t mo);
+
+u8 kt_atomic8_compare_exchange(kt_thr_t *thr, uptr_t pc, void *addr, u8 old,
+			       u8 new, ktsan_memory_order_t mo);
+u16 kt_atomic16_compare_exchange(kt_thr_t *thr, uptr_t pc, void *addr, u16 old,
+				 u16 new, ktsan_memory_order_t mo);
+u32 kt_atomic32_compare_exchange(kt_thr_t *thr, uptr_t pc, void *addr, u32 old,
+				 u32 new, ktsan_memory_order_t mo);
+u64 kt_atomic64_compare_exchange(kt_thr_t *thr, uptr_t pc, void *addr, u64 old,
+				 u64 new, ktsan_memory_order_t mo);
+
+u8 kt_atomic8_fetch_add(kt_thr_t *thr, uptr_t pc, void *addr, u8 value,
+			ktsan_memory_order_t mo);
+u16 kt_atomic16_fetch_add(kt_thr_t *thr, uptr_t pc, void *addr, u16 value,
+			  ktsan_memory_order_t mo);
+u32 kt_atomic32_fetch_add(kt_thr_t *thr, uptr_t pc, void *addr, u32 value,
+			  ktsan_memory_order_t mo);
+u64 kt_atomic64_fetch_add(kt_thr_t *thr, uptr_t pc, void *addr, u64 value,
+			  ktsan_memory_order_t mo);
+
+void kt_atomic_set_bit(kt_thr_t *thr, uptr_t pc, void *addr, long nr,
+		       ktsan_memory_order_t mo);
+void kt_atomic_clear_bit(kt_thr_t *thr, uptr_t pc, void *addr, long nr,
+			 ktsan_memory_order_t mo);
+void kt_atomic_change_bit(kt_thr_t *thr, uptr_t pc, void *addr, long nr,
+			  ktsan_memory_order_t mo);
+
+int kt_atomic_fetch_set_bit(kt_thr_t *thr, uptr_t pc, void *addr, long nr,
+			    ktsan_memory_order_t mo);
+int kt_atomic_fetch_clear_bit(kt_thr_t *thr, uptr_t pc, void *addr, long nr,
+			      ktsan_memory_order_t mo);
+int kt_atomic_fetch_change_bit(kt_thr_t *thr, uptr_t pc, void *addr, long nr,
+			       ktsan_memory_order_t mo);
+
+void kt_thread_fence_no_ktsan(ktsan_memory_order_t mo);
+
+static __always_inline u8 kt_atomic8_load_no_ktsan(const void *addr)
+{
+	return *(volatile u8 *)addr;
+}
+
+static __always_inline u16 kt_atomic16_load_no_ktsan(const void *addr)
+{
+	return *(volatile u16 *)addr;
+}
+
+static __always_inline u32 kt_atomic32_load_no_ktsan(const void *addr)
+{
+	return *(volatile u32 *)addr;
+}
+
+static __always_inline u64 kt_atomic64_load_no_ktsan(const void *addr)
+{
+	return *(volatile u64 *)addr;
+}
+
+static __always_inline void kt_atomic8_store_no_ktsan(void *addr, u8 value)
+{
+	*(volatile u8 *)addr = value;
+}
+
+static __always_inline void kt_atomic16_store_no_ktsan(void *addr, u16 value)
+{
+	*(volatile u16 *)addr = value;
+}
+
+static __always_inline void kt_atomic32_store_no_ktsan(void *addr, u32 value)
+{
+	*(volatile u32 *)addr = value;
+}
+
+static __always_inline void kt_atomic64_store_no_ktsan(void *addr, u64 value)
+{
+	*(volatile u64 *)addr = value;
+}
+
+u8 kt_atomic8_exchange_no_ktsan(void *addr, u8 value);
+u16 kt_atomic16_exchange_no_ktsan(void *addr, u16 value);
+u32 kt_atomic32_exchange_no_ktsan(void *addr, u32 value);
+u64 kt_atomic64_exchange_no_ktsan(void *addr, u64 value);
+
+u8 kt_atomic8_compare_exchange_no_ktsan(void *addr, u8 old, u8 new);
+u16 kt_atomic16_compare_exchange_no_ktsan(void *addr, u16 old, u16 new);
+u32 kt_atomic32_compare_exchange_no_ktsan(void *addr, u32 old, u32 new);
+u64 kt_atomic64_compare_exchange_no_ktsan(void *addr, u64 old, u64 new);
+
+u8 kt_atomic8_fetch_add_no_ktsan(void *addr, u8 value);
+u16 kt_atomic16_fetch_add_no_ktsan(void *addr, u16 value);
+u32 kt_atomic32_fetch_add_no_ktsan(void *addr, u32 value);
+u64 kt_atomic64_fetch_add_no_ktsan(void *addr, u64 value);
+
+void kt_atomic_set_bit_no_ktsan(void *addr, long nr);
+void kt_atomic_clear_bit_no_ktsan(void *addr, long nr);
+void kt_atomic_change_bit_no_ktsan(void *addr, long nr);
+
+int kt_atomic_fetch_set_bit_no_ktsan(void *addr, long nr);
+int kt_atomic_fetch_clear_bit_no_ktsan(void *addr, long nr);
+int kt_atomic_fetch_change_bit_no_ktsan(void *addr, long nr);
+
+/* Per-cpu synchronization. */
+
+void kt_preempt_add(kt_thr_t *thr, uptr_t pc, int value);
+void kt_preempt_sub(kt_thr_t *thr, uptr_t pc, int value);
+
+void kt_irq_disable(kt_thr_t *thr, uptr_t pc);
+void kt_irq_enable(kt_thr_t *thr, uptr_t pc);
+void kt_irq_save(kt_thr_t *thr, uptr_t pc);
+void kt_irq_restore(kt_thr_t *thr, uptr_t pc, unsigned long flags);
+
+void kt_percpu_acquire(kt_thr_t *thr, uptr_t pc, uptr_t addr);
+void kt_percpu_release(kt_thr_t *thr, uptr_t pc);
+
+/* Memory block allocation. */
+
+uptr_t kt_memblock_addr(uptr_t addr);
+void kt_memblock_add_sync(kt_thr_t *thr, uptr_t addr, kt_tab_sync_t *sync);
+void kt_memblock_remove_sync(kt_thr_t *thr, uptr_t addr, kt_tab_sync_t *sync);
+void kt_memblock_alloc(kt_thr_t *thr, uptr_t pc, uptr_t addr, size_t size,
+		       bool write_to_shadow);
+void kt_memblock_free(kt_thr_t *thr, uptr_t pc, uptr_t addr, size_t size,
+		      bool write_to_shadow);
+
+/* For usage in ktsan_free_page, which doesn't use ENTER / LEAVE. */
+void ktsan_memblock_free(void *addr, unsigned long size, bool write_to_shadow);
+
+/* Generic memory access. */
+
+void kt_access(kt_thr_t *thr, uptr_t pc, uptr_t addr, size_t size, bool read,
+	       bool atomic);
+void kt_access_range(kt_thr_t *thr, uptr_t pc, uptr_t addr, size_t size,
+		     bool read);
+
+void kt_access_range_imitate(kt_thr_t *thr, uptr_t pc, uptr_t addr, size_t size,
+			     bool read);
+
+/* Function tracing. */
+
+void kt_func_entry(kt_thr_t *thr, uptr_t pc);
+void kt_func_exit(kt_thr_t *thr);
+
+/* Reports. */
+
+void kt_report_race(kt_thr_t *thr, kt_race_info_t *info);
+void kt_report_bad_mtx_unlock(kt_thr_t *thr, uptr_t pc, kt_tab_sync_t *sync);
+void kt_report_sync_usage(void);
+
+#if KT_DEBUG
+void kt_report_sync_usage(void);
+#endif /* KT_DEBUG */
+
+/* Suppressions. */
+void kt_supp_init(void);
+bool kt_supp_suppressed(unsigned long pc);
+
+/* Internal allocator. */
+
+void kt_cache_init(kt_cache_t *cache, size_t obj_size, size_t obj_max_num);
+void kt_cache_destroy(kt_cache_t *cache);
+void *kt_cache_alloc(kt_cache_t *cache);
+void kt_cache_free(kt_cache_t *cache, void *obj);
+
+/*
+ * Hash table. Maps an address to an arbitrary object.
+ * The object must start with kt_tab_obj_t.
+ */
+
+void kt_tab_init(kt_tab_t *tab, unsigned size, unsigned obj_size,
+		 unsigned obj_max_num);
+void kt_tab_destroy(kt_tab_t *tab);
+void *kt_tab_access(kt_tab_t *tab, uptr_t key, bool *created, bool destroy);
+
+/* Tests. */
+
+void kt_tests_init(void);
+void kt_tests_run_noinst(void);
+void kt_tests_run_inst(void);
+void kt_tests_run(void);
+
+#endif /* __X86_MM_KTSAN_KTSAN_H */
diff --git a/mm/ktsan/memblock.c b/mm/ktsan/memblock.c
new file mode 100644
index 000000000000..eb4244049665
--- /dev/null
+++ b/mm/ktsan/memblock.c
@@ -0,0 +1,123 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+#include <linux/list.h>
+#include <linux/mm.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/slab_def.h>
+#include <linux/spinlock.h>
+
+uptr_t kt_memblock_addr(uptr_t addr)
+{
+	struct page *page;
+	struct kmem_cache *cache;
+	u32 offset;
+	u32 idx;
+	uptr_t obj_addr;
+
+	if (!virt_addr_valid(addr))
+		return 0;
+	page = virt_to_head_page((void *)addr);
+
+	/* If the page is a slab page, we want to delete
+	   sync objects when a slab object is freed. */
+	if (PageSlab(page)) {
+		cache = page->slab_cache;
+		BUG_ON(addr < (uptr_t)page->s_mem);
+		offset = addr - (uptr_t)page->s_mem;
+		idx = reciprocal_divide(offset, cache->reciprocal_buffer_size);
+		obj_addr = (uptr_t)(page->s_mem + cache->size * idx);
+		return obj_addr;
+	}
+
+	return (uptr_t)page_address(page);
+}
+
+static kt_tab_memblock_t *kt_memblock_ensure_created(kt_thr_t *thr, uptr_t addr)
+{
+	kt_tab_memblock_t *memblock;
+	bool created;
+
+	memblock = kt_tab_access(&kt_ctx.memblock_tab, addr, &created, false);
+	BUG_ON(memblock == NULL); /* Ran out of memory. */
+
+	if (created) {
+		INIT_LIST_HEAD(&memblock->sync_list);
+		INIT_LIST_HEAD(&memblock->lock_list);
+
+		kt_stat_inc(kt_stat_memblock_objects);
+		kt_stat_inc(kt_stat_memblock_alloc);
+	}
+
+	return memblock;
+}
+
+void kt_memblock_add_sync(kt_thr_t *thr, uptr_t addr, kt_tab_sync_t *sync)
+{
+	kt_tab_memblock_t *memblock;
+
+	memblock = kt_memblock_ensure_created(thr, addr);
+	list_add(&sync->list, &memblock->sync_list);
+	kt_spin_unlock(&memblock->tab.lock);
+}
+
+void kt_memblock_remove_sync(kt_thr_t *thr, uptr_t addr, kt_tab_sync_t *sync)
+{
+	kt_tab_memblock_t *memblock;
+	struct list_head *entry, *tmp;
+	bool deleted = false;
+
+	memblock = kt_tab_access(&kt_ctx.memblock_tab, addr, NULL, false);
+	BUG_ON(memblock == NULL);
+
+	list_for_each_safe(entry, tmp, &memblock->sync_list) {
+		if (list_entry(entry, kt_tab_sync_t, list) == sync) {
+			list_del_init(entry);
+			deleted = true;
+			break;
+		}
+	}
+
+	BUG_ON(!deleted);
+	kt_spin_unlock(&memblock->tab.lock);
+}
+
+void kt_memblock_alloc(kt_thr_t *thr, uptr_t pc, uptr_t addr, size_t size,
+			bool write_to_shadow)
+{
+	/* Memory block size is multiple of KT_GRAIN, so round the size up.
+	 * In the "worst" case we will catch OOB accesses due to this. */
+	size = round_up(size, KT_GRAIN);
+
+	if (write_to_shadow)
+		kt_access_range_imitate(thr, pc, addr, size, false);
+}
+
+void kt_memblock_free(kt_thr_t *thr, uptr_t pc, uptr_t addr, size_t size,
+			bool write_to_shadow)
+{
+	kt_tab_memblock_t *memblock;
+	struct list_head *entry, *tmp;
+	kt_tab_sync_t *sync;
+
+	if (write_to_shadow)
+		kt_access_range(thr, pc, addr, size, false);
+
+	memblock = kt_tab_access(&kt_ctx.memblock_tab, addr, NULL, true);
+
+	if (memblock == NULL)
+		return;
+
+	list_for_each_safe(entry, tmp, &memblock->sync_list) {
+		sync = list_entry(entry, kt_tab_sync_t, list);
+		list_del_init(entry);
+		kt_sync_free(thr, sync->tab.key);
+	}
+
+	kt_spin_unlock(&memblock->tab.lock);
+	kt_cache_free(&kt_ctx.memblock_tab.obj_cache, memblock);
+
+	kt_stat_dec(kt_stat_memblock_objects);
+	kt_stat_inc(kt_stat_memblock_free);
+}
diff --git a/mm/ktsan/mutexset.c b/mm/ktsan/mutexset.c
new file mode 100644
index 000000000000..f3e92ca0c951
--- /dev/null
+++ b/mm/ktsan/mutexset.c
@@ -0,0 +1,77 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+void kt_mutexset_init(kt_mutexset_t *set)
+{
+	set->size = 0;
+}
+
+void kt_mutexset_lock(kt_mutexset_t *set, u64 uid, kt_stack_handle_t stk,
+	bool wr)
+{
+	kt_locked_mutex_t *mtx;
+
+	BUG_ON(set->size >= ARRAY_SIZE(set->mtx));
+	mtx = &set->mtx[set->size++];
+	mtx->uid = uid;
+	mtx->write = wr;
+	mtx->stack = stk;
+}
+
+void kt_mutexset_unlock(kt_mutexset_t *set, u64 uid, bool wr)
+{
+	int i;
+
+	for (i = set->size - 1; i >= 0; i--) {
+		if (set->mtx[i].uid == uid) {
+			BUG_ON(set->mtx[i].write != wr);
+			set->mtx[i] = set->mtx[set->size - 1];
+			set->size--;
+			return;
+		}
+	}
+
+	/* We could BUG() here, but instead in kt_mtx_pre_unlock we print
+	   a report saying that mutex was unlocked in another thread. */
+}
+
+void kt_mutexset_downgrade(kt_mutexset_t *set, u64 uid)
+{
+	int i;
+
+	for (i = set->size - 1; i >= 0; i--) {
+		if (set->mtx[i].uid == uid) {
+			BUG_ON(!set->mtx[i].write);
+			set->mtx[i].write = false;
+			return;
+		}
+	}
+}
+
+/* Add mutex to mutexset and add event to trace. */
+void kt_mutex_lock(kt_thr_t *thr, uptr_t pc, u64 sync_uid, bool write)
+{
+	kt_stack_handle_t stack_handle;
+
+	/* Temporary push the pc onto stack so that it is recorded. */
+	kt_func_entry(thr, pc);
+	stack_handle = kt_stack_depot_save(&kt_ctx.stack_depot, &thr->stack);
+	kt_trace_add_event2(thr, write ? kt_event_lock : kt_event_rlock,
+				sync_uid, stack_handle);
+	kt_mutexset_lock(&thr->mutexset, sync_uid, stack_handle, write);
+	kt_func_exit(thr);
+}
+
+/* Remove mutex from mutexset and add event to trace. */
+void kt_mutex_unlock(kt_thr_t *thr, u64 sync_uid, bool write)
+{
+	kt_trace_add_event(thr, write ? kt_event_unlock : kt_event_runlock,
+				sync_uid);
+	kt_mutexset_unlock(&thr->mutexset, sync_uid, write);
+}
+
+void kt_mutex_downgrade(kt_thr_t *thr, u64 sync_uid)
+{
+	kt_trace_add_event(thr, kt_event_downgrade, sync_uid);
+	kt_mutexset_downgrade(&thr->mutexset, sync_uid);
+}
\ No newline at end of file
diff --git a/mm/ktsan/report.c b/mm/ktsan/report.c
new file mode 100644
index 000000000000..64ddad489c00
--- /dev/null
+++ b/mm/ktsan/report.c
@@ -0,0 +1,256 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+#include <linux/printk.h>
+#include <linux/thread_info.h>
+#include <linux/sort.h>
+#include <linux/spinlock.h>
+
+#define MAX_FUNCTION_NAME_SIZE (128)
+
+static kt_spinlock_t kt_report_lock;
+
+static unsigned long racy_pc[1024];
+static unsigned nracy_pc;
+
+#if KT_DEBUG
+
+uptr_t sync_objects[KT_MAX_SYNC_COUNT];
+
+struct sync_entry_s {
+	uptr_t pc;
+	int count;
+};
+
+typedef struct sync_entry_s sync_entry_t;
+
+sync_entry_t sync_entries[KT_MAX_SYNC_COUNT];
+
+int u64_cmp(const void *a, const void *b)
+{
+	if (*(u64 *)a < *(u64 *)b)
+		return -1;
+	else if (*(u64 *)a > *(u64 *)b)
+		return 1;
+	return 0;
+}
+
+int sync_entry_cmp(const void *a, const void *b)
+{
+	sync_entry_t *sa = (sync_entry_t *)a;
+	sync_entry_t *sb = (sync_entry_t *)b;
+	return sb->count - sa->count;
+}
+
+void kt_report_sync_usage(void)
+{
+	int sync_objects_count = 0;
+	int sync_entries_count = 0;
+	kt_tab_part_t *part;
+	kt_tab_obj_t *obj;
+	kt_tab_sync_t *sync;
+	int i, p, curr;
+	uptr_t curr_pc;
+	static int counter; /* = 0 */
+
+	if (counter++ % 8 != 0)
+		return;
+
+	for (p = 0; p < kt_ctx.sync_tab.size; p++) {
+		part = &kt_ctx.sync_tab.parts[p];
+		kt_spin_lock(&part->lock);
+		for (obj = part->head; obj != NULL; obj = obj->link) {
+			sync = (kt_tab_sync_t *)obj;
+			sync_objects[sync_objects_count++] = sync->pc;
+		}
+		kt_spin_unlock(&part->lock);
+	}
+
+	sort(&sync_objects[0], sync_objects_count,
+		sizeof(uptr_t), &u64_cmp, NULL);
+
+	i = 0;
+	while (i < sync_objects_count) {
+		curr = 0;
+		curr_pc = sync_objects[i];
+		while (i < sync_objects_count && sync_objects[i] == curr_pc) {
+			i++;
+			curr++;
+		}
+		sync_entries[sync_entries_count].pc = curr_pc;
+		sync_entries[sync_entries_count].count = curr;
+		sync_entries_count++;
+	}
+
+	sort(&sync_entries[0], sync_entries_count, sizeof(sync_entry_t),
+			&sync_entry_cmp, NULL);
+
+	pr_err("\n");
+	pr_err("Most syncs created at (totally %d):\n", sync_objects_count);
+	for (i = 0; i < 32; i++) {
+		pr_err(" %6d [<%p>] %pS\n", sync_entries[i].count,
+			(void *)sync_entries[i].pc, (void *)sync_entries[i].pc);
+	}
+}
+
+#endif /* KT_DEBUG */
+
+static void kt_print_mutexset(kt_mutexset_t *set)
+{
+	kt_locked_mutex_t *mtx;
+	int i;
+
+	for (i = 0; i < set->size; i++) {
+		mtx = &set->mtx[i];
+		pr_err("Mutex %llu is %slocked here:\n",
+			mtx->uid, mtx->write ? "" : "read ");
+		kt_stack_print(kt_stack_depot_get(
+			&kt_ctx.stack_depot, mtx->stack), 0);
+	}
+}
+
+static void print_mop(bool new, bool wr, bool atomic,
+		uptr_t addr, int sz, int pid, int cpu)
+{
+	pr_err("%s at 0x%p of size %d by thread %d on CPU %d:\n",
+		new ? (
+			atomic ? (
+				wr ? "Atomic write" : "Atomic read"
+			) : (
+				wr ? "Write" : "Read"
+			)
+		) : (
+			atomic ? (
+				wr ? "Previous atomic write" :
+					"Previous atomic read"
+			) : (
+				wr ? "Previous write" : "Previous read"
+			)
+		),
+		(void *)addr, sz, pid, cpu);
+}
+
+void kt_report_race(kt_thr_t *new, kt_race_info_t *info)
+{
+	int i, n;
+	kt_thr_t *old;
+	uptr_t new_pc, old_pc;
+	kt_trace_state_t old_state;
+	char function[MAX_FUNCTION_NAME_SIZE];
+
+	if (new->report_disable_depth != 0)
+		return;
+
+	BUG_ON(new->stack.size == 0);
+	new_pc = kt_decompress(kt_trace_last_data(new));
+	if (kt_supp_suppressed(new_pc))
+		return;
+
+	old = kt_thr_get(info->old.tid);
+	BUG_ON(old == NULL);
+	kt_trace_restore_state(old, info->old.clock, &old_state);
+	if (old_state.stack.size == 0)
+		return;
+	old_pc = kt_decompress(
+			old_state.stack.pc[old_state.stack.size - 1]);
+	if (kt_supp_suppressed(old_pc))
+		return;
+
+	n = kt_atomic32_load_no_ktsan(&nracy_pc);
+	for (i = 0; i < n; i += 2) {
+		if (new_pc == racy_pc[i] && old_pc == racy_pc[i + 1])
+			return;
+	}
+
+	sprintf(function, "%pS", (void *)new_pc);
+	for (i = 0; i < MAX_FUNCTION_NAME_SIZE; i++) {
+		if (function[i] == '+') {
+			function[i] = '\0';
+			break;
+		}
+	}
+
+	kt_spin_lock(&kt_report_lock);
+
+	if (nracy_pc < ARRAY_SIZE(racy_pc)) {
+		racy_pc[nracy_pc] = new_pc;
+		racy_pc[nracy_pc + 1] = old_pc;
+		kt_atomic32_store_no_ktsan(&nracy_pc, nracy_pc + 2);
+	}
+
+	pr_err("==================================================================\n");
+	pr_err("ThreadSanitizer: data-race in %s\n\n", function);
+
+	print_mop(true, !info->new.read, info->new.atomic, info->addr,
+		(1 << info->new.size), new->pid, smp_processor_id());
+	kt_stack_print(&new->stack, new_pc);
+
+	print_mop(false, !info->old.read, info->old.atomic, info->addr,
+		(1 << info->old.size), old_state.pid, old_state.cpu_id);
+	kt_stack_print(&old_state.stack, 0);
+
+	if (new->mutexset.size) {
+		pr_err("Mutexes locked by thread %d:\n", new->pid);
+		kt_print_mutexset(&new->mutexset);
+	}
+
+	if (old_state.mutexset.size) {
+		pr_err("Mutexes locked by thread %d:\n", old_state.pid);
+		kt_print_mutexset(&old_state.mutexset);
+	}
+
+#if KT_DEBUG
+	pr_err("Thread %d clock: {T%d: %lu, T%d: %lu}\n", new->pid,
+			new->pid, kt_clk_get(&new->clk, new->id),
+			old->pid, kt_clk_get(&new->clk, old->id));
+	pr_err("Thread %d clock: {T%d: %lu}\n", old->pid,
+			old->pid, (unsigned long)info->old.clock);
+#endif /* KT_DEBUG */
+
+#if KT_DEBUG_TRACE
+	pr_err("\n");
+	pr_err("Thread %d trace:\n", new->pid);
+	kt_trace_dump(&new->trace, kt_clk_get(&new->clk, new->id) - 50,
+				kt_clk_get(&new->clk, new->id));
+
+	pr_err("\n");
+	pr_err("Thread %d trace:\n", old->pid);
+	kt_trace_dump(&old->trace, kt_clk_get(&new->clk, old->id) - 20,
+				(uptr_t)info->old.clock + 50);
+#endif /* KT_DEBUG_TRACE */
+	pr_err("==================================================================\n");
+
+	kt_stat_inc(kt_stat_reports);
+
+	kt_spin_unlock(&kt_report_lock);
+}
+
+void kt_report_bad_mtx_unlock(kt_thr_t *new, uptr_t pc, kt_tab_sync_t *sync)
+{
+	kt_thr_t *old;
+	kt_trace_state_t state;
+
+	BUG_ON(sync->lock_tid == -1);
+	BUG_ON(sync->lock_tid == new->id);
+
+	old = kt_thr_get(sync->lock_tid);
+	BUG_ON(old == NULL);
+	kt_trace_restore_state(old, sync->last_lock_time, &state);
+	if (state.stack.size == 0)
+		return;
+
+	pr_err("==================================================================\n");
+
+	pr_err("ThreadSanitizer: mutex unlocked in a different thread\n");
+	pr_err("\n");
+
+	pr_err("Unlock by thread %d on CPU %d:\n",
+		new->pid, smp_processor_id());
+	kt_stack_print(&new->stack, pc);
+
+	pr_err("Previous lock by thread %d on CPU %d:\n",
+		state.pid, state.cpu_id);
+	kt_stack_print(&state.stack, 0);
+
+	pr_err("==================================================================\n");
+}
diff --git a/mm/ktsan/shadow.c b/mm/ktsan/shadow.c
new file mode 100644
index 000000000000..0f5970dbe3c9
--- /dev/null
+++ b/mm/ktsan/shadow.c
@@ -0,0 +1,109 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+#include <linux/gfp.h>
+#include <linux/kernel.h>
+#include <linux/mm_types.h>
+#include <linux/mm.h>
+#include <linux/printk.h>
+#include <linux/slab.h>
+#include <linux/slab_def.h>
+
+unsigned long kt_shadow_pages;
+
+void ktsan_alloc_page(struct page *page, unsigned int order,
+		     gfp_t flags, int node)
+{
+	struct page *shadow;
+	int pages = 1 << order;
+	int i;
+
+	if (flags & __GFP_HIGHMEM)
+		return;
+
+	/* Drop __GFP_NOFAIL flag, since it's to be used in old code only. */
+	flags &= ~__GFP_NOFAIL;
+
+	shadow = alloc_pages_node(node, flags, order + KT_SHADOW_SLOTS_LOG);
+	BUG_ON(!shadow);
+
+	kt_atomic64_fetch_add_no_ktsan(&kt_shadow_pages,
+		1 << (order + KT_SHADOW_SLOTS_LOG));
+
+	memset(page_address(shadow), 0,
+	       PAGE_SIZE * (1 << (order + KT_SHADOW_SLOTS_LOG)));
+
+	for (i = 0; i < pages; i++)
+		page[i].shadow = page_address(&shadow[i * KT_SHADOW_SLOTS]);
+}
+EXPORT_SYMBOL(ktsan_alloc_page);
+
+void ktsan_free_page(struct page *page, unsigned int order)
+{
+	struct page *shadow;
+	int pages = 1 << order;
+	int i;
+	unsigned long page_addr, first_obj_addr, obj_addr;
+	struct kmem_cache *cache;
+
+	page_addr = (unsigned long)page_address(page);
+	first_obj_addr = (unsigned long)page->s_mem;
+	if (PageSlab(page)) {
+		cache = page->slab_cache;
+		for (obj_addr = first_obj_addr;
+		     obj_addr < page_addr + (PAGE_SIZE << order);
+		     obj_addr += cache->size)
+			ktsan_memblock_free((void *)obj_addr,
+						cache->size, false);
+	}
+
+	ktsan_memblock_free((void *)page_addr, PAGE_SIZE << order,
+				page[0].shadow ? true : false);
+
+	if (!page[0].shadow)
+		return;
+
+	shadow = virt_to_page(page[0].shadow);
+
+	for (i = 0; i < pages; i++)
+		page[i].shadow = NULL;
+
+	kt_atomic64_fetch_add_no_ktsan(&kt_shadow_pages,
+		-(1 << (order + KT_SHADOW_SLOTS_LOG)));
+
+	__free_pages(shadow, order + KT_SHADOW_SLOTS_LOG);
+}
+EXPORT_SYMBOL(ktsan_free_page);
+
+void ktsan_split_page(struct page *page, unsigned int order)
+{
+	struct page *shadow;
+
+	if (!page[0].shadow)
+		return;
+
+	shadow = virt_to_page(page[0].shadow);
+	split_page(shadow, order);
+}
+EXPORT_SYMBOL(ktsan_split_page);
+
+void kt_shadow_clear(uptr_t addr, size_t size)
+{
+	void *shadow_beg;
+	void *shadow_end;
+	size_t shadow_size;
+
+	shadow_beg = kt_shadow_get(addr);
+	shadow_end = kt_shadow_get(addr);
+
+	BUG_ON(shadow_beg == NULL && shadow_end != NULL);
+	BUG_ON(shadow_beg != NULL && shadow_end == NULL);
+
+	if (shadow_beg == NULL && shadow_end == NULL)
+		return;
+
+	BUG_ON(shadow_beg > shadow_end);
+
+	shadow_size = (uptr_t)shadow_end - (uptr_t)shadow_beg;
+	memset(shadow_beg, 0, shadow_size);
+}
diff --git a/mm/ktsan/spinlock.c b/mm/ktsan/spinlock.c
new file mode 100644
index 000000000000..eb5e436f0c5e
--- /dev/null
+++ b/mm/ktsan/spinlock.c
@@ -0,0 +1,28 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+void kt_spin_init(kt_spinlock_t *l)
+{
+	l->state = 0;
+}
+
+void kt_spin_lock(kt_spinlock_t *l)
+{
+	for (;;) {
+		if (kt_atomic8_exchange_no_ktsan(&l->state, 1) == 0)
+			return;
+		while (kt_atomic8_load_no_ktsan(&l->state) != 0)
+			cpu_relax();
+	}
+}
+
+void kt_spin_unlock(kt_spinlock_t *l)
+{
+	kt_thread_fence_no_ktsan(ktsan_memory_order_release);
+	kt_atomic8_store_no_ktsan(&l->state, 0);
+}
+
+int kt_spin_is_locked(kt_spinlock_t *l)
+{
+	return kt_atomic8_load_no_ktsan(&l->state) != 0;
+}
diff --git a/mm/ktsan/stack.c b/mm/ktsan/stack.c
new file mode 100644
index 000000000000..2dcd1de0ce34
--- /dev/null
+++ b/mm/ktsan/stack.c
@@ -0,0 +1,57 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+#include <linux/kernel.h>
+#include <linux/stacktrace.h>
+
+void kt_stack_copy(kt_stack_t *dst, kt_stack_t *src)
+{
+	/* Faster than *dst = *src, since copies only the needed frames. */
+	memcpy(&dst->pc[0], &src->pc[0], src->size * sizeof(src->pc[0]));
+	dst->size = src->size;
+}
+
+void kt_stack_print(kt_stack_t *stack, uptr_t top_pc)
+{
+	int i;
+	long pc;
+
+	if (top_pc)
+		pr_err(" [<%p>] %pS\n", (void *)top_pc, (void *)top_pc);
+	for (i = stack->size - 1; i >= 0; i--) {
+		pc = kt_decompress(stack->pc[i]);
+		pr_err(" [<%p>] %pS\n", (void *)pc, (void *)pc);
+	}
+	pr_err("\n");
+}
+
+#if KT_DEBUG
+/* The following functions can't work reliably without frame pointers and are
+ * for debugging only (consider removing -fomit-frame-pointer from Makefile
+ * locally if you use them). */
+void kt_stack_print_current(unsigned long strip_addr)
+{
+	kt_stack_t stack;
+
+	kt_stack_save_current(&stack, strip_addr);
+	kt_stack_print(&stack, 0);
+}
+
+void kt_stack_save_current(kt_stack_t *stack, unsigned long strip_addr)
+{
+	unsigned long entries[KT_MAX_STACK_FRAMES];
+	unsigned int beg = 0, end, i;
+	int nr_entries;
+
+	nr_entries = stack_trace_save(entries, KT_MAX_STACK_FRAMES, 1);
+	/* The last frame is always 0xffffffffffffffff. */
+	end = nr_entries - 1;
+	while (entries[beg] != strip_addr && beg < end)
+		beg++;
+
+	/* Save stack frames in reversed order (deepest first). */
+	for (i = 0; i < end - beg; i++)
+		stack->pc[i] = kt_compress(entries[end - 1 - i]);
+	stack->size = end - beg;
+}
+#endif
diff --git a/mm/ktsan/stack_depot.c b/mm/ktsan/stack_depot.c
new file mode 100644
index 000000000000..f0d79ca64996
--- /dev/null
+++ b/mm/ktsan/stack_depot.c
@@ -0,0 +1,114 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+#include <linux/jhash.h>
+#include <linux/kernel.h>
+
+/* Only available during early boot. */
+void __init kt_stack_depot_init(kt_stack_depot_t *depot)
+{
+	unsigned i;
+
+	for (i = 0; i < KT_STACK_DEPOT_PARTS; i++)
+		depot->parts[i] = NULL;
+	kt_cache_init(&depot->stack_cache, KT_STACK_DEPOT_MEMORY_LIMIT, 1);
+	depot->stack_offset = 0;
+	depot->nstacks = 0;
+	kt_spin_init(&depot->lock);
+}
+
+static inline u32 kt_stack_hash(kt_stack_t *stack)
+{
+	return jhash2((u32 *)&stack->pc[0],
+			stack->size * sizeof(stack->pc[0]) / sizeof(u32),
+			0x9747b28c);
+}
+
+static inline kt_stack_t *kt_stack_depot_obj_to_stack(kt_stack_depot_obj_t *obj)
+{
+	return (kt_stack_t *)&obj->stack_size;
+}
+
+static inline kt_stack_depot_obj_t *kt_stack_depot_lookup(
+			kt_stack_depot_obj_t *obj, kt_stack_t *stack, u32 hash)
+{
+	for (; obj; obj = obj->link) {
+		if (obj->hash != hash || obj->stack_size != stack->size)
+			continue;
+		if (memcmp(&obj->stack_pc[0], &stack->pc[0],
+			sizeof(stack->pc[0]) * stack->size) == 0)
+			return obj;
+	}
+	return NULL;
+}
+
+static inline kt_stack_handle_t kt_stack_depot_obj_to_handle(
+			kt_stack_depot_t *depot, kt_stack_depot_obj_t *obj)
+{
+	return ((uptr_t)obj - depot->stack_cache.base) / sizeof(u32);
+}
+
+static inline kt_stack_depot_obj_t *kt_stack_depot_handle_to_obj(
+			kt_stack_depot_t *depot, kt_stack_handle_t handle)
+{
+	return (kt_stack_depot_obj_t *)(depot->stack_cache.base +
+					handle * sizeof(u32));
+}
+
+kt_stack_handle_t kt_stack_depot_save(kt_stack_depot_t *depot,
+					kt_stack_t *stack)
+{
+	u32 hash;
+	kt_stack_depot_obj_t **part;
+	kt_stack_depot_obj_t *obj, *head;
+
+	BUG_ON(stack->size == 0);
+
+	hash = kt_stack_hash(stack);
+	part = &depot->parts[hash % KT_STACK_DEPOT_PARTS];
+	head = smp_load_acquire(part);
+	obj = kt_stack_depot_lookup(head, stack, hash);
+	if (obj)
+		return kt_stack_depot_obj_to_handle(depot, obj);
+
+	kt_spin_lock(&depot->lock);
+	head = *part;
+	obj = kt_stack_depot_lookup(head, stack, hash);
+	if (!obj) {
+		obj = (kt_stack_depot_obj_t *)(depot->stack_cache.base +
+						depot->stack_offset);
+		depot->stack_offset += sizeof(*obj) +
+			sizeof(stack->pc[0]) * stack->size;
+		BUG_ON(depot->stack_offset > KT_STACK_DEPOT_MEMORY_LIMIT);
+		depot->nstacks++;
+
+		obj->link = head;
+		obj->hash = hash;
+		obj->stack_size = stack->size;
+		memcpy(&obj->stack_pc[0], &stack->pc[0],
+				sizeof(stack->pc[0]) * stack->size);
+
+		smp_store_release(part, obj);
+	}
+
+	kt_spin_unlock(&depot->lock);
+
+	return kt_stack_depot_obj_to_handle(depot, obj);
+}
+
+kt_stack_t *kt_stack_depot_get(kt_stack_depot_t *depot,
+				kt_stack_handle_t handle)
+{
+	kt_stack_depot_obj_t *obj = kt_stack_depot_handle_to_obj(depot, handle);
+	return kt_stack_depot_obj_to_stack(obj);
+}
+
+void kt_stack_depot_stats(kt_stack_depot_t *depot, unsigned long *nstacks,
+	unsigned long *memory)
+{
+	kt_spin_lock(&depot->lock);
+	*nstacks = depot->nstacks;
+	*memory = depot->stack_offset;
+	kt_spin_unlock(&depot->lock);
+}
+
diff --git a/mm/ktsan/stat.c b/mm/ktsan/stat.c
new file mode 100644
index 000000000000..f32b1992bcc4
--- /dev/null
+++ b/mm/ktsan/stat.c
@@ -0,0 +1,81 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+#include <linux/atomic.h>
+#include <linux/fs.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+#include <linux/utsname.h>
+
+static struct {
+	kt_stat_t	i;
+	const char	*s;
+} desc[] = {
+	{kt_stat_reports,		"reports"},
+	{kt_stat_access_read,		"access_read"},
+	{kt_stat_access_write,		"access_write"},
+	{kt_stat_sync_objects,		"sync_objects"},
+	{kt_stat_sync_alloc,		"sync_alloc"},
+	{kt_stat_sync_free,		"sync_free"},
+	{kt_stat_memblock_objects,	"memblock_objects"},
+	{kt_stat_memblock_alloc,	"memblock_alloc"},
+	{kt_stat_memblock_free,		"memblock_free"},
+	{kt_stat_threads,		"threads"},
+	{kt_stat_thread_create,		"thread_create"},
+	{kt_stat_thread_destroy,	"thread_destroy"},
+	{kt_stat_acquire,		"acquire"},
+	{kt_stat_release,		"release"},
+	{kt_stat_func_entry,		"func_entry"},
+	{kt_stat_func_exit,		"func_exit"},
+	{kt_stat_trace_event,		"trace_event"},
+};
+
+void kt_stat_collect(kt_stats_t *stat)
+{
+	kt_stats_t *stat1;
+	int cpu, i;
+
+	memset(stat, 0, sizeof(*stat));
+	for_each_possible_cpu(cpu) {
+		stat1 = &per_cpu_ptr(kt_ctx.cpus, cpu)->stat;
+		for (i = 0; i < kt_stat_count; i++)
+			stat->stat[i] += stat1->stat[i];
+	}
+}
+
+static int kt_stat_show(struct seq_file *m, void *v)
+{
+	kt_stats_t stat;
+	unsigned long depot_nstacks, depot_memory;
+	int i;
+
+	kt_stat_collect(&stat);
+	for (i = 0; i < ARRAY_SIZE(desc); i++)
+		seq_printf(m, "%s: %lu\n", desc[i].s, stat.stat[desc[i].i]);
+	seq_printf(m, "shadow_memory_mb: %lu\n",
+		(kt_shadow_pages * PAGE_SIZE) >> 20);
+	kt_stack_depot_stats(&kt_ctx.stack_depot,
+		&depot_nstacks, &depot_memory);
+	seq_printf(m, "depot_nstacks: %lu\n", depot_nstacks);
+	seq_printf(m, "depot_memory_mb: %lu\n", depot_memory >> 20);
+	return 0;
+}
+
+static int kt_stat_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, kt_stat_show, NULL);
+}
+
+static const struct file_operations kt_stat_ops = {
+	.open		= kt_stat_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release
+};
+
+void kt_stat_init(void)
+{
+	proc_create("ktsan_stats", S_IRUSR, NULL, &kt_stat_ops);
+}
diff --git a/mm/ktsan/supp.c b/mm/ktsan/supp.c
new file mode 100644
index 000000000000..af917a921408
--- /dev/null
+++ b/mm/ktsan/supp.c
@@ -0,0 +1,60 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+#include <linux/kernel.h>
+#include <linux/kallsyms.h>
+
+#ifndef CONFIG_KALLSYMS
+#error "KTSAN suppressions require CONFIG_KALLSYMS"
+#endif
+
+struct kt_supp_s {
+	const char *func;
+	unsigned long start;
+	unsigned long end;
+	unsigned hits;
+};
+typedef struct kt_supp_s kt_supp_t;
+
+static kt_supp_t suppressions[] = {
+	{"generic_fillattr"},		/* Non-atomic read/write of timespec. https://lkml.org/lkml/2015/8/28/400 */
+	{"atime_needs_update"},		/* Non-atomic read/write of timespec */
+	{"generic_update_time"},	/* Non-atomic read/write of timespec */
+	{"tcp_poll"},			/* Race on tp->rcv_nxt. Seems benign */
+	{"vm_stat_account"},		/* Race on mm->total_vm. Stat accounting */
+	{"radix_tree_next_chunk"},	/* Race when checking root_tag_get */
+	{"pid_revalidate"},		/* Seems to be intentionally racy write to inode */
+};
+
+void kt_supp_init(void)
+{
+	int res, i;
+	unsigned long pc, size, off;
+	kt_supp_t *s;
+
+	for (i = 0; i < ARRAY_SIZE(suppressions); i++) {
+		s = &suppressions[i];
+		pc = kallsyms_lookup_name(s->func);
+		BUG_ON(pc == 0);
+		size = off = 0;
+		res = kallsyms_lookup_size_offset(pc, &size, &off);
+		BUG_ON(!res || size == 0 || off != 0);
+		s->start = pc;
+		s->end = pc + size;
+	}
+}
+
+bool kt_supp_suppressed(unsigned long pc)
+{
+	int i;
+	kt_supp_t *s;
+
+	for (i = 0; i < ARRAY_SIZE(suppressions); i++) {
+		s = &suppressions[i];
+		if (pc >= s->start && pc < s->end) {
+			kt_atomic32_fetch_add_no_ktsan(&s->hits, 1);
+			return true;
+		}
+	}
+	return false;
+}
diff --git a/mm/ktsan/sync.c b/mm/ktsan/sync.c
new file mode 100644
index 000000000000..8a4bb1acea6c
--- /dev/null
+++ b/mm/ktsan/sync.c
@@ -0,0 +1,122 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+#include <linux/list.h>
+#include <linux/mmzone.h>
+#include <linux/spinlock.h>
+
+static bool is_page_struct_addr(uptr_t addr)
+{
+	uptr_t start, end;
+
+	/* TODO: works only with UMA. */
+	start = (uptr_t)pfn_to_page(NODE_DATA(0)->node_start_pfn);
+	end = (uptr_t)(pfn_to_page(NODE_DATA(0)->node_start_pfn)
+		+ NODE_DATA(0)->node_spanned_pages);
+	return start <= addr && addr < end;
+}
+
+
+kt_tab_sync_t *kt_sync_ensure_created(kt_thr_t *thr, uptr_t pc, uptr_t addr)
+{
+	kt_tab_sync_t *sync;
+	bool created;
+	uptr_t memblock_addr;
+
+	/* Ignore all atomics that are stored inside page_struct structs.
+	   This significantly reduces the number of syncs objects. This might
+	   potentially lead to false positives, but none were observed. */
+	if (is_page_struct_addr(addr))
+		return NULL;
+
+	sync = kt_tab_access(&kt_ctx.sync_tab, addr, &created, false);
+	if (sync == NULL) {
+		pr_err("KTSAN: limit on number of sync objects is reached\n");
+#if KT_DEBUG
+		kt_report_sync_usage();
+#endif
+		BUG();
+	}
+
+	if (created) {
+		kt_clk_init(&sync->clk);
+		sync->lock_tid = -1;
+		INIT_LIST_HEAD(&sync->list);
+		sync->pc = pc;
+
+		memblock_addr = kt_memblock_addr(addr);
+		kt_memblock_add_sync(thr, memblock_addr, sync);
+
+		/* Allocated unique id for the sync object. */
+		if (thr->cpu->sync_uid_pos == thr->cpu->sync_uid_end) {
+			const u64 batch = 64;
+
+			thr->cpu->sync_uid_pos = kt_atomic64_fetch_add_no_ktsan
+				(&kt_ctx.sync_uid_gen, batch);
+			thr->cpu->sync_uid_end = thr->cpu->sync_uid_pos + batch;
+		}
+		sync->uid = thr->cpu->sync_uid_pos++;
+
+		kt_stat_inc(kt_stat_sync_objects);
+		kt_stat_inc(kt_stat_sync_alloc);
+	}
+
+	return sync;
+}
+
+/* Removes sync object from hash table and frees it. */
+void kt_sync_free(kt_thr_t *thr, uptr_t addr)
+{
+	kt_tab_sync_t *sync;
+
+	sync = kt_tab_access(&kt_ctx.sync_tab, addr, NULL, true);
+	BUG_ON(sync == NULL);
+
+	kt_spin_unlock(&sync->tab.lock);
+	kt_cache_free(&kt_ctx.sync_tab.obj_cache, sync);
+
+	kt_stat_dec(kt_stat_sync_objects);
+	kt_stat_inc(kt_stat_sync_free);
+}
+
+void kt_sync_acquire(kt_thr_t *thr, uptr_t pc, uptr_t addr)
+{
+	kt_tab_sync_t *sync;
+
+#if KT_DEBUG
+	kt_trace_add_event(thr, kt_event_acquire, kt_compress(pc));
+#endif /* KT_DEBUG */
+
+	sync = kt_sync_ensure_created(thr, pc, addr);
+	if (sync == NULL)
+		return;
+	kt_acquire(thr, pc, sync);
+	kt_spin_unlock(&sync->tab.lock);
+}
+
+void kt_sync_release(kt_thr_t *thr, uptr_t pc, uptr_t addr)
+{
+	kt_tab_sync_t *sync;
+
+#if KT_DEBUG
+	kt_trace_add_event(thr, kt_event_release, kt_compress(pc));
+#endif /* KT_DEBUG */
+
+	sync = kt_sync_ensure_created(thr, pc, addr);
+	if (sync == NULL)
+		return;
+	kt_release(thr, pc, sync);
+	kt_spin_unlock(&sync->tab.lock);
+}
+
+void kt_acquire(kt_thr_t *thr, uptr_t pc, kt_tab_sync_t *sync)
+{
+	kt_stat_inc(kt_stat_acquire);
+	kt_clk_acquire(&thr->clk, &sync->clk);
+}
+
+void kt_release(kt_thr_t *thr, uptr_t pc, kt_tab_sync_t *sync)
+{
+	kt_stat_inc(kt_stat_release);
+	kt_clk_acquire(&sync->clk, &thr->clk);
+}
diff --git a/mm/ktsan/sync_atomic.c b/mm/ktsan/sync_atomic.c
new file mode 100644
index 000000000000..0c8e11418361
--- /dev/null
+++ b/mm/ktsan/sync_atomic.c
@@ -0,0 +1,392 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+#include <linux/atomic.h>
+#include <linux/spinlock.h>
+
+void kt_thread_fence(kt_thr_t *thr, uptr_t pc, ktsan_memory_order_t mo)
+{
+	if (mo == ktsan_memory_order_acquire ||
+	    mo == ktsan_memory_order_acq_rel) {
+		if (thr->acquire_active) {
+#if KT_DEBUG
+			kt_trace_add_event(thr, kt_event_membar_acquire,
+						kt_compress(pc));
+#endif
+			kt_clk_acquire(&thr->clk, &thr->acquire_clk);
+			kt_stat_inc(kt_stat_acquire);
+		}
+	}
+
+	kt_thread_fence_no_ktsan(mo);
+
+	if (mo == ktsan_memory_order_release ||
+	    mo == ktsan_memory_order_acq_rel) {
+#if KT_DEBUG
+		kt_trace_add_event(thr, kt_event_membar_release,
+					kt_compress(pc));
+#endif
+		if (thr->release_active)
+			kt_clk_acquire(&thr->release_clk, &thr->clk);
+		else
+			kt_clk_set(&thr->release_clk, &thr->clk);
+		thr->release_active = KT_TAME_COUNTER_LIMIT;
+	}
+}
+
+static kt_tab_sync_t *kt_atomic_pre_op(kt_thr_t *thr, uptr_t pc, uptr_t addr,
+				size_t size, ktsan_memory_order_t mo,
+				bool read, bool write, kt_tab_sync_t *sync)
+{
+	/* This will catch races between atomic operations and non-atomic
+	 * writes (in particular with kfree).
+	 */
+	if (write)
+		kt_access(thr, pc, addr, size, true, true);
+
+	if (mo == ktsan_memory_order_release ||
+	    mo == ktsan_memory_order_acq_rel)
+		kt_thread_fence_no_ktsan(ktsan_memory_order_release);
+
+	if (mo == ktsan_memory_order_release ||
+	    mo == ktsan_memory_order_acq_rel) {
+		if (sync == NULL)
+			sync = kt_sync_ensure_created(thr, pc, addr);
+		if (sync == NULL)
+			return NULL;
+#if KT_DEBUG
+		kt_trace_add_event(thr, kt_event_release, kt_compress(pc));
+#endif /* KT_DEBUG */
+		kt_release(thr, pc, sync);
+	} else if (write) {
+		if (thr->release_active) {
+			if (sync == NULL)
+				sync = kt_sync_ensure_created(thr, pc, addr);
+			if (sync == NULL)
+				return NULL;
+#if KT_DEBUG
+			kt_trace_add_event(thr, kt_event_nonmat_release,
+						kt_compress(pc));
+#endif /* KT_DEBUG */
+			kt_clk_acquire(&sync->clk, &thr->release_clk);
+			kt_stat_inc(kt_stat_release);
+		}
+	}
+
+	return sync;
+}
+
+static kt_tab_sync_t *kt_atomic_post_op(kt_thr_t *thr, uptr_t pc, uptr_t addr,
+				size_t size, ktsan_memory_order_t mo,
+				bool read, bool write, kt_tab_sync_t *sync)
+{
+	if (mo == ktsan_memory_order_acquire ||
+	    mo == ktsan_memory_order_acq_rel)
+		kt_thread_fence_no_ktsan(ktsan_memory_order_acquire);
+
+	if (sync == NULL) {
+		sync = kt_tab_access(&kt_ctx.sync_tab, addr, NULL, false);
+		if (sync == NULL)
+			return NULL;
+	}
+
+	if (mo == ktsan_memory_order_acquire ||
+	    mo == ktsan_memory_order_acq_rel) {
+#if KT_DEBUG
+		kt_trace_add_event(thr, kt_event_acquire, kt_compress(pc));
+#endif /* KT_DEBUG */
+		kt_acquire(thr, pc, sync);
+	} else if (read) {
+#if KT_DEBUG
+		kt_trace_add_event(thr, kt_event_nonmat_acquire,
+					kt_compress(pc));
+#endif /* KT_DEBUG */
+		if (thr->acquire_active)
+			kt_clk_acquire(&thr->acquire_clk, &sync->clk);
+		else
+			kt_clk_set(&thr->acquire_clk, &sync->clk);
+		thr->acquire_active = KT_TAME_COUNTER_LIMIT;
+	}
+
+	/* This will catch races between atomic operations and non-atomic
+	 * writes (in particular with kfree).
+	 */
+	if (read && !write)
+		kt_access(thr, pc, addr, size, true, true);
+
+	return sync;
+}
+
+#define KT_ATOMIC_OP(op, ad, size, mo, read, write)			\
+do {									\
+	kt_tab_sync_t *sync = NULL;					\
+									\
+	sync = kt_atomic_pre_op(thr, pc, ad, size,			\
+				mo, read, write, sync);			\
+									\
+	(op);								\
+									\
+	sync = kt_atomic_post_op(thr, pc, ad, size,			\
+				 mo, read, write, sync);		\
+									\
+	if (sync != NULL)						\
+		kt_spin_unlock(&sync->tab.lock);			\
+} while (0)
+
+void kt_atomic8_store(kt_thr_t *thr, uptr_t pc,
+		void *addr, u8 value, ktsan_memory_order_t mo)
+{
+	KT_ATOMIC_OP(kt_atomic8_store_no_ktsan(addr, value),
+		(uptr_t)addr, KT_ACCESS_SIZE_1, mo, false, true);
+}
+
+void kt_atomic16_store(kt_thr_t *thr, uptr_t pc,
+		void *addr, u16 value, ktsan_memory_order_t mo)
+{
+	KT_ATOMIC_OP(kt_atomic16_store_no_ktsan(addr, value),
+		(uptr_t)addr, KT_ACCESS_SIZE_2, mo, false, true);
+}
+
+void kt_atomic32_store(kt_thr_t *thr, uptr_t pc,
+		void *addr, u32 value, ktsan_memory_order_t mo)
+{
+	KT_ATOMIC_OP(kt_atomic32_store_no_ktsan(addr, value),
+		(uptr_t)addr, KT_ACCESS_SIZE_4, mo, false, true);
+}
+
+void kt_atomic64_store(kt_thr_t *thr, uptr_t pc,
+		void *addr, u64 value, ktsan_memory_order_t mo)
+{
+	KT_ATOMIC_OP(kt_atomic64_store_no_ktsan(addr, value),
+		(uptr_t)addr, KT_ACCESS_SIZE_8, mo, false, true);
+}
+
+u8 kt_atomic8_load(kt_thr_t *thr, uptr_t pc,
+		const void *addr, ktsan_memory_order_t mo)
+{
+	u8 rv;
+
+	KT_ATOMIC_OP(rv = kt_atomic8_load_no_ktsan(addr),
+		(uptr_t)addr, KT_ACCESS_SIZE_1, mo, true, false);
+
+	return rv;
+}
+
+u16 kt_atomic16_load(kt_thr_t *thr, uptr_t pc,
+		const void *addr, ktsan_memory_order_t mo)
+{
+	u16 rv;
+
+	KT_ATOMIC_OP(rv = kt_atomic16_load_no_ktsan(addr),
+		(uptr_t)addr, KT_ACCESS_SIZE_2, mo, true, false);
+
+	return rv;
+}
+
+u32 kt_atomic32_load(kt_thr_t *thr, uptr_t pc,
+		const void *addr, ktsan_memory_order_t mo)
+{
+	u32 rv;
+
+	KT_ATOMIC_OP(rv = kt_atomic32_load_no_ktsan(addr),
+		(uptr_t)addr, KT_ACCESS_SIZE_4, mo, true, false);
+
+	return rv;
+}
+
+u64 kt_atomic64_load(kt_thr_t *thr, uptr_t pc,
+		const void *addr, ktsan_memory_order_t mo)
+{
+	u64 rv;
+
+	KT_ATOMIC_OP(rv = kt_atomic64_load_no_ktsan(addr),
+		(uptr_t)addr, KT_ACCESS_SIZE_8, mo, true, false);
+
+	return rv;
+}
+
+u8 kt_atomic8_exchange(kt_thr_t *thr, uptr_t pc,
+		void *addr, u8 value, ktsan_memory_order_t mo)
+{
+	u8 rv;
+
+	KT_ATOMIC_OP(rv = kt_atomic8_exchange_no_ktsan(addr, value),
+		(uptr_t)addr, KT_ACCESS_SIZE_1, mo, true, true);
+
+	return rv;
+}
+
+u16 kt_atomic16_exchange(kt_thr_t *thr, uptr_t pc,
+		void *addr, u16 value, ktsan_memory_order_t mo)
+{
+	u16 rv;
+
+	KT_ATOMIC_OP(rv = kt_atomic16_exchange_no_ktsan(addr, value),
+		(uptr_t)addr, KT_ACCESS_SIZE_2, mo, true, true);
+
+	return rv;
+}
+
+u32 kt_atomic32_exchange(kt_thr_t *thr, uptr_t pc,
+		void *addr, u32 value, ktsan_memory_order_t mo)
+{
+	u32 rv;
+
+	KT_ATOMIC_OP(rv = kt_atomic32_exchange_no_ktsan(addr, value),
+		(uptr_t)addr, KT_ACCESS_SIZE_4, mo, true, true);
+
+	return rv;
+}
+
+u64 kt_atomic64_exchange(kt_thr_t *thr, uptr_t pc,
+		void *addr, u64 value, ktsan_memory_order_t mo)
+{
+	u64 rv;
+
+	KT_ATOMIC_OP(rv = kt_atomic64_exchange_no_ktsan(addr, value),
+		(uptr_t)addr, KT_ACCESS_SIZE_8, mo, true, true);
+
+	return rv;
+}
+
+u8 kt_atomic8_compare_exchange(kt_thr_t *thr, uptr_t pc,
+		void *addr, u8 old, u8 new, ktsan_memory_order_t mo)
+{
+	u8 rv;
+
+	KT_ATOMIC_OP(rv = kt_atomic8_compare_exchange_no_ktsan(addr, old, new),
+		(uptr_t)addr, KT_ACCESS_SIZE_1, mo, true, true);
+
+	return rv;
+}
+
+u16 kt_atomic16_compare_exchange(kt_thr_t *thr, uptr_t pc,
+		void *addr, u16 old, u16 new, ktsan_memory_order_t mo)
+{
+	u16 rv;
+
+	KT_ATOMIC_OP(rv = kt_atomic16_compare_exchange_no_ktsan(addr, old, new),
+		(uptr_t)addr, KT_ACCESS_SIZE_2, mo, true, true);
+
+	return rv;
+}
+
+u32 kt_atomic32_compare_exchange(kt_thr_t *thr, uptr_t pc,
+		void *addr, u32 old, u32 new, ktsan_memory_order_t mo)
+{
+	u32 rv;
+
+	KT_ATOMIC_OP(rv = kt_atomic32_compare_exchange_no_ktsan(addr, old, new),
+		(uptr_t)addr, KT_ACCESS_SIZE_4, mo, true, true);
+
+	return rv;
+}
+
+u64 kt_atomic64_compare_exchange(kt_thr_t *thr, uptr_t pc,
+		void *addr, u64 old, u64 new, ktsan_memory_order_t mo)
+{
+	u64 rv;
+
+	KT_ATOMIC_OP(rv = kt_atomic64_compare_exchange_no_ktsan(addr, old, new),
+		(uptr_t)addr, KT_ACCESS_SIZE_8, mo, true, true);
+
+	return rv;
+}
+
+u8 kt_atomic8_fetch_add(kt_thr_t *thr, uptr_t pc,
+		void *addr, u8 value, ktsan_memory_order_t mo)
+{
+	u8 rv;
+
+	KT_ATOMIC_OP(rv = kt_atomic8_fetch_add_no_ktsan(addr, value),
+		(uptr_t)addr, KT_ACCESS_SIZE_1, mo, true, true);
+
+	return rv;
+}
+
+u16 kt_atomic16_fetch_add(kt_thr_t *thr, uptr_t pc,
+		void *addr, u16 value, ktsan_memory_order_t mo)
+{
+	u16 rv;
+
+	KT_ATOMIC_OP(rv = kt_atomic16_fetch_add_no_ktsan(addr, value),
+		(uptr_t)addr, KT_ACCESS_SIZE_2, mo, true, true);
+
+	return rv;
+}
+
+u32 kt_atomic32_fetch_add(kt_thr_t *thr, uptr_t pc,
+		void *addr, u32 value, ktsan_memory_order_t mo)
+{
+	u32 rv;
+
+	KT_ATOMIC_OP(rv = kt_atomic32_fetch_add_no_ktsan(addr, value),
+		(uptr_t)addr, KT_ACCESS_SIZE_4, mo, true, true);
+
+	return rv;
+}
+
+u64 kt_atomic64_fetch_add(kt_thr_t *thr, uptr_t pc,
+		void *addr, u64 value, ktsan_memory_order_t mo)
+{
+	u64 rv;
+
+	KT_ATOMIC_OP(rv = kt_atomic64_fetch_add_no_ktsan(addr, value),
+		(uptr_t)addr, KT_ACCESS_SIZE_8, mo, true, true);
+
+	return rv;
+}
+
+void kt_atomic_set_bit(kt_thr_t *thr, uptr_t pc,
+		void *addr, long nr, ktsan_memory_order_t mo)
+{
+	KT_ATOMIC_OP(kt_atomic_set_bit_no_ktsan(addr, nr),
+		(uptr_t)addr + (nr >> 3), KT_ACCESS_SIZE_1, mo, false, true);
+}
+
+void kt_atomic_clear_bit(kt_thr_t *thr, uptr_t pc,
+		void *addr, long nr, ktsan_memory_order_t mo)
+{
+	KT_ATOMIC_OP(kt_atomic_clear_bit_no_ktsan(addr, nr),
+		(uptr_t)addr + (nr >> 3), KT_ACCESS_SIZE_1, mo, false, true);
+}
+
+void kt_atomic_change_bit(kt_thr_t *thr, uptr_t pc,
+		void *addr, long nr, ktsan_memory_order_t mo)
+{
+	KT_ATOMIC_OP(kt_atomic_change_bit_no_ktsan(addr, nr),
+		(uptr_t)addr + (nr >> 3), KT_ACCESS_SIZE_1, mo, true, true);
+}
+
+int kt_atomic_fetch_set_bit(kt_thr_t *thr, uptr_t pc,
+		void *addr, long nr, ktsan_memory_order_t mo)
+{
+	int rv;
+
+	KT_ATOMIC_OP(rv = kt_atomic_fetch_set_bit_no_ktsan(addr, nr),
+		(uptr_t)addr + (nr >> 3), KT_ACCESS_SIZE_1, mo, true, true);
+
+	return rv;
+}
+
+int kt_atomic_fetch_clear_bit(kt_thr_t *thr, uptr_t pc,
+		void *addr, long nr, ktsan_memory_order_t mo)
+{
+	int rv;
+
+	KT_ATOMIC_OP(rv = kt_atomic_fetch_clear_bit_no_ktsan(addr, nr),
+		(uptr_t)addr + (nr >> 3), KT_ACCESS_SIZE_1, mo, true, true);
+
+	return rv;
+}
+
+int kt_atomic_fetch_change_bit(kt_thr_t *thr, uptr_t pc,
+		void *addr, long nr, ktsan_memory_order_t mo)
+{
+	int rv;
+
+	KT_ATOMIC_OP(rv = kt_atomic_fetch_change_bit_no_ktsan(addr, nr),
+		(uptr_t)addr + (nr >> 3), KT_ACCESS_SIZE_1, mo, true, true);
+
+	return rv;
+}
diff --git a/mm/ktsan/sync_atomic_no_ktsan.c b/mm/ktsan/sync_atomic_no_ktsan.c
new file mode 100644
index 000000000000..bc9493fa2661
--- /dev/null
+++ b/mm/ktsan/sync_atomic_no_ktsan.c
@@ -0,0 +1,114 @@
+// SPDX-License-Identifier: GPL-2.0
+#undef CONFIG_KTSAN
+
+#include <linux/atomic.h>
+#include <linux/bitops.h>
+
+#include <asm/barrier.h>
+
+void kt_thread_fence_no_ktsan(ktsan_memory_order_t mo)
+{
+	switch (mo) {
+	case ktsan_memory_order_acquire:
+		rmb();
+		break;
+	case ktsan_memory_order_release:
+		wmb();
+		break;
+	case ktsan_memory_order_acq_rel:
+		mb();
+		break;
+	default:
+		break;
+	}
+}
+
+u8 kt_atomic8_exchange_no_ktsan(void *addr, u8 value)
+{
+	return xchg((u8 *)addr, value);
+}
+
+u16 kt_atomic16_exchange_no_ktsan(void *addr, u16 value)
+{
+	return xchg((u16 *)addr, value);
+}
+
+u32 kt_atomic32_exchange_no_ktsan(void *addr, u32 value)
+{
+	return xchg((u32 *)addr, value);
+}
+
+u64 kt_atomic64_exchange_no_ktsan(void *addr, u64 value)
+{
+	return xchg((u64 *)addr, value);
+}
+
+u8 kt_atomic8_compare_exchange_no_ktsan(void *addr, u8 old, u8 new)
+{
+	return cmpxchg((u8 *)addr, old, new);
+}
+
+u16 kt_atomic16_compare_exchange_no_ktsan(void *addr, u16 old, u16 new)
+{
+	return cmpxchg((u16 *)addr, old, new);
+}
+
+u32 kt_atomic32_compare_exchange_no_ktsan(void *addr, u32 old, u32 new)
+{
+	return cmpxchg((u32 *)addr, old, new);
+}
+
+u64 kt_atomic64_compare_exchange_no_ktsan(void *addr, u64 old, u64 new)
+{
+	return cmpxchg((u64 *)addr, old, new);
+}
+
+u8 kt_atomic8_fetch_add_no_ktsan(void *addr, u8 value)
+{
+	return xadd((u8 *)addr, value);
+}
+
+u16 kt_atomic16_fetch_add_no_ktsan(void *addr, u16 value)
+{
+	return xadd((u16 *)addr, value);
+}
+
+u32 kt_atomic32_fetch_add_no_ktsan(void *addr, u32 value)
+{
+	return xadd((u32 *)addr, value);
+}
+
+u64 kt_atomic64_fetch_add_no_ktsan(void *addr, u64 value)
+{
+	return xadd((u64 *)addr, value);
+}
+
+void kt_atomic_set_bit_no_ktsan(void *addr, long nr)
+{
+	set_bit(nr, addr);
+}
+
+void kt_atomic_clear_bit_no_ktsan(void *addr, long nr)
+{
+	clear_bit(nr, addr);
+}
+
+void kt_atomic_change_bit_no_ktsan(void *addr, long nr)
+{
+	change_bit(nr, addr);
+}
+
+int kt_atomic_fetch_set_bit_no_ktsan(void *addr, long nr)
+{
+	return test_and_set_bit(nr, addr);
+}
+
+int kt_atomic_fetch_clear_bit_no_ktsan(void *addr, long nr)
+{
+	return test_and_clear_bit(nr, addr);
+}
+
+int kt_atomic_fetch_change_bit_no_ktsan(void *addr, long nr)
+{
+	return test_and_change_bit(nr, addr);
+}
diff --git a/mm/ktsan/sync_mtx.c b/mm/ktsan/sync_mtx.c
new file mode 100644
index 000000000000..17f2f7feaa0f
--- /dev/null
+++ b/mm/ktsan/sync_mtx.c
@@ -0,0 +1,91 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+#include <linux/spinlock.h>
+
+void kt_mtx_pre_lock(kt_thr_t *thr, uptr_t pc, uptr_t addr, bool wr, bool try)
+{
+	/* Will be used for deadlock detection.
+	   We can also put sleeps for random time here. */
+}
+
+void kt_mtx_post_lock(kt_thr_t *thr, uptr_t pc, uptr_t addr, bool wr, bool try,
+		      bool success)
+{
+	kt_tab_sync_t *sync;
+
+	/* Sometimes even locks that are not trylocks might fail.
+	   For example a thread calling mutex_lock might be rescheduled.
+	   In that case we call mtx_post_lock(try = false, success = false). */
+	/* BUG_ON(!try && !success); */
+
+	if (!success)
+		return;
+
+	sync = kt_sync_ensure_created(thr, pc, addr);
+	if (!sync)
+		return;
+
+	/* This can catch unsafe publication of a mutex. */
+	kt_access(thr, pc, addr, KT_ACCESS_SIZE_1, true, false);
+
+	kt_mutex_lock(thr, pc, sync->uid, wr);
+
+	kt_acquire(thr, pc, sync);
+
+	BUG_ON(sync->lock_tid != -1);
+	if (wr)
+		sync->lock_tid = thr->id;
+	sync->last_lock_time = kt_clk_get(&thr->clk, thr->id);
+
+	kt_spin_unlock(&sync->tab.lock);
+}
+
+void kt_mtx_pre_unlock(kt_thr_t *thr, uptr_t pc, uptr_t addr, bool wr)
+{
+	kt_tab_sync_t *sync;
+
+	sync = kt_sync_ensure_created(thr, pc, addr);
+	if (!sync)
+		return;
+
+	/* This can catch race between unlock and mutex destruction. */
+	kt_access(thr, pc, addr, KT_ACCESS_SIZE_1, true, false);
+
+	kt_mutex_unlock(thr, sync->uid, wr);
+
+	kt_release(thr, pc, sync);
+
+	if (wr) {
+		BUG_ON(sync->lock_tid == -1);
+		if (wr && sync->lock_tid != thr->id)
+			kt_report_bad_mtx_unlock(thr, pc, sync);
+		sync->lock_tid = -1;
+	}
+	BUG_ON(sync->lock_tid != -1);
+	sync->last_unlock_time = kt_clk_get(&thr->clk, thr->id);
+
+	kt_spin_unlock(&sync->tab.lock);
+}
+
+void kt_mtx_post_unlock(kt_thr_t *thr, uptr_t pc, uptr_t addr, bool wr)
+{
+}
+
+void kt_mtx_downgrade(kt_thr_t *thr, uptr_t pc, uptr_t addr)
+{
+	kt_tab_sync_t *sync;
+
+	sync = kt_sync_ensure_created(thr, pc, addr);
+	if (!sync)
+		return;
+
+	kt_mutex_downgrade(thr, sync->uid);
+
+	BUG_ON(sync->lock_tid == -1);
+	if (sync->lock_tid != thr->id)
+		kt_report_bad_mtx_unlock(thr, pc, sync);
+	sync->lock_tid = -1;
+
+	kt_spin_unlock(&sync->tab.lock);
+}
diff --git a/mm/ktsan/sync_percpu.c b/mm/ktsan/sync_percpu.c
new file mode 100644
index 000000000000..fde1db030f1a
--- /dev/null
+++ b/mm/ktsan/sync_percpu.c
@@ -0,0 +1,103 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+#include <linux/irqflags.h>
+#include <linux/list.h>
+#include <linux/preempt.h>
+
+void kt_percpu_release(kt_thr_t *thr, uptr_t pc)
+{
+	struct list_head *entry, *tmp;
+	kt_percpu_sync_t *sync;
+
+	list_for_each_safe(entry, tmp, &thr->percpu_list) {
+		sync = list_entry(entry, kt_percpu_sync_t, list);
+		list_del(entry);
+		kt_sync_release(thr, pc, sync->addr);
+		kt_cache_free(&kt_ctx.percpu_sync_cache, sync);
+	}
+}
+
+static void kt_percpu_try_release(kt_thr_t *thr, uptr_t pc)
+{
+	if (thr->preempt_disable_depth > 0 || thr->irqs_disabled)
+		return;
+
+	kt_percpu_release(thr, pc);
+}
+
+void kt_percpu_acquire(kt_thr_t *thr, uptr_t pc, uptr_t addr)
+{
+	kt_percpu_sync_t *percpu_sync;
+
+	/* Since a sync object may be the first field of a per-cpu structure,
+	   the per-cpu sync is bound to the second byte of the structure. */
+	addr += 1;
+
+	/* This BUG_ON is failing since a pointer to a per-cpu structure
+	   may be acquired via &__get_cpu_var(...) before disabling
+	   irqs/preemption without actually accessing the structure itself. */
+	/* BUG_ON(thr->preempt_disable_depth == 0 && !thr->irqs_disabled); */
+
+	kt_sync_acquire(thr, pc, addr);
+
+	list_for_each_entry(percpu_sync, &thr->percpu_list, list)
+		if (percpu_sync->addr == addr)
+			return;
+
+	percpu_sync = kt_cache_alloc(&kt_ctx.percpu_sync_cache);
+	BUG_ON(percpu_sync == NULL); /* Ran out of memory. */
+	percpu_sync->addr = addr;
+	INIT_LIST_HEAD(&percpu_sync->list);
+	list_add(&percpu_sync->list, &thr->percpu_list);
+}
+
+void kt_preempt_add(kt_thr_t *thr, uptr_t pc, int value)
+{
+#if KT_DEBUG
+	kt_trace_add_event(thr, kt_event_preempt_disable, kt_compress(pc));
+#endif /* KT_DEBUG */
+
+	thr->preempt_disable_depth += value;
+}
+
+void kt_preempt_sub(kt_thr_t *thr, uptr_t pc, int value)
+{
+#if KT_DEBUG
+	kt_trace_add_event(thr, kt_event_preempt_enable, kt_compress(pc));
+#endif /* KT_DEBUG */
+
+	thr->preempt_disable_depth -= value;
+	BUG_ON(thr->preempt_disable_depth < 0);
+	kt_percpu_try_release(thr, pc);
+}
+
+void kt_irq_disable(kt_thr_t *thr, uptr_t pc)
+{
+#if KT_DEBUG
+	kt_trace_add_event(thr, kt_event_irq_disable, kt_compress(pc));
+#endif /* KT_DEBUG */
+
+	thr->irqs_disabled = true;
+}
+
+void kt_irq_enable(kt_thr_t *thr, uptr_t pc)
+{
+#if KT_DEBUG
+	kt_trace_add_event(thr, kt_event_irq_enable, kt_compress(pc));
+#endif /* KT_DEBUG */
+
+	thr->irqs_disabled = false;
+	kt_percpu_try_release(thr, pc);
+}
+
+void kt_irq_save(kt_thr_t *thr, uptr_t pc)
+{
+	kt_irq_disable(thr, pc);
+}
+
+void kt_irq_restore(kt_thr_t *thr, uptr_t pc, unsigned long flags)
+{
+	if (!irqs_disabled_flags(flags))
+		kt_irq_enable(thr, pc);
+}
diff --git a/mm/ktsan/sync_seqlock.c b/mm/ktsan/sync_seqlock.c
new file mode 100644
index 000000000000..630b112d33de
--- /dev/null
+++ b/mm/ktsan/sync_seqlock.c
@@ -0,0 +1,84 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+void kt_seqcount_begin(kt_thr_t *thr, uptr_t pc, uptr_t addr)
+{
+	int i;
+
+	if (thr->seqcount_ignore)
+		return;
+
+	/* Find a slot for this seqcount and store it. */
+	BUG_ON(addr == 0);
+	for (i = 0; i < ARRAY_SIZE(thr->seqcount); i++) {
+		if (thr->seqcount[i] == 0) {
+			thr->seqcount[i] = addr;
+			thr->seqcount_pc[i] = pc;
+			break;
+		}
+	}
+	if (i == ARRAY_SIZE(thr->seqcount))
+		kt_seqcount_bug(thr, addr, "seqcount overflow");
+
+	thr->read_disable_depth++;
+	if (thr->read_disable_depth > ARRAY_SIZE(thr->seqcount) + 1)
+		kt_seqcount_bug(thr, addr, "read_disable_depth overflow");
+}
+
+void kt_seqcount_end(kt_thr_t *thr, uptr_t pc, uptr_t addr)
+{
+	int i;
+
+	if (thr->seqcount_ignore)
+		return;
+
+	/* Find and remove the seqcount (reversed to support nested locks). */
+	BUG_ON(addr == 0);
+	for (i = ARRAY_SIZE(thr->seqcount) - 1; i >= 0; i--) {
+		if (thr->seqcount[i] == addr) {
+			thr->seqcount[i] = 0;
+			thr->seqcount_pc[i] = 0;
+			break;
+		}
+	}
+	if (i < 0)
+		kt_seqcount_bug(thr, addr, "seqcount is not acquired");
+
+	thr->read_disable_depth--;
+	if (thr->read_disable_depth < 0)
+		kt_seqcount_bug(thr, addr, "read_disable_depth underflow");
+}
+
+void kt_seqcount_ignore_begin(kt_thr_t *thr, uptr_t pc)
+{
+	/* This is counter-measure against fs/namei.c. */
+	BUG_ON(thr->seqcount_ignore);
+	thr->seqcount_ignore = 1;
+	thr->read_disable_depth++;
+	if (thr->read_disable_depth > ARRAY_SIZE(thr->seqcount) + 1)
+		kt_seqcount_bug(thr, 0, "read_disable_depth overflow");
+}
+
+void kt_seqcount_ignore_end(kt_thr_t *thr, uptr_t pc)
+{
+	BUG_ON(!thr->seqcount_ignore);
+	thr->seqcount_ignore = 0;
+	thr->read_disable_depth--;
+	if (thr->read_disable_depth < 0)
+		kt_seqcount_bug(thr, 0, "read_disable_depth underflow");
+}
+
+void kt_seqcount_bug(kt_thr_t *thr, uptr_t addr, const char *what)
+{
+	int i;
+
+	pr_err("kt_seqcount_bug: %s\n", what);
+	pr_err(" seqlock=%p read_disable_depth=%d\n",
+		(void *)addr, thr->read_disable_depth);
+	for (i = 0; i < ARRAY_SIZE(thr->seqcount); i++)
+		pr_err(" slot #%d: %p [<%p>] %pS\n", i,
+			(void *)thr->seqcount[i],
+			(void *)thr->seqcount_pc[i],
+			(void *)thr->seqcount_pc[i]);
+	BUG();
+}
diff --git a/mm/ktsan/tab.c b/mm/ktsan/tab.c
new file mode 100644
index 000000000000..2e23f73e948e
--- /dev/null
+++ b/mm/ktsan/tab.c
@@ -0,0 +1,133 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+#include <linux/kernel.h>
+
+/* Only available during early boot. */
+void __init kt_tab_init(kt_tab_t *tab, unsigned size,
+			unsigned obj_size, unsigned obj_max_num)
+{
+	kt_tab_part_t *part;
+	unsigned i;
+
+	tab->size = size;
+	tab->objsize = obj_size;
+
+	kt_cache_init(&tab->parts_cache, sizeof(*tab->parts) * size, 1);
+	tab->parts = kt_cache_alloc(&tab->parts_cache);
+	BUG_ON(tab->parts == NULL);
+
+	for (i = 0; i < size; i++) {
+		part = &tab->parts[i];
+		kt_spin_init(&part->lock);
+		part->head = NULL;
+	}
+
+	kt_cache_init(&tab->obj_cache, obj_size, obj_max_num);
+}
+
+/* Only available during early boot. */
+void __init kt_tab_destroy(kt_tab_t *tab)
+{
+	kt_cache_destroy(&tab->obj_cache);
+	kt_cache_destroy(&tab->parts_cache);
+	tab->parts = NULL;
+}
+
+static inline void *kt_part_access(kt_tab_t *tab, kt_tab_part_t *part,
+				   uptr_t key, bool *created, bool destroy)
+{
+	kt_tab_obj_t *obj;
+	kt_tab_obj_t *prev;
+
+	for (prev = NULL, obj = part->head; obj; prev = obj, obj = obj->link)
+		if (obj->key == key)
+			break;
+
+	/* Get object if exists. */
+	if (created == NULL && destroy == false) {
+		if (obj) {
+			kt_spin_lock(&obj->lock);
+			return obj;
+		}
+		return NULL;
+	}
+
+	/* Remove object from table if exists. */
+	if (created == NULL && destroy == true) {
+		if (obj) {
+			if (!prev)
+				part->head = obj->link;
+			else
+				prev->link = obj->link;
+
+			kt_spin_lock(&obj->lock);
+			return obj;
+		}
+		return NULL;
+	}
+
+	/* Create object if not exists. */
+	if (created != NULL && destroy == false) {
+		if (!obj) {
+			obj = kt_cache_alloc(&tab->obj_cache);
+			if (!obj)
+				return NULL;
+
+			kt_spin_init(&obj->lock);
+			obj->link = part->head;
+			part->head = obj;
+			obj->key = key;
+
+			*created = true;
+		} else {
+			*created = false;
+		}
+
+		kt_spin_lock(&obj->lock);
+		return obj;
+	}
+
+	BUG();
+	return NULL;
+}
+
+/*
+ * When (created == NULL) and (destroy == false)
+ *      returns the object if it exists, returns NULL otherwise.
+ * When (created == NULL) and (destroy == true)
+ *      removes the object from the table if it exists and returns it,
+ *      returns NULL otherwise.
+ *      The object must be freed by the caller via kt_cache_free.
+ * When (created != NULL) and (destroy == false)
+ *      creates an object if it doesn't exist and returns it.
+ *      Sets *created = false if the object existed, *c = true otherwise.
+ * Parameters (created != NULL) and (destroy == true) are incorrect.
+ * The returned object is always locked via spin_lock(object->lock).
+ */
+void *kt_tab_access(kt_tab_t *tab, uptr_t key, bool *created, bool destroy)
+{
+	unsigned int hash;
+	kt_tab_part_t *part;
+	void *result;
+	kt_tab_obj_t *obj;
+
+	BUG_ON(created != NULL && destroy == true);
+
+	hash = key % tab->size;
+	part = &tab->parts[hash];
+
+	kt_spin_lock(&part->lock);
+
+	for (obj = part->head; obj != NULL; obj = obj->link)
+		BUG_ON((uptr_t)obj < PAGE_OFFSET);
+
+	result = kt_part_access(tab, part, key, created, destroy);
+
+	for (obj = part->head; obj != NULL; obj = obj->link)
+		BUG_ON((uptr_t)obj < PAGE_OFFSET);
+
+	kt_spin_unlock(&part->lock);
+
+	return result;
+}
diff --git a/mm/ktsan/tests.c b/mm/ktsan/tests.c
new file mode 100644
index 000000000000..175699fcf5e8
--- /dev/null
+++ b/mm/ktsan/tests.c
@@ -0,0 +1,34 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+#include <linux/fs.h>
+#include <linux/kernel.h>
+#include <linux/proc_fs.h>
+#include <linux/string.h>
+#include <linux/uaccess.h>
+
+static ssize_t kt_tests_write(struct file *file, const char __user *buf,
+			      size_t count, loff_t *offset)
+{
+	char buffer[16];
+
+	memset(buffer, 0, sizeof(buffer));
+	if (count > sizeof(buffer) - 1)
+		count = sizeof(buffer) - 1;
+	if (copy_from_user(buffer, buf, count))
+		return -EFAULT;
+
+	if (!strcmp(buffer, "tsan_run_tests\n"))
+		kt_tests_run();
+
+	return count;
+}
+
+static const struct file_operations kt_tests_operations = {
+	.write = kt_tests_write,
+};
+
+void kt_tests_init(void)
+{
+	proc_create("ktsan_tests", S_IWUSR, NULL, &kt_tests_operations);
+}
diff --git a/mm/ktsan/tests_inst.c b/mm/ktsan/tests_inst.c
new file mode 100644
index 000000000000..73c5e3c757c8
--- /dev/null
+++ b/mm/ktsan/tests_inst.c
@@ -0,0 +1,1081 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+#include <linux/bitops.h>
+#include <linux/completion.h>
+#include <linux/kernel.h>
+#include <linux/kthread.h>
+#include <linux/mutex.h>
+#include <linux/percpu.h>
+#include <linux/percpu-rwsem.h>
+#include <linux/preempt.h>
+#include <linux/printk.h>
+#include <linux/rcupdate.h>
+#include <linux/rwlock.h>
+#include <linux/rwsem.h>
+#include <linux/sched.h>
+#include <linux/semaphore.h>
+#include <linux/slab.h>
+#include <linux/delay.h>
+
+typedef void (*thr_func_t)(void *);
+
+struct thr_arg_s {
+	void *value;
+	thr_func_t func;
+	struct completion *completion;
+};
+
+typedef struct thr_arg_s thr_arg_t;
+
+int thr_func(void *arg)
+{
+	thr_arg_t *thr_arg = (thr_arg_t *)arg;
+
+	thr_arg->func(thr_arg->value);
+	complete(thr_arg->completion);
+
+	return 0;
+}
+
+static volatile int always_false;
+static noinline void use(int x)
+{
+	if (always_false)
+		always_false = x;
+}
+
+DECLARE_COMPLETION(thr_fst_compl);
+DECLARE_COMPLETION(thr_snd_compl);
+
+void kt_test(thr_func_t setup, thr_func_t teardown,
+	     thr_func_t first, thr_func_t second,
+	     const char *name, bool on_stack, bool has_race)
+{
+	struct task_struct *thr_fst, *thr_snd;
+	char thr_fst_name[] = "thr-fst";
+	char thr_snd_name[] = "thr-snd";
+	thr_arg_t thr_fst_arg, thr_snd_arg;
+	int *value, i;
+
+	pr_err("ktsan: starting %s test, %s.\n", name,
+		has_race ? "race expected" : "no race expected");
+
+	/*
+	 * Run each test 16 times.
+	 * Due to racy race detection algorithm tsan can miss races sometimes,
+	 * so we require it to catch a race at least once in 16 runs.
+	 * For tests without races, it would not be out of place to ensure
+	 * that no runs result in false race reports.
+	 */
+	for (i = 0; i < 16; i++) {
+		char buffer[1024];
+
+		if (!on_stack)
+			value = kmalloc(1024, GFP_KERNEL);
+		else
+			value = (int *)&buffer[0];
+
+		BUG_ON(!value);
+		BUG_ON(!kt_shadow_get((uptr_t)value));
+
+		setup(value);
+
+		reinit_completion(&thr_fst_compl);
+		reinit_completion(&thr_snd_compl);
+
+		thr_fst_arg.value = value;
+		thr_fst_arg.func = first;
+		thr_fst_arg.completion = &thr_fst_compl;
+
+		thr_snd_arg.value = value;
+		thr_snd_arg.func = second;
+		thr_snd_arg.completion = &thr_snd_compl;
+
+		thr_fst = kthread_create(thr_func, &thr_fst_arg, thr_fst_name);
+		thr_snd = kthread_create(thr_func, &thr_snd_arg, thr_snd_name);
+
+		if (IS_ERR(thr_fst) || IS_ERR(thr_snd)) {
+			pr_err("ktsan: could not create kernel threads.\n");
+			return;
+		}
+
+		wake_up_process(thr_fst);
+		wake_up_process(thr_snd);
+
+		wait_for_completion(&thr_fst_compl);
+		wait_for_completion(&thr_snd_compl);
+
+		teardown(value);
+
+		if (!on_stack)
+			kfree(value);
+	}
+
+	pr_err("ktsan: end of test.\n");
+}
+
+static void kt_nop(void *arg) {}
+
+/* ktsan test: race in slab. */
+
+static void slab_race_read(void *arg)
+{
+	use(*((char *)arg));
+}
+
+static void slab_race_write(void *arg)
+{
+	*((int *)arg) = 1;
+}
+
+static void kt_test_slab_race(void)
+{
+	kt_test(kt_nop, kt_nop, slab_race_read, slab_race_write,
+		"slab-race", false, true);
+}
+
+/* ktsan test: race on global. */
+
+int global;
+
+static void global_race_write(void *arg)
+{
+	global = 1;
+}
+
+static void kt_test_global_race(void)
+{
+	kt_test(kt_nop, kt_nop, global_race_write, global_race_write,
+		"global-race", false, true);
+}
+
+/* ktsan test: race on stack. */
+
+static void stack_race_write(void *arg)
+{
+	*((int *)arg) = 1;
+}
+
+static void kt_test_stack_race(void)
+{
+	kt_test(kt_nop, kt_nop, stack_race_write, stack_race_write,
+		"stack-race", true, true);
+}
+
+/* ktsan test: racy-use-after-free */
+
+struct uaf_obj {
+	int data[32];
+};
+
+struct uaf_arg {
+	struct kmem_cache *cache;
+	struct uaf_obj *obj;
+};
+
+void kt_uaf_setup(void *p)
+{
+	struct uaf_arg *arg = (struct uaf_arg *)p;
+
+	arg->cache = kmem_cache_create("uaf_cache", sizeof(struct uaf_obj),
+					0, 0, NULL);
+	BUG_ON(!arg->cache);
+	arg->obj = kmem_cache_alloc(arg->cache, GFP_KERNEL);
+	BUG_ON(!arg->obj);
+}
+
+void kt_uaf_teardown(void *p)
+{
+	struct uaf_arg *arg = (struct uaf_arg *)p;
+
+	kmem_cache_destroy(arg->cache);
+}
+
+void kt_uaf_free(void *p)
+{
+	struct uaf_arg *arg = (struct uaf_arg *)p;
+
+	kmem_cache_free(arg->cache, arg->obj);
+}
+
+void kt_uaf_use(void *p)
+{
+	struct uaf_arg *arg = (struct uaf_arg *)p;
+
+	use(arg->obj->data[0]);
+}
+
+void kt_test_racy_use_after_free(void)
+{
+	kt_test(kt_uaf_setup, kt_uaf_teardown, kt_uaf_free, kt_uaf_use,
+		"racy-use-after-free", false, true);
+}
+
+/* ktsan test: SLAB_TYPESAFE_BY_RCU  */
+
+struct sdbr_obj {
+	int data[32];
+};
+
+struct sdbr_arg {
+	struct kmem_cache *cache;
+	struct sdbr_obj *obj;
+};
+
+static void sdbr_setup(void *p)
+{
+	struct sdbr_arg *arg = (struct sdbr_arg *)p;
+
+	arg->cache = kmem_cache_create("sdbr_cache", sizeof(struct sdbr_obj),
+					0, SLAB_TYPESAFE_BY_RCU, NULL);
+	BUG_ON(!arg->cache);
+	arg->obj = kmem_cache_alloc(arg->cache, GFP_KERNEL);
+	BUG_ON(!arg->obj);
+}
+
+static void sdbr_teardown(void *p)
+{
+	struct sdbr_arg *arg = (struct sdbr_arg *)p;
+
+	if (arg->obj)
+		kmem_cache_free(arg->cache, arg->obj);
+	kmem_cache_destroy(arg->cache);
+}
+
+static void sdbr_obj_free(void *p)
+{
+	struct sdbr_arg *arg = (struct sdbr_arg *)p;
+	struct sdbr_obj *obj;
+
+	obj = arg->obj;
+	WRITE_ONCE(arg->obj, NULL);
+	kmem_cache_free(arg->cache, obj);
+}
+
+static void sdbr_obj_use(void *p)
+{
+	struct sdbr_arg *arg = (struct sdbr_arg *)p;
+	struct sdbr_obj *obj;
+	int i;
+
+	rcu_read_lock();
+	obj = rcu_dereference(arg->obj);
+	if (obj)
+		for (i = 0; i < 100 * 1000; i++)
+			use(obj->data[i % 32]);
+	rcu_read_unlock();
+}
+
+static void sdbr_obj_realloc(void *p)
+{
+	struct sdbr_arg *arg = (struct sdbr_arg *)p;
+	struct sdbr_obj *obj;
+
+	obj = arg->obj;
+	kmem_cache_free(arg->cache, obj);
+
+	obj = kmem_cache_alloc(arg->cache, GFP_KERNEL);
+	BUG_ON(!obj);
+
+	rcu_assign_pointer(arg->obj, obj);
+}
+
+static void kt_test_slab_destroy_by_rcu(void)
+{
+	kt_test(sdbr_setup, sdbr_teardown, sdbr_obj_free, sdbr_obj_use,
+		"SLAB_TYPESAFE_BY_RCU-use-vs-free", false, false);
+	kt_test(sdbr_setup, sdbr_teardown, sdbr_obj_realloc, sdbr_obj_use,
+		"SLAB_TYPESAFE_BY_RCU-use-vs-realloc", false, false);
+}
+
+/* ktsan test: offset. */
+
+static void offset_first(void *arg)
+{
+	*((int *)arg) = 1;
+}
+
+static void offset_second(void *arg)
+{
+	*((int *)arg + 1) = 1;
+}
+
+static void kt_test_offset(void)
+{
+	kt_test(kt_nop, kt_nop, offset_first, offset_second,
+		"offset", false, false);
+}
+
+/* ktsan test: spinlock. */
+
+DEFINE_SPINLOCK(spinlock_sync);
+
+static void spinlock_first(void *arg)
+{
+	spin_lock(&spinlock_sync);
+	use(*((char *)arg));
+	spin_unlock(&spinlock_sync);
+}
+
+static void spinlock_second(void *arg)
+{
+	spin_lock(&spinlock_sync);
+	*((int *)arg) = 1;
+	spin_unlock(&spinlock_sync);
+}
+
+static void kt_test_spinlock(void)
+{
+	kt_test(kt_nop, kt_nop, spinlock_first, spinlock_second,
+		"spinlock", false, false);
+}
+
+/* ktsan test: atomic. */
+
+static void atomic_first(void *arg)
+{
+	use(atomic_read((atomic_t *)arg));
+}
+
+static void atomic_second(void *arg)
+{
+	atomic_set((atomic_t *)arg, 1);
+}
+
+static void atomic64_first(void *arg)
+{
+	use(atomic64_read((atomic64_t *)arg));
+}
+
+static void atomic64_second(void *arg)
+{
+	atomic64_set((atomic64_t *)arg, 1);
+}
+
+static void atomic_xvx_xchg(void *arg)
+{
+	xchg((int *)arg, 42);
+}
+
+static void atomic_xvx_xadd(void *arg)
+{
+	xadd((int *)arg, 42);
+}
+
+static void kt_test_atomic(void)
+{
+	kt_test(kt_nop, kt_nop, atomic_first, atomic_second,
+		"atomic", false, false);
+	kt_test(kt_nop, kt_nop, atomic64_first, atomic64_second,
+		"atomic64", false, false);
+	kt_test(kt_nop, kt_nop, atomic_xvx_xchg, atomic_xvx_xadd,
+		"xchg-vs-xadd", false, false);
+}
+
+/* ktsan test: mop vs atomic */
+
+static void mva_write(void *arg)
+{
+	*((int *)arg) = 1;
+}
+
+static void mva_read(void *arg)
+{
+	use(*((int *)arg));
+}
+
+static void mva_atomic_write(void *arg)
+{
+	atomic_set((atomic_t *)arg, 1);
+}
+
+static void mva_atomic_read(void *arg)
+{
+	use(atomic_read((atomic_t *)arg));
+}
+
+static void kt_test_mop_vs_atomic(void)
+{
+	kt_test(kt_nop, kt_nop, mva_write, mva_atomic_write,
+		"mop-vs-atomic-write-write", false, true);
+	kt_test(kt_nop, kt_nop, mva_write, mva_atomic_read,
+		"mop-vs-atomic-write-read", false, true);
+	kt_test(kt_nop, kt_nop, mva_read, mva_atomic_write,
+		"mop-vs-atomic-read-write", false, true);
+	kt_test(kt_nop, kt_nop, mva_read, mva_atomic_read,
+		"mop-vs-atomic-read-read", false, false);
+}
+
+/* ktsan test: use-after-acquire */
+
+static void uaa_setup(void *arg)
+{
+	int *value = (int *)arg;
+
+	*value = 0;
+}
+
+static void uaa_release(void *arg)
+{
+	int *value = (int *)arg;
+
+	smp_store_release(value, 1);
+}
+
+static void uaa_acquire(void *arg)
+{
+	int *value = (int *)arg;
+
+	while (smp_load_acquire(value) != 1);
+
+	*value = 2;
+}
+
+static void kt_test_use_after_acquire(void)
+{
+	kt_test(uaa_setup, kt_nop, uaa_release, uaa_acquire,
+		"use-after-acquire", false, false);
+}
+
+/* ktsan test: completion. */
+
+DECLARE_COMPLETION(completion_sync);
+
+static void compl_first(void *arg)
+{
+	wait_for_completion(&completion_sync);
+	use(*((char *)arg));
+}
+
+static void compl_second(void *arg)
+{
+	*((int *)arg) = 1;
+	complete(&completion_sync);
+}
+
+static void kt_test_completion(void)
+{
+	kt_test(kt_nop, kt_nop, compl_first, compl_second,
+		"completion", false, false);
+}
+
+/* ktsan test: mutex. */
+
+DEFINE_MUTEX(mutex_sync);
+
+static void mutex_first(void *arg)
+{
+	mutex_lock(&mutex_sync);
+	use(*((char *)arg));
+	mutex_unlock(&mutex_sync);
+}
+
+static void mutex_second(void *arg)
+{
+	mutex_lock(&mutex_sync);
+	*((int *)arg) = 1;
+	mutex_unlock(&mutex_sync);
+}
+
+static void kt_test_mutex(void)
+{
+	kt_test(kt_nop, kt_nop, mutex_first, mutex_second,
+		"mutex", false, false);
+}
+
+/* ktsan test: semaphore. */
+
+DEFINE_SEMAPHORE(sema_sync);
+
+static void sema_first(void *arg)
+{
+	down(&sema_sync);
+	*((int *)arg) = 2;
+	up(&sema_sync);
+}
+
+static void sema_second(void *arg)
+{
+	down(&sema_sync);
+	*((int *)arg) = 1;
+	up(&sema_sync);
+}
+
+static void kt_test_semaphore(void)
+{
+	kt_test(kt_nop, kt_nop, sema_first, sema_second,
+		"semaphore", false, false);
+}
+
+/* ktsan test: rwlock. */
+
+DEFINE_RWLOCK(rwlock_sync);
+
+static void rwlock_first(void *arg)
+{
+	write_lock(&rwlock_sync);
+	*((int *)arg) = 1;
+	write_unlock(&rwlock_sync);
+}
+
+static void rwlock_second(void *arg)
+{
+	write_lock(&rwlock_sync);
+	*((int *)arg) = 1;
+	write_unlock(&rwlock_sync);
+}
+
+static void kt_test_rwlock(void)
+{
+	kt_test(kt_nop, kt_nop, rwlock_first, rwlock_second,
+		"rwlock", false, false);
+}
+
+/* ktsan test: rwsem. */
+
+DECLARE_RWSEM(rwsem_sync);
+
+static void rwsem_write_write(void *arg)
+{
+	down_write(&rwsem_sync);
+	*((int *)arg) = 1;
+	up_write(&rwsem_sync);
+}
+
+static void rwsem_read_read(void *arg)
+{
+	down_read(&rwsem_sync);
+	*((int *)arg + 4) = *((int *)arg);
+	up_read(&rwsem_sync);
+}
+
+static void rwsem_read_write(void *arg)
+{
+	down_read(&rwsem_sync);
+	*((int *)arg) = 1;
+	up_read(&rwsem_sync);
+}
+
+static void kt_test_rwsem(void)
+{
+	kt_test(kt_nop, kt_nop, rwsem_write_write, rwsem_write_write,
+		"rwsem-write-write", false, false);
+	kt_test(kt_nop, kt_nop, rwsem_write_write, rwsem_read_read,
+		"rwsem-write-read", false, false);
+	kt_test(kt_nop, kt_nop, rwsem_write_write, rwsem_read_write,
+		"rwsem-write-write-bad", false, true);
+}
+
+/* ktsan test: percpu-rwsem. */
+
+struct percpu_rw_semaphore pcrws_sync;
+
+static void pcrws_main(void *arg)
+{
+	int rv = percpu_init_rwsem(&pcrws_sync);
+	BUG_ON(rv != 0);
+}
+
+static void pcrws_write_write(void *arg)
+{
+	percpu_down_write(&pcrws_sync);
+	*((int *)arg) = 1;
+	percpu_up_write(&pcrws_sync);
+}
+
+static void pcrws_read_read(void *arg)
+{
+	percpu_down_read(&pcrws_sync);
+	*((int *)arg + 4) = *((int *)arg);
+	percpu_up_read(&pcrws_sync);
+}
+
+static void pcrws_read_write(void *arg)
+{
+	percpu_down_read(&pcrws_sync);
+	*((int *)arg) = 1;
+	percpu_up_read(&pcrws_sync);
+}
+
+static void kt_test_percpu_rwsem(void)
+{
+	kt_test(pcrws_main, kt_nop, pcrws_write_write, pcrws_write_write,
+		"percpu-rwsem-write-write", false, false);
+	kt_test(pcrws_main, kt_nop, pcrws_write_write, pcrws_read_read,
+		"percpu-rwsem-write-read", false, false);
+	kt_test(pcrws_main, kt_nop, pcrws_write_write, pcrws_read_write,
+		"percpu-rwsem-write-write-bad", false, true);
+}
+
+/* ktsan test: thread create. */
+
+static void thr_crt_main(void *arg)
+{
+	*((int *)arg) = 1;
+}
+
+static void thr_crt_first(void *arg)
+{
+	*((int *)arg) = 1;
+}
+
+static void kt_test_thread_create(void)
+{
+	kt_test(thr_crt_main, kt_nop, thr_crt_first, kt_nop,
+		"thread creation", false, false);
+}
+
+/* ktsan tests: percpu. */
+
+DEFINE_PER_CPU(int, percpu_var);
+DEFINE_PER_CPU(int, percpu_array[128]);
+
+static void percpu_get_put(void *arg)
+{
+	get_cpu_var(percpu_var) = 0;
+	put_cpu_var(percpu_var);
+}
+
+static void percpu_irq(void *arg)
+{
+	unsigned long flags;
+
+	local_irq_save(flags);
+	*this_cpu_ptr(&percpu_var) = 0;
+	local_irq_restore(flags);
+}
+
+static void percpu_preempt_array(void *arg)
+{
+	int i;
+
+	preempt_disable();
+	for (i = 0; i < 128; i++)
+		*this_cpu_ptr(&percpu_array[i]) = i;
+	preempt_enable();
+}
+
+/* FIXME(xairy): this test doesn't produce report at all. */
+static void percpu_access_one(void *arg)
+{
+	preempt_disable();
+	per_cpu(percpu_var, 0) = 0;
+	preempt_enable();
+}
+
+/* FIXME(xairy): this test doesn't produce report sometimes. */
+static void percpu_race(void *arg)
+{
+	*((int *)arg) = 1;
+
+	preempt_disable();
+	*this_cpu_ptr(&percpu_var) = 0;
+	preempt_enable();
+}
+
+static void kt_test_percpu(void)
+{
+	kt_test(kt_nop, kt_nop, percpu_get_put, percpu_get_put,
+		"percpu preempt", false, false);
+	kt_test(kt_nop, kt_nop, percpu_irq, percpu_irq,
+		"percpu irq", false, false);
+	kt_test(kt_nop, kt_nop, percpu_preempt_array, percpu_preempt_array,
+		"percpu array", false, false);
+	kt_test(kt_nop, kt_nop, percpu_access_one, percpu_access_one,
+		"percpu access one", false, true);
+	kt_test(kt_nop, kt_nop, percpu_race, percpu_race,
+		"percpu race", false, true);
+}
+
+/* ktsan test: rcu */
+
+static void rcu_read_under_lock(void *arg)
+{
+	rcu_read_lock();
+	use(*((int *)arg));
+	rcu_read_unlock();
+}
+
+static void rcu_synchronize(void *arg)
+{
+	synchronize_rcu();
+	*((int *)arg) = 0;
+}
+
+static void rcu_write_under_lock(void *arg)
+{
+	rcu_read_lock();
+	*((int *)arg) = 0;
+	rcu_read_unlock();
+}
+
+static void rcu_init_ptr(void *arg)
+{
+	*((int *)arg + 4) = 1;
+	*(int **)arg = (int *)arg + 4;
+}
+
+static void rcu_assign_ptr(void *arg)
+{
+	*((int *)arg + 8) = 4242;
+	rcu_assign_pointer(*(int **)arg, (int *)arg + 8);
+}
+
+static void rcu_deref_ptr(void *arg)
+{
+	int *ptr = rcu_dereference(*(int **)arg);
+	*ptr = 42;
+}
+
+static void kt_test_rcu(void)
+{
+	kt_test(kt_nop, kt_nop, rcu_read_under_lock, rcu_synchronize,
+		"rcu-read-synchronize", false, false);
+
+	/* FIXME(xairy): this test doesn't produce report. */
+	kt_test(kt_nop, kt_nop, rcu_write_under_lock, rcu_write_under_lock,
+		"rcu-write-write", false, true);
+
+	/* FIXME(xairy): this test doesn't produce report. */
+	kt_test(kt_nop, kt_nop, rcu_read_under_lock, rcu_write_under_lock,
+		"rcu-read-write", false, true);
+
+	kt_test(kt_nop, kt_nop, rcu_read_under_lock, rcu_assign_ptr,
+		"rcu-read-assign", false, false);
+
+	kt_test(rcu_init_ptr, kt_nop, rcu_deref_ptr, rcu_assign_ptr,
+		"rcu-deref-assign", false, false);
+}
+
+/* ktsan test: seqlock */
+
+struct wait_on_bit_arg {
+	unsigned long bit;
+	unsigned long data;
+};
+
+static void wait_on_bit_main(void *p)
+{
+	struct wait_on_bit_arg *arg = p;
+
+	arg->bit = 1;
+}
+
+static void wait_on_bit_thr1(void *p)
+{
+	struct wait_on_bit_arg *arg = p;
+
+	arg->data = 1;
+	clear_bit_unlock(0, &arg->bit);
+	wake_up_bit(&arg->bit, 0);
+}
+
+static void wait_on_bit_thr2(void *p)
+{
+	struct wait_on_bit_arg *arg = p;
+
+	wait_on_bit(&arg->bit, 0, TASK_UNINTERRUPTIBLE);
+	if (arg->data != 1)
+		BUG_ON(1);
+}
+
+static void kt_test_wait_on_bit(void)
+{
+	kt_test(wait_on_bit_main, kt_nop, wait_on_bit_thr1, wait_on_bit_thr2,
+		"wait_on_bit", false, false);
+}
+
+struct seqcount_arg {
+	seqcount_t seq[3];
+	int data[6];
+};
+
+static void seq_main(void *p)
+{
+	struct seqcount_arg *arg = p;
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(arg->seq); i++)
+		seqcount_init(&arg->seq[i]);
+	for (i = 0; i < ARRAY_SIZE(arg->data); i++)
+		arg->data[i] = 0;
+}
+
+static void seq_write1(void *p)
+{
+	struct seqcount_arg *arg = p;
+	int i;
+
+	for (i = 0; i < 1000; i++) {
+		write_seqcount_begin(&arg->seq[0]);
+		arg->data[0]++;
+		arg->data[1]++;
+		arg->data[2]++;
+		arg->data[3]++;
+		write_seqcount_end(&arg->seq[0]);
+	}
+}
+
+static void seq_read1(void *p)
+{
+	struct seqcount_arg *arg = p;
+	unsigned seq;
+	int sum, i;
+
+	for (i = 0; i < 1000; i++) {
+		do {
+			sum = 0;
+			seq = __read_seqcount_begin(&arg->seq[0]);
+			rmb();
+			sum = arg->data[0] + arg->data[1] +
+				arg->data[2] + arg->data[3];
+			rmb();
+		} while (__read_seqcount_retry(&arg->seq[0], seq));
+		BUG_ON((sum % 4) != 0);
+	}
+}
+
+static void seq_read2(void *p)
+{
+	struct seqcount_arg *arg = p;
+	unsigned seq;
+	int sum, i;
+
+	for (i = 0; i < 1000; i++) {
+		do {
+			sum = 0;
+			seq = read_seqcount_begin(&arg->seq[0]);
+			sum = arg->data[0] + arg->data[1] +
+				arg->data[2] + arg->data[3];
+		} while (read_seqcount_retry(&arg->seq[0], seq));
+		BUG_ON((sum % 4) != 0);
+	}
+}
+
+static void seq_read3(void *p)
+{
+	struct seqcount_arg *arg = p;
+	unsigned seq;
+	int sum, i;
+
+	for (i = 0; i < 1000; i++) {
+		do {
+			sum = 0;
+			seq = raw_read_seqcount_latch(&arg->seq[0]);
+			sum = arg->data[0] + arg->data[1] +
+				arg->data[2] + arg->data[3];
+		} while (read_seqcount_retry(&arg->seq[0], seq));
+		/* don't BUG_ON, we use latch incorrectly */
+		use((sum % 4) != 0);
+	}
+}
+
+static void seq_write4(void *p)
+{
+	struct seqcount_arg *arg = p;
+	int i;
+
+	for (i = 0; i < 1000; i++) {
+		write_seqcount_begin(&arg->seq[0]);
+		arg->data[0]++;
+		arg->data[1]++;
+		write_seqcount_end(&arg->seq[0]);
+
+		write_seqcount_begin(&arg->seq[1]);
+		arg->data[2]++;
+		arg->data[3]++;
+		write_seqcount_end(&arg->seq[1]);
+
+		write_seqcount_begin(&arg->seq[2]);
+		arg->data[4]++;
+		arg->data[5]++;
+		write_seqcount_end(&arg->seq[2]);
+	}
+}
+
+static void seq_read4(void *p)
+{
+	struct seqcount_arg *arg = p;
+	unsigned seq[3];
+	int sum, i;
+
+	for (i = 0; i < 1000; i++) {
+		/*
+		 * A crazy mix of nested and overlapping read critical sections.
+		 * fs/namei.c:path_init actually does this.
+		 */
+		for (;;) {
+			seq[0] = read_seqcount_begin(&arg->seq[0]);
+			for (;;) {
+				sum = 0;
+				seq[1] = read_seqcount_begin(&arg->seq[1]);
+				sum = arg->data[2] + arg->data[3];
+				seq[2] = read_seqcount_begin(&arg->seq[2]);
+				if (read_seqcount_retry(&arg->seq[1], seq[1])) {
+					read_seqcount_cancel(&arg->seq[2]);
+					continue;
+				}
+				break;
+			}
+			sum += arg->data[4] + arg->data[5];
+			if (read_seqcount_retry(&arg->seq[2], seq[2])) {
+				read_seqcount_cancel(&arg->seq[0]);
+				continue;
+			}
+			sum += arg->data[0] + arg->data[1];
+			if (read_seqcount_retry(&arg->seq[0], seq[0]))
+				continue;
+			break;
+		}
+		use(sum);
+	}
+}
+
+static void seq_read_cancel(void *p)
+{
+	struct seqcount_arg *arg = p;
+	unsigned seq;
+	int sum, i;
+
+	for (i = 0; i < 1000; i++) {
+		do {
+			sum = 0;
+			seq = read_seqcount_begin(&arg->seq[0]);
+			sum = arg->data[0] + arg->data[1] +
+				arg->data[2] + arg->data[3];
+			if (sum != 0) {
+				read_seqcount_cancel(&arg->seq[0]);
+				break;
+			}
+		} while (read_seqcount_retry(&arg->seq[0], seq));
+		use(sum);
+	}
+}
+
+static void kt_test_seqcount(void)
+{
+	kt_test(seq_main, kt_nop, seq_write1, seq_read1,
+		"seqcount1", false, false);
+	kt_test(seq_main, kt_nop, seq_write1, seq_read2,
+		"seqcount2", false, false);
+	kt_test(seq_main, kt_nop, seq_write1, seq_read3,
+		"seqcount3", false, false);
+	kt_test(seq_main, kt_nop, seq_write4, seq_read4,
+		"seqcount4", false, false);
+	kt_test(seq_main, kt_nop, seq_write1, seq_read_cancel,
+		"seqcount_cancel", false, false);
+}
+
+/* ktsan test: parasitic synchronization by kmalloc */
+
+static void kt_malloc1(void *p)
+{
+	*(int *)p = 1;
+	kfree(kmalloc(1, GFP_KERNEL));
+}
+
+static void kt_malloc2(void *p)
+{
+	/* The intention is that this thread is scheduled after kt_malloc1
+	   on the same CPU. */
+	msleep_interruptible(100);
+	kfree(kmalloc(1, GFP_KERNEL));
+	*(int *)p = 1;
+}
+
+static void kt_test_malloc(void)
+{
+	/* Test that kmalloc does not introduce parasitic synchronization
+	   for threads running on the same CPU. Currently fails. */
+	kt_test(kt_nop, kt_nop, kt_malloc1, kt_malloc2,
+		"kmalloc", false, true);
+}
+
+/* ktsan test: bad-unlock: unlock mutex in another thread */
+
+int bu_locked;
+DEFINE_SPINLOCK(bu_spinlock);
+
+static void bu_setup(void *arg)
+{
+	bu_locked = 0;
+}
+
+static void bu_lock(void *arg)
+{
+	spin_lock(&bu_spinlock);
+
+	/* Fixup preempt counter. */
+	preempt_enable();
+
+	smp_store_release(&bu_locked, 1);
+}
+
+static void bu_unlock(void *arg)
+{
+	while (smp_load_acquire(&bu_locked) != 1);
+
+	/* Fixup preempt counter. */
+	preempt_disable();
+
+	spin_unlock(&bu_spinlock);
+}
+
+static void kt_test_bad_unlock(void)
+{
+	kt_test(bu_setup, kt_nop, bu_lock, bu_unlock,
+		"bad-unlock", false, true);
+}
+
+/* Instrumented tests. */
+
+void kt_tests_run_inst(void)
+{
+	pr_err("ktsan: running instrumented tests, T%d.\n", current->pid);
+	pr_err("\n");
+
+	kt_test_slab_race();
+	kt_test_global_race();
+	kt_test_stack_race();
+	pr_err("\n");
+	kt_test_racy_use_after_free();
+	kt_test_slab_destroy_by_rcu();
+	pr_err("\n");
+	kt_test_offset();
+	pr_err("\n");
+	kt_test_spinlock();
+	pr_err("\n");
+	kt_test_atomic();
+	pr_err("\n");
+	kt_test_mop_vs_atomic();
+	pr_err("\n");
+	kt_test_use_after_acquire();
+	pr_err("\n");
+	kt_test_completion();
+	pr_err("\n");
+	kt_test_mutex();
+	pr_err("\n");
+	kt_test_semaphore();
+	pr_err("\n");
+	kt_test_rwlock();
+	pr_err("\n");
+	kt_test_rwsem();
+	pr_err("\n");
+	kt_test_percpu_rwsem();
+	pr_err("\n");
+	kt_test_thread_create();
+	pr_err("\n");
+	kt_test_percpu();
+	pr_err("\n");
+	kt_test_rcu();
+	pr_err("\n");
+	kt_test_wait_on_bit();
+	pr_err("\n");
+	kt_test_seqcount();
+	pr_err("\n");
+	kt_test_malloc();
+	pr_err("\n");
+	kt_test_bad_unlock();
+	pr_err("\n");
+}
diff --git a/mm/ktsan/tests_noinst.c b/mm/ktsan/tests_noinst.c
new file mode 100644
index 000000000000..77ed10023523
--- /dev/null
+++ b/mm/ktsan/tests_noinst.c
@@ -0,0 +1,147 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+#include <linux/kernel.h>
+#include <linux/printk.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+
+/* Hash table test. */
+
+void kt_test_hash_table(void)
+{
+	kt_ctx_t *ctx;
+	kt_tab_test_t *obj, *obj1, *obj2, *obj3;
+	bool created;
+
+	pr_err("ktsan: starting hash table test.\n");
+
+	/* The test table is initialized in ktsan_init_early. */
+
+	ctx = &kt_ctx;
+
+	obj = kt_tab_access(&ctx->test_tab, 10, NULL, false);
+	BUG_ON(obj != NULL);
+
+	/* Creating. */
+
+	obj = kt_tab_access(&ctx->test_tab, 7, &created, false);
+	BUG_ON(obj == NULL);
+	BUG_ON(created != true);
+	BUG_ON(!kt_spin_is_locked(&obj->tab.lock));
+	kt_spin_unlock(&obj->tab.lock);
+
+	obj1 = kt_tab_access(&ctx->test_tab, 7, &created, false);
+	BUG_ON(obj1 != obj);
+	BUG_ON(created != false);
+	BUG_ON(!kt_spin_is_locked(&obj1->tab.lock));
+	kt_spin_unlock(&obj1->tab.lock);
+
+	obj2 = kt_tab_access(&ctx->test_tab, 7 + 13, &created, false);
+	BUG_ON(obj2 == NULL);
+	BUG_ON(obj2 == obj1);
+	BUG_ON(created != true);
+	BUG_ON(!kt_spin_is_locked(&obj2->tab.lock));
+	kt_spin_unlock(&obj2->tab.lock);
+
+	obj3 = kt_tab_access(&ctx->test_tab, 7 + 13, NULL, false);
+	BUG_ON(obj3 != obj2);
+	BUG_ON(!kt_spin_is_locked(&obj3->tab.lock));
+	kt_spin_unlock(&obj3->tab.lock);
+
+	obj3 = kt_tab_access(&ctx->test_tab, 3, &created, false);
+	BUG_ON(obj3 == NULL);
+	BUG_ON(obj3 == obj1 || obj3 == obj2);
+	BUG_ON(created != true);
+	BUG_ON(!kt_spin_is_locked(&obj3->tab.lock));
+	kt_spin_unlock(&obj3->tab.lock);
+
+	/* Accessing. */
+
+	obj = kt_tab_access(&ctx->test_tab, 7, NULL, false);
+	BUG_ON(obj == NULL);
+	BUG_ON(obj != obj1);
+	BUG_ON(!kt_spin_is_locked(&obj->tab.lock));
+	kt_spin_unlock(&obj->tab.lock);
+
+	obj = kt_tab_access(&ctx->test_tab, 7 + 13, &created, false);
+	BUG_ON(obj == NULL);
+	BUG_ON(obj != obj2);
+	BUG_ON(created != false);
+	BUG_ON(!kt_spin_is_locked(&obj->tab.lock));
+	kt_spin_unlock(&obj->tab.lock);
+
+	obj = kt_tab_access(&ctx->test_tab, 3, NULL, false);
+	BUG_ON(obj == NULL);
+	BUG_ON(obj != obj3);
+	BUG_ON(!kt_spin_is_locked(&obj->tab.lock));
+	kt_spin_unlock(&obj->tab.lock);
+
+	obj = kt_tab_access(&ctx->test_tab, 4, NULL, false);
+	BUG_ON(obj != NULL);
+
+	/* Destroying. */
+
+	obj = kt_tab_access(&ctx->test_tab, 3, NULL, true);
+	BUG_ON(obj == NULL);
+	BUG_ON(obj != obj3);
+	BUG_ON(!kt_spin_is_locked(&obj3->tab.lock));
+	kt_spin_unlock(&obj3->tab.lock);
+	kt_cache_free(&ctx->test_tab.obj_cache, obj3);
+
+	obj = kt_tab_access(&ctx->test_tab, 7 + 13, NULL, true);
+	BUG_ON(obj == NULL);
+	BUG_ON(obj != obj2);
+	BUG_ON(!kt_spin_is_locked(&obj2->tab.lock));
+	kt_spin_unlock(&obj2->tab.lock);
+	kt_cache_free(&ctx->test_tab.obj_cache, obj2);
+
+	obj = kt_tab_access(&ctx->test_tab, 7, NULL, true);
+	BUG_ON(obj == NULL);
+	BUG_ON(obj != obj1);
+	BUG_ON(!kt_spin_is_locked(&obj1->tab.lock));
+	kt_spin_unlock(&obj1->tab.lock);
+	kt_cache_free(&ctx->test_tab.obj_cache, obj1);
+
+	pr_err("ktsan: end of test.\n");
+}
+
+/* Trace test. */
+
+void kt_test_trace(void)
+{
+	kt_thr_t *thr;
+	kt_time_t clock;
+	kt_trace_state_t state;
+	int *fake;
+
+	pr_err("ktsan: starting trace test.\n");
+
+	thr = current->ktsan.task->thr;
+	clock = kt_clk_get(&thr->clk, thr->id);
+
+	/* Fake mop event. */
+	fake = kmalloc(sizeof(*fake), GFP_KERNEL);
+	kt_access(thr, (uptr_t)_RET_IP_, (uptr_t)fake, 1, false, false);
+	kfree(fake);
+
+	kt_trace_restore_state(thr, clock, &state);
+
+	pr_err("Restored stack trace:\n");
+	kt_stack_print(&state.stack, 0);
+
+	pr_err("ktsan: end of test.\n");
+}
+
+/* Not instrumented tests, should be called inside ENTER/LEAVE section. */
+
+void kt_tests_run_noinst(void)
+{
+	pr_err("ktsan: running not instrumented tests, T%d.\n", current->pid);
+	pr_err("\n");
+
+	kt_test_hash_table();
+	pr_err("\n");
+	kt_test_trace();
+	pr_err("\n");
+}
diff --git a/mm/ktsan/thr.c b/mm/ktsan/thr.c
new file mode 100644
index 000000000000..0a3b3638dbb8
--- /dev/null
+++ b/mm/ktsan/thr.c
@@ -0,0 +1,309 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+#include <linux/atomic.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/sched.h>
+#include <linux/spinlock.h>
+
+__init void kt_thr_pool_init(void)
+{
+	kt_thr_pool_t *pool = &kt_ctx.thr_pool;
+
+	kt_cache_init(&pool->cache, sizeof(kt_thr_t), KT_MAX_THREAD_COUNT);
+	memset(pool->thrs, 0, sizeof(pool->thrs));
+	pool->new_id = 0;
+	pool->new_pid = 0;
+	INIT_LIST_HEAD(&pool->quarantine);
+	pool->quarantine_size = 0;
+	kt_spin_init(&pool->lock);
+}
+
+kt_thr_t *kt_thr_create(kt_thr_t *thr, int pid)
+{
+	kt_thr_pool_t *pool = &kt_ctx.thr_pool;
+	kt_thr_t *new;
+	int i;
+
+	kt_spin_lock(&pool->lock);
+	if ((new = kt_cache_alloc(&pool->cache)) != NULL) {
+		new->id = pool->new_id;
+		pool->new_id++;
+		pool->thrs[new->id] = new;
+	} else if (pool->quarantine_size) {
+		new = list_first_entry(&pool->quarantine,
+				kt_thr_t, quarantine_list);
+		list_del(&new->quarantine_list);
+		pool->quarantine_size--;
+	} else {
+		pr_err("KTSAN: maximum number of threads is reached\n");
+		BUG();
+	}
+	/* Kernel does not assign PIDs to some threads, give them fake
+	 * negative PIDs so that they are distinguishable in reports. */
+	if (pid == 0)
+		pid = --pool->new_pid;
+	kt_spin_unlock(&pool->lock);
+
+	new->pid = pid;
+	new->inside = 0;
+	new->cpu = NULL;
+	kt_clk_init(&new->clk);
+	kt_clk_init(&new->acquire_clk);
+	new->acquire_active = 0;
+	kt_clk_init(&new->release_clk);
+	new->release_active = 0;
+	kt_stack_init(&new->stack);
+	kt_trace_init(&new->trace);
+	new->read_disable_depth = 0;
+	new->event_disable_depth = 0;
+	new->report_disable_depth = 0;
+	new->preempt_disable_depth = 0;
+	new->irqs_disabled = false;
+	INIT_LIST_HEAD(&new->quarantine_list);
+	INIT_LIST_HEAD(&new->percpu_list);
+	for (i = 0; i < ARRAY_SIZE(new->seqcount); i++) {
+		new->seqcount[i] = 0;
+		new->seqcount_pc[i] = 0;
+	}
+	new->seqcount_ignore = 0;
+	new->interrupt_depth = 0;
+
+	kt_stat_inc(kt_stat_thread_create);
+	kt_stat_inc(kt_stat_threads);
+
+	/* thr == NULL when thread #0 is being initialized. */
+	if (thr == NULL)
+		return new;
+
+	kt_clk_acquire(&new->clk, &thr->clk);
+
+	return new;
+}
+
+void kt_thr_destroy(kt_thr_t *thr, kt_thr_t *old)
+{
+	kt_thr_pool_t *pool = &kt_ctx.thr_pool;
+	int i;
+
+	BUG_ON(old->event_disable_depth != 0);
+	for (i = 0; i < ARRAY_SIZE(old->seqcount); i++) {
+		if (old->seqcount[i] != 0)
+			kt_seqcount_bug(old, 0, "acquired seqlock on thr end");
+	}
+	if (old->read_disable_depth != 0)
+		kt_seqcount_bug(old, 0, "read_disable_depth on thr end");
+	BUG_ON(old->seqcount_ignore != 0);
+	BUG_ON(old->interrupt_depth != 0);
+
+	kt_spin_lock(&pool->lock);
+	list_add_tail(&old->quarantine_list, &pool->quarantine);
+	pool->quarantine_size++;
+	kt_spin_unlock(&pool->lock);
+
+	kt_stat_inc(kt_stat_thread_destroy);
+	kt_stat_dec(kt_stat_threads);
+}
+
+kt_thr_t *kt_thr_get(int id)
+{
+	kt_thr_pool_t *pool = &kt_ctx.thr_pool;
+	void *thr;
+
+	BUG_ON(id < 0);
+	BUG_ON(id >= KT_MAX_THREAD_COUNT);
+	kt_spin_lock(&pool->lock);
+	thr = pool->thrs[id];
+	kt_spin_unlock(&pool->lock);
+
+	return thr;
+}
+
+void kt_thr_start(kt_thr_t *thr, uptr_t pc)
+{
+	kt_trace_add_event(thr, kt_event_thr_start,
+		smp_processor_id() | ((u32)thr->pid << 16));
+
+	thr->cpu = this_cpu_ptr(kt_ctx.cpus);
+	BUG_ON(thr->cpu->thr != NULL);
+	thr->cpu->thr = thr;
+}
+
+void kt_thr_stop(kt_thr_t *thr, uptr_t pc)
+{
+	BUG_ON(thr->event_disable_depth != 0);
+
+	/* Current thread might be rescheduled even if preemption is disabled
+	   (for example using might_sleep()). Therefore, percpu syncs won't
+	   be released before thread switching. Release them here. */
+	kt_percpu_release(thr, pc);
+
+	kt_trace_add_event(thr, kt_event_thr_stop, smp_processor_id());
+
+	BUG_ON(thr->cpu == NULL);
+	BUG_ON(thr->cpu->thr != thr);
+	thr->cpu->thr = NULL;
+	thr->cpu = NULL;
+}
+
+void kt_thr_wakeup(kt_thr_t *thr, kt_thr_t *other)
+{
+	kt_clk_acquire(&other->clk, &thr->clk);
+}
+
+/* Returns true if events were enabled before the call. */
+bool kt_thr_event_disable(kt_thr_t *thr, uptr_t pc, unsigned long *flags)
+{
+#if KT_DEBUG
+	kt_trace_add_event(thr, kt_event_event_disable, kt_compress(pc));
+	if (thr->event_disable_depth == 0)
+		thr->last_event_disable_time = kt_clk_get(&thr->clk, thr->id);
+#endif /* KT_DEBUG */
+
+	thr->event_disable_depth++;
+	BUG_ON(thr->event_disable_depth >= 3);
+
+	if (thr->event_disable_depth - 1 == 0) {
+		/* Disable interrupts as well. Otherwise all events
+		   that happen in interrupts will be ignored. */
+		thr->irq_flags_before_disable = *flags;
+		/* Set all disabled in *flags. */
+		*flags = arch_local_irq_save();
+	}
+
+	return (thr->event_disable_depth - 1 == 0);
+}
+
+/* Returns true if events became enabled after the call. */
+bool kt_thr_event_enable(kt_thr_t *thr, uptr_t pc, unsigned long *flags)
+{
+#if KT_DEBUG
+	kt_trace_add_event(thr, kt_event_event_enable, kt_compress(pc));
+	if (thr->event_disable_depth - 1 == 0)
+		thr->last_event_enable_time = kt_clk_get(&thr->clk, thr->id);
+	/* We are about to crash, let's print some debug data */
+	if (thr->event_disable_depth == 0)
+		ktsan_print_diagnostics();
+#endif /* KT_DEBUG */
+
+	thr->event_disable_depth--;
+	BUG_ON(thr->event_disable_depth < 0);
+
+	if (thr->event_disable_depth == 0) {
+		BUG_ON(!arch_irqs_disabled());
+		*flags = thr->irq_flags_before_disable;
+	}
+
+	return (thr->event_disable_depth == 0);
+}
+
+void kt_thr_report_disable(kt_thr_t *thr)
+{
+	thr->report_disable_depth++;
+}
+
+void kt_thr_report_enable(kt_thr_t *thr)
+{
+	thr->report_disable_depth--;
+	BUG_ON(thr->report_disable_depth < 0);
+}
+
+void kt_thr_interrupt(kt_thr_t *thr, uptr_t pc, kt_interrupted_t *state)
+{
+	BUG_ON(state->thr != NULL);
+	state->thr = thr;
+
+	BUG_ON(thr->event_disable_depth);
+	/* FIXME: fails during boot.
+	 * How can we receive an interrupt when interrupts are disabled?
+	 * We probably miss some enable of interrupt.
+	 * BUG_ON(thr->irqs_disabled);
+	 */
+
+	kt_stack_copy(&state->stack, &thr->stack);
+	kt_stack_init(&thr->stack);
+
+	state->mutexset = thr->mutexset;
+	kt_mutexset_init(&thr->mutexset);
+
+	state->acquire_active = thr->acquire_active;
+	if (thr->acquire_active) {
+		thr->acquire_active = 0;
+		state->acquire_clk = thr->acquire_clk;
+	}
+	state->release_active = thr->release_active;
+	if (thr->release_active) {
+		thr->release_active = 0;
+		state->release_clk = thr->release_clk;
+	}
+
+	state->read_disable_depth = thr->read_disable_depth;
+	thr->read_disable_depth = 0;
+	state->report_disable_depth = thr->report_disable_depth;
+	thr->report_disable_depth = 0;
+	state->preempt_disable_depth = thr->preempt_disable_depth;
+	thr->preempt_disable_depth = 0;
+
+	list_replace_init(&thr->percpu_list, &state->percpu_list);
+
+	memcpy(&state->seqcount, &thr->seqcount, sizeof(state->seqcount));
+	memcpy(&state->seqcount_pc, &thr->seqcount_pc,
+		sizeof(state->seqcount_pc));
+	memset(&thr->seqcount, 0, sizeof(thr->seqcount));
+	memset(&thr->seqcount_pc, 0, sizeof(thr->seqcount_pc));
+	state->seqcount_ignore = thr->seqcount_ignore;
+	thr->seqcount_ignore = 0;
+
+	/* This resets stack and mutexset during trace replay. */
+	kt_trace_add_event(thr, kt_event_interrupt, 0);
+}
+
+void kt_thr_resume(kt_thr_t *thr, uptr_t pc, kt_interrupted_t *state)
+{
+	int i;
+
+	BUG_ON(state->thr != thr);
+	state->thr = NULL;
+
+	BUG_ON(thr->mutexset.size);
+	BUG_ON(thr->event_disable_depth);
+	BUG_ON(thr->read_disable_depth);
+	BUG_ON(thr->report_disable_depth);
+	BUG_ON(thr->preempt_disable_depth);
+	BUG_ON(thr->seqcount[0]);
+	BUG_ON(thr->seqcount_ignore);
+
+	/* This resets stack and mutexset during trace replay. */
+	kt_trace_add_event(thr, kt_event_interrupt, 0);
+	kt_stack_init(&thr->stack);
+	for (i = 0; i < state->stack.size; i++)
+		kt_func_entry(thr, kt_decompress(state->stack.pc[i]));
+
+	thr->mutexset = state->mutexset;
+	for (i = 0; i < thr->mutexset.size; i++) {
+		kt_locked_mutex_t *mtx = &thr->mutexset.mtx[i];
+
+		kt_trace_add_event2(thr, mtx->write ? kt_event_lock :
+			kt_event_rlock, mtx->uid, mtx->stack);
+	}
+
+	thr->acquire_active = state->acquire_active;
+	if (thr->acquire_active)
+		thr->acquire_clk = state->acquire_clk;
+	thr->release_active = state->release_active;
+	if (thr->release_active)
+		thr->release_clk = state->release_clk;
+
+	thr->read_disable_depth = state->read_disable_depth;
+	thr->report_disable_depth = state->report_disable_depth;
+	thr->preempt_disable_depth = state->preempt_disable_depth;
+
+	kt_percpu_release(thr, pc);
+	list_replace_init(&state->percpu_list, &thr->percpu_list);
+
+	memcpy(&thr->seqcount, &state->seqcount, sizeof(thr->seqcount));
+	memcpy(&thr->seqcount_pc, &state->seqcount_pc,
+		sizeof(thr->seqcount_pc));
+	thr->seqcount_ignore = state->seqcount_ignore;
+}
diff --git a/mm/ktsan/trace.c b/mm/ktsan/trace.c
new file mode 100644
index 000000000000..dd5d56e2b454
--- /dev/null
+++ b/mm/ktsan/trace.c
@@ -0,0 +1,259 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "ktsan.h"
+
+#include <linux/kernel.h>
+#include <linux/spinlock.h>
+
+static inline void kt_trace_follow(kt_trace_t *trace, unsigned long beg,
+				   unsigned long end, kt_trace_state_t *state)
+{
+	unsigned long i;
+	kt_stack_handle_t stk;
+	kt_event_t event;
+
+	for (i = beg; i <= end; i++) {
+		event = trace->events[i];
+		if (event.type == kt_event_func_enter) {
+			kt_stack_push(&state->stack, event.data);
+		} else if (event.type == kt_event_func_exit) {
+			kt_stack_pop(&state->stack);
+		} else if (event.type == kt_event_thr_start) {
+			int cpu = event.data & 0xffff;
+			int pid = (s32)(u32)(event.data >> 16);
+			BUG_ON(state->cpu_id != -1);
+			state->cpu_id = cpu;
+			state->pid = pid;
+		} else if (event.type == kt_event_thr_stop) {
+			BUG_ON(state->cpu_id != event.data);
+			state->cpu_id = -1;
+		} else if (event.type == kt_event_lock ||
+				event.type == kt_event_rlock) {
+			stk = *(u64 *)&trace->events[++i];
+			kt_mutexset_lock(&state->mutexset, event.data, stk,
+				event.type == kt_event_lock);
+		} else if (event.type == kt_event_unlock) {
+			kt_mutexset_unlock(&state->mutexset, event.data, true);
+		} else if (event.type == kt_event_runlock) {
+			kt_mutexset_unlock(&state->mutexset, event.data, false);
+		} else if (event.type == kt_event_downgrade) {
+			kt_mutexset_downgrade(&state->mutexset, event.data);
+		} else if (event.type == kt_event_interrupt) {
+			kt_stack_init(&state->stack);
+			kt_mutexset_init(&state->mutexset);
+		}
+	}
+}
+
+void kt_trace_switch(kt_thr_t *thr)
+{
+	kt_trace_t *trace;
+	kt_time_t clock;
+	unsigned part;
+	kt_trace_part_header_t *header;
+
+	trace = &thr->trace;
+	clock = kt_clk_get(&thr->clk, thr->id);
+	kt_spin_lock(&trace->lock);
+	part = (clock % KT_TRACE_SIZE) / KT_TRACE_PART_SIZE;
+	header = &trace->headers[part];
+	header->state.stack = thr->stack;
+	header->state.mutexset = thr->mutexset;
+	header->state.pid = thr->pid;
+	/* -1 for case we are called from kt_thr_start. */
+	header->state.cpu_id = thr->cpu ? smp_processor_id() : -1;
+	header->clock = clock;
+	kt_spin_unlock(&trace->lock);
+}
+
+void kt_trace_add_event2(kt_thr_t *thr, kt_event_type_t type, u64 data,
+	u64 data2)
+{
+	kt_time_t clock;
+	unsigned pos;
+
+	clock = kt_clk_get(&thr->clk, thr->id);
+	if (((clock + 2) % KT_TRACE_PART_SIZE) == 0) {
+		/* The trace would switch between the two data items.
+		 * Push a fake event to precent it. */
+		kt_trace_add_event(thr, kt_event_nop, 0);
+	}
+	kt_trace_add_event(thr, type, data);
+
+	clock = kt_clk_get(&thr->clk, thr->id);
+	pos = clock % KT_TRACE_SIZE + 1;
+	BUG_ON((pos % KT_TRACE_PART_SIZE) == 0);
+	BUG_ON(sizeof(kt_event_t) != sizeof(data2));
+	*(u64 *)&thr->trace.events[pos] = data2;
+	kt_clk_tick(&thr->clk, thr->id);
+}
+
+void kt_trace_init(kt_trace_t *trace)
+{
+	int i;
+
+	memset(trace, 0, sizeof(*trace));
+	for (i = 0; i < KT_TRACE_PARTS; i++)
+		trace->headers[i].state.cpu_id = -1;
+	kt_spin_init(&trace->lock);
+}
+
+u64 kt_trace_last_data(kt_thr_t *thr)
+{
+	kt_time_t clock;
+	kt_event_t event;
+
+	clock = kt_clk_get(&thr->clk, thr->id);
+	event = thr->trace.events[clock % KT_TRACE_SIZE];
+	BUG_ON(event.type != kt_event_mop);
+	return event.data;
+}
+
+void kt_trace_restore_state(kt_thr_t *thr, kt_time_t clock,
+				kt_trace_state_t *state)
+{
+	kt_trace_t *trace;
+	unsigned part;
+	kt_trace_part_header_t *header;
+	unsigned long beg, end;
+	kt_event_t event;
+
+	trace = &thr->trace;
+	part = (clock % KT_TRACE_SIZE) / KT_TRACE_PART_SIZE;
+	header = &trace->headers[part];
+
+	kt_spin_lock(&trace->lock);
+
+	if (header->clock > clock) {
+		kt_spin_unlock(&trace->lock);
+		memset(state, 0, sizeof(*state));
+		state->cpu_id = -1;
+		return;
+	}
+
+	memcpy(state, &header->state, sizeof(*state));
+	end = clock % KT_TRACE_SIZE;
+	beg = round_down(end, KT_TRACE_PART_SIZE);
+	kt_trace_follow(trace, beg, end, state);
+
+	event = trace->events[end];
+	if (event.type != kt_event_nop
+	    && event.type != kt_event_func_enter
+	    && event.type != kt_event_func_exit
+	    && event.type != kt_event_thr_stop
+	    && event.type != kt_event_thr_start
+	    && event.type != kt_event_lock
+	    && event.type != kt_event_unlock
+	    && event.type != kt_event_rlock
+	    && event.type != kt_event_runlock
+	    && event.type != kt_event_downgrade
+	) {
+		BUG_ON(state->stack.size + 1 == KT_MAX_STACK_FRAMES);
+		state->stack.pc[state->stack.size] = event.data;
+		state->stack.size++;
+	}
+
+	kt_spin_unlock(&trace->lock);
+}
+
+void kt_trace_dump(kt_trace_t *trace, uptr_t beg, uptr_t end)
+{
+	unsigned long i;
+	kt_event_t event;
+	uptr_t pc;
+
+	for (i = beg; i <= end; i++) {
+		event = trace->events[i % KT_TRACE_SIZE];
+		if (event.type == kt_event_mop) {
+			pc = kt_decompress(event.data);
+			pr_err(" i: %lu, access , pc: [<%p>] %pS\n",
+				i, (void *)pc, (void *)pc);
+		} else if (event.type == kt_event_func_enter) {
+			pc = kt_decompress(event.data);
+			pr_err(" i: %lu, enter  , pc: [<%p>] %pS\n",
+				i, (void *)pc, (void *)pc);
+		} else if (event.type == kt_event_func_exit) {
+			pc = kt_decompress(event.data);
+			pr_err(" i: %lu, exit   , pc: [<%p>] %pS\n",
+				i, (void *)pc, (void *)pc);
+		} else if (event.type == kt_event_thr_stop) {
+			int cpu = event.data & 0xffff;
+			int pid = (s32)(u32)(event.data >> 16);
+			pr_err(" i: %lu, stop   , cpu: %d, thread: %d\n",
+				i, cpu, pid);
+		} else if (event.type == kt_event_thr_start) {
+			int cpu = event.data & 0xffff;
+			int pid = (s32)(u32)(event.data >> 16);
+			pr_err(" i: %lu, start  , cpu: %d, thread: %d\n",
+				i, cpu, pid);
+		} else if (event.type == kt_event_lock) {
+			pr_err(" i: %lu, lock   , mutex: %llu\n",
+				i, (u64)event.data);
+			i++; /* consume stack id */
+		} else if (event.type == kt_event_unlock) {
+			pr_err(" i: %lu, unlock , mutex: %llu\n",
+				i, (u64)event.data);
+		} else if (event.type == kt_event_rlock) {
+			pr_err(" i: %lu, rlock  , mutex: %llu\n",
+				i, (u64)event.data);
+			i++; /* consume stack id */
+		} else if (event.type == kt_event_runlock) {
+			pr_err(" i: %lu, runlock, mutex: %llu\n",
+				i, (u64)event.data);
+		} else if (event.type == kt_event_downgrade) {
+			pr_err(" i: %lu, downgrade , mutex: %llu\n",
+				i, (u64)event.data);
+		} else if (event.type == kt_event_interrupt) {
+			pr_err(" i: %lu, interrupt\n", i);
+#if KT_DEBUG
+		} else if (event.type == kt_event_preempt_enable) {
+			pc = kt_decompress(event.data);
+			pr_err(" i: %lu, prm on , pc: [<%p>] %pS\n",
+				i, (void *)pc, (void *)pc);
+		} else if (event.type == kt_event_preempt_disable) {
+			pc = kt_decompress(event.data);
+			pr_err(" i: %lu, prm off, pc: [<%p>] %pS\n",
+				i, (void *)pc, (void *)pc);
+		} else if (event.type == kt_event_irq_enable) {
+			pc = kt_decompress(event.data);
+			pr_err(" i: %lu, irq on , pc: [<%p>] %pS\n",
+				i, (void *)pc, (void *)pc);
+		} else if (event.type == kt_event_irq_disable) {
+			pc = kt_decompress(event.data);
+			pr_err(" i: %lu, irq off, pc: [<%p>] %pS\n",
+				i, (void *)pc, (void *)pc);
+		} else if (event.type == kt_event_acquire) {
+			pc = kt_decompress(event.data);
+			pr_err(" i: %lu, acquire, pc: [<%p>] %pS\n",
+				i, (void *)pc, (void *)pc);
+		} else if (event.type == kt_event_release) {
+			pc = kt_decompress(event.data);
+			pr_err(" i: %lu, release, pc: [<%p>] %pS\n",
+				i, (void *)pc, (void *)pc);
+		} else if (event.type == kt_event_nonmat_acquire) {
+			pc = kt_decompress(event.data);
+			pr_err(" i: %lu, nm acq , pc: [<%p>] %pS\n",
+				i, (void *)pc, (void *)pc);
+		} else if (event.type == kt_event_nonmat_release) {
+			pc = kt_decompress(event.data);
+			pr_err(" i: %lu, nm rel , pc: [<%p>] %pS\n",
+				i, (void *)pc, (void *)pc);
+		} else if (event.type == kt_event_membar_acquire) {
+			pc = kt_decompress(event.data);
+			pr_err(" i: %lu, mb acq , pc: [<%p>] %pS\n",
+				i, (void *)pc, (void *)pc);
+		} else if (event.type == kt_event_membar_release) {
+			pc = kt_decompress(event.data);
+			pr_err(" i: %lu, mb rel , pc: [<%p>] %pS\n",
+				i, (void *)pc, (void *)pc);
+		} else if (event.type == kt_event_event_enable) {
+			pc = kt_decompress(event.data);
+			pr_err(" i: %lu, evt on , pc: [<%p>] %pS\n",
+				i, (void *)pc, (void *)pc);
+		} else if (event.type == kt_event_event_disable) {
+			pc = kt_decompress(event.data);
+			pr_err(" i: %lu, evt off, pc: [<%p>] %pS\n",
+				i, (void *)pc, (void *)pc);
+#endif /* KT_DEBUG */
+		}
+	}
+}
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 8e3bc949ebcc..527997dd754f 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -69,6 +69,7 @@
 #include <linux/lockdep.h>
 #include <linux/nmi.h>
 #include <linux/psi.h>
+#include <linux/ktsan.h>
 
 #include <asm/sections.h>
 #include <asm/tlbflush.h>
@@ -1146,6 +1147,7 @@ static __always_inline bool free_pages_prepare(struct page *page,
 		kernel_map_pages(page, 1 << order, 0);
 
 	kasan_free_nondeferred_pages(page, order);
+	ktsan_free_page(page, order);
 
 	return true;
 }
@@ -3054,6 +3056,8 @@ void split_page(struct page *page, unsigned int order)
 	VM_BUG_ON_PAGE(PageCompound(page), page);
 	VM_BUG_ON_PAGE(!page_count(page), page);
 
+	ktsan_split_page(page, order);
+
 	for (i = 1; i < (1 << order); i++)
 		set_page_refcounted(page + i);
 	split_page_owner(page, order);
@@ -4570,6 +4574,8 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,
 	warn_alloc(gfp_mask, ac->nodemask,
 			"page allocation failure: order:%u", order);
 got_pg:
+	ktsan_alloc_page(page, order, gfp_mask, -1);
+
 	return page;
 }
 
@@ -4676,6 +4682,8 @@ __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
 		ac.nodemask = nodemask;
 
 	page = __alloc_pages_slowpath(alloc_mask, order, &ac);
+	if (page)
+		ktsan_alloc_page(page, order, gfp_mask, -1);
 
 out:
 	if (memcg_kmem_enabled() && (gfp_mask & __GFP_ACCOUNT) && page &&
diff --git a/mm/slab.c b/mm/slab.c
index f7117ad9b3a3..3e1d704f7854 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -117,6 +117,7 @@
 #include	<linux/memory.h>
 #include	<linux/prefetch.h>
 #include	<linux/sched/task_stack.h>
+#include	<linux/ktsan.h>
 
 #include	<net/sock.h>
 
@@ -1392,6 +1393,8 @@ static struct page *kmem_getpages(struct kmem_cache *cachep, gfp_t flags,
 	if (sk_memalloc_socks() && page_is_pfmemalloc(page))
 		SetPageSlabPfmemalloc(page);
 
+	ktsan_alloc_page(page, cachep->gfporder, cachep->flags, nodeid);
+
 	return page;
 }
 
@@ -1403,6 +1406,8 @@ static void kmem_freepages(struct kmem_cache *cachep, struct page *page)
 	int order = cachep->gfporder;
 	unsigned long nr_freed = (1 << order);
 
+	ktsan_free_page(page, cachep->gfporder);
+
 	if (cachep->flags & SLAB_RECLAIM_ACCOUNT)
 		mod_lruvec_page_state(page, NR_SLAB_RECLAIMABLE, -nr_freed);
 	else
@@ -3168,6 +3173,8 @@ static void *fallback_alloc(struct kmem_cache *cache, gfp_t flags)
 
 	if (unlikely(!obj && read_mems_allowed_retry(cpuset_mems_cookie)))
 		goto retry_cpuset;
+	if (obj)
+		read_mems_allowed_cancel();
 	return obj;
 }
 
@@ -3263,6 +3270,7 @@ slab_alloc_node(struct kmem_cache *cachep, gfp_t flags, int nodeid,
 	local_irq_restore(save_flags);
 	ptr = cache_alloc_debugcheck_after(cachep, flags, ptr, caller);
 
+	ktsan_slab_alloc(ptr, cachep->object_size, cachep->flags);
 	if (unlikely(flags & __GFP_ZERO) && ptr)
 		memset(ptr, 0, cachep->object_size);
 
@@ -3320,6 +3328,7 @@ slab_alloc(struct kmem_cache *cachep, gfp_t flags, unsigned long caller)
 	objp = cache_alloc_debugcheck_after(cachep, flags, objp, caller);
 	prefetchw(objp);
 
+	ktsan_slab_alloc(objp, cachep->object_size, cachep->flags);
 	if (unlikely(flags & __GFP_ZERO) && objp)
 		memset(objp, 0, cachep->object_size);
 
@@ -3444,6 +3453,7 @@ void ___cache_free(struct kmem_cache *cachep, void *objp,
 	kmemleak_free_recursive(objp, cachep->flags);
 	objp = cache_free_debugcheck(cachep, objp, caller);
 
+	ktsan_slab_free(objp, cachep->object_size, cachep->flags);
 	/*
 	 * Skip calling cache_free_alien() when the platform is not numa.
 	 * This will avoid cache misses that happen while accessing slabp (which
diff --git a/mm/slub.c b/mm/slub.c
index cd04dbd2b5d0..91c5177969f7 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -1938,6 +1938,7 @@ static void *get_any_partial(struct kmem_cache *s, gfp_t flags,
 					 * between allocation and the cpuset
 					 * update
 					 */
+					read_mems_allowed_cancel();
 					return object;
 				}
 			}
diff --git a/mm/vmalloc.c b/mm/vmalloc.c
index 0f76cca32a1c..7a289d415ca9 100644
--- a/mm/vmalloc.c
+++ b/mm/vmalloc.c
@@ -34,6 +34,7 @@
 #include <linux/llist.h>
 #include <linux/bitops.h>
 #include <linux/rbtree_augmented.h>
+#include <linux/ktsan.h>
 
 #include <linux/uaccess.h>
 #include <asm/tlbflush.h>
diff --git a/net/core/dev.c b/net/core/dev.c
index d6edd218babd..ebb93fc527d4 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -914,6 +914,7 @@ int netdev_get_name(struct net *net, char *name, int ifindex)
 	dev = dev_get_by_index_rcu(net, ifindex);
 	if (!dev) {
 		rcu_read_unlock();
+		read_seqcount_cancel(&devnet_rename_seq);
 		return -ENODEV;
 	}
 
diff --git a/net/netfilter/x_tables.c b/net/netfilter/x_tables.c
index ce70c2576bb2..cb43e2386b65 100644
--- a/net/netfilter/x_tables.c
+++ b/net/netfilter/x_tables.c
@@ -1398,13 +1398,13 @@ xt_replace_table(struct xt_table *table,
 	/* ... so wait for even xt_recseq on all cpus */
 	for_each_possible_cpu(cpu) {
 		seqcount_t *s = &per_cpu(xt_recseq, cpu);
-		u32 seq = raw_read_seqcount(s);
+		u32 seq = raw_read_seqcount_nocritical(s);
 
 		if (seq & 1) {
 			do {
 				cond_resched();
 				cpu_relax();
-			} while (seq == raw_read_seqcount(s));
+			} while (seq == raw_read_seqcount_nocritical(s));
 		}
 	}
 
diff --git a/net/tls/tls_device.c b/net/tls/tls_device.c
index 1f9cf57d9754..d869b31c02e2 100644
--- a/net/tls/tls_device.c
+++ b/net/tls/tls_device.c
@@ -846,7 +846,7 @@ int tls_set_device_offload(struct sock *sk, struct tls_context *ctx)
 	 * will return true and the context might be accessed
 	 * by the netdev's xmit function.
 	 */
-	smp_store_release(&sk->sk_validate_xmit_skb, tls_validate_xmit_skb);
+	smp_store_release(&sk->sk_validate_xmit_skb, (uintptr_t)tls_validate_xmit_skb);
 	dev_put(netdev);
 	up_read(&device_offload_lock);
 	goto out;
diff --git a/scripts/Makefile.lib b/scripts/Makefile.lib
index f1f38c8cdc74..41182c8fbd7f 100644
--- a/scripts/Makefile.lib
+++ b/scripts/Makefile.lib
@@ -137,6 +137,16 @@ _c_flags += $(if $(patsubst n%,, \
 	$(CFLAGS_KCOV))
 endif
 
+#
+# Enable ThreadSanitizer flags for kernel except some files or directories
+# we don't want to check (depends on variables KTSAN_SANITIZE_obj.o, KTSAN_SANITIZE)
+#
+ifeq ($(CONFIG_KTSAN),y)
+_c_flags += $(if $(patsubst n%,, \
+	$(KTSAN_SANITIZE_$(basetarget).o)$(KTSAN_SANITIZE)y), \
+	$(CFLAGS_KTSAN))
+endif
+
 # $(srctree)/$(src) for including checkin headers from generated source files
 # $(objtree)/$(obj) for including generated headers from checkin source files
 ifeq ($(KBUILD_EXTMOD),)
diff --git a/tools/objtool/check.c b/tools/objtool/check.c
index 172f99195726..ba15f78a0978 100644
--- a/tools/objtool/check.c
+++ b/tools/objtool/check.c
@@ -470,6 +470,31 @@ static const char *uaccess_safe_builtin[] = {
 	"__asan_report_store4_noabort",
 	"__asan_report_store8_noabort",
 	"__asan_report_store16_noabort",
+	/* KTSAN out-of-line */
+	"__tsan_read_range",
+	"__tsan_read1",
+	"__tsan_read2",
+	"__tsan_read4",
+	"__tsan_read8",
+	"__tsan_read16",
+	"__tsan_unaligned_read_range",
+	"__tsan_unaligned_read1",
+	"__tsan_unaligned_read2",
+	"__tsan_unaligned_read4",
+	"__tsan_unaligned_read8",
+	"__tsan_unaligned_read16",
+	"__tsan_write_range",
+	"__tsan_write1",
+	"__tsan_write2",
+	"__tsan_write4",
+	"__tsan_write8",
+	"__tsan_write16",
+	"__tsan_unaligned_write_range",
+	"__tsan_unaligned_write1",
+	"__tsan_unaligned_write2",
+	"__tsan_unaligned_write4",
+	"__tsan_unaligned_write8",
+	"__tsan_unaligned_write16",
 	/* KCOV */
 	"write_comp_data",
 	"__sanitizer_cov_trace_pc",
